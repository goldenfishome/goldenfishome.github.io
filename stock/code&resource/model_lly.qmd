---
title: "ARMA/ARIMA/SARIMA Models for LLY"
---



```{r ,echo=FALSE, message=FALSE, warning=FALSE}
library(flipbookr)
library(tidyverse)
library(ggplot2)
library(forecast)
library(astsa) 
library(xts)
library(tseries)
library(fpp2)
library(fma)
library(lubridate)
library(tidyverse)
library(TSstudio)
library(quantmod)
library(tidyquant)
library(plotly)
require(gridExtra)
library(knitr)
library(kableExtra)

options("getSymbols.warning4.0"=FALSE)
options("getSymbols.yahoo.warning"=FALSE)

tickers = c("PFE","UNH","JNJ", "AZN", "SYK", "CVS","LLY")
for (i in tickers){
  getSymbols(i,
             from = "2010-1-1",
             to = "2023-1-29")}

x <- list(
  title = "date"
)
y <- list(
  title = "value"
)

stock <- data.frame(PFE$PFE.Adjusted,
                    UNH$UNH.Adjusted,
                    AZN$AZN.Adjusted,
                    SYK$SYK.Adjusted,
                    CVS$CVS.Adjusted,
                    JNJ$JNJ.Adjusted,
                    LLY$LLY.Adjusted)


stock <- data.frame(stock,rownames(stock))
colnames(stock) <- append(tickers,'Dates')

stock$date<-as.Date(stock$Dates,"%Y-%m-%d")
#write.csv(stock, 'data/stock_model.csv', row.names = FALSE)

stock_df <- read.csv('data/stock_model.csv')
stock_df$Dates <- as.Date(stock_df$Dates)

LLY_df <- as.data.frame(LLY)
LLY_df$Dates <- as.Date(rownames(LLY_df))
LLY_ts <- ts(stock_df$LLY, start = c(2010,1),end = c(2023,1),
             frequency = 251)
```



### Step 1: Determine the stationality of time series

Stationality is a pre-requirement of training ARIMA model. This is because term 'Auto Regressive' in ARIMA means it is a linear regression model that uses its own lags as predictors, which work best when the predictors are not correlated and are independent of each other. Stationary time series make sure the statistical properties of time series do not change over time.

Based on information obtained from both ACF graphs and Augmented Dickey-Fuller Test, the time series data is non-stationary.

```{r}
LLY_acf <- ggAcf(LLY_ts,100)+ggtitle("LLY ACF Plot")

LLY_pacf <- ggPacf(LLY_ts,100)+ggtitle("PACF Plot for UHNs")
grid.arrange(LLY_acf, LLY_pacf,nrow=2)
```

```{r}
tseries::adf.test(LLY_ts)
```

### Step 2: Eliminate Non-Stationality

Since this data is non-stationary, it is important to necessary to convert it to stationary time series. This step employs a series of actions to eliminate non-stationality, i.e. differencing the data. It turns out the 1st differened data has shown good stationary property, there are no need to go further at 2nd differencing. What is more, the Augmented Dickey-Fuller Test also confirmed that the 1st differenced data is stationary. Therefore, the1st differencing would be the actions taken to eliminate the non-stationality.

```{r, warning=FALSE}
plot1<- ggAcf((LLY_ts) %>%diff(), 50, main="ACF Plot for 1st differenced Data") 
plot2<- ggAcf((LLY_ts) %>%diff()%>%diff(),50, main="ACF Plot for 2nd differenced Data") 

grid.arrange(plot1, plot2,nrow=2)
```

```{r, message=FALSE, warning=FALSE}
tseries::adf.test((LLY_ts) %>%diff())
```





### Step 3: Determine p,d,q Parameters

The standard notation of ARIMA(p,d,q) include p,d,q 3 parameters. Here are the representations: - p: The number of lag observations included in the model, also called the lag order; order of the AR term. - d: The number of times that the raw observations are differenced, also called the degree of differencing; number of differencing required to make the time series stationary. - q: order of moving average; order of the MA term. It refers to the number of lagged forecast errors that should go into the ARIMA Model.

```{r, message=FALSE, warning=FALSE}
plot3<- ggPacf((LLY_ts) %>%diff(),50, main="PACF Plot for Log Transformed & 1st differenced Data") 

grid.arrange(plot1,plot3)
```

According to the PACF plot and ACF plot above, there are no significant peaks on both graph, therefore, here set both p and q as 0. Since I only differenced the data once, the d would be 1.

### Step 4: Fit ARIMA(p,d,q) model
```{r}
fit1 <- Arima((LLY_ts), order=c(0, 1, 0),include.drift = TRUE) 
summary(fit1)
```

#### Model Diagnostics

-   Inspection of the time plot of the standardized residuals below shows no obvious patterns.
-   Notice that there may be outliers, with a few values exceeding 3 standard deviations in magnitude.
-   The ACF of the standardized residuals shows no apparent departure from the model assumptions, no significant lags shown.
-   The normal Q-Q plot of the residuals shows that the assumption of normality is reasonable, with the exception of the fat-tailed.
-   The model appears to fit well.

```{r}
model_output <- capture.output(sarima((LLY_ts), 0,1,0))
```

```{r}
cat(model_output[9:37], model_output[length(model_output)], sep = "\n") #to get rid of the convergence status and details of the optimization algorithm used by the sarima() 
```



#### Compare with auto.arima() function

auto.arima() returns best ARIMA model according to either AIC, AICc or BIC value. The function conducts a search over possible model within the order constraints provided. The parameters matches with the model.

```{r}
auto.arima((LLY_ts))
```


### Step 5: Forecast

The blue part in graph below forecast the next 100 values of LLY stock price in 80% and 95% confidence level. 

```{r}
LLY_ts %>%
  Arima(order=c(0,1,0),include.drift = TRUE) %>%
  forecast(100) %>%
  autoplot() +
  ylab("LLY stock prices prediction") + xlab("Year")
```



### Step 6: Compare ARIMA model with the benchmark methods
Forecasting benchmarks are very important when testing new forecasting methods, to see how well they perform against some simple alternatives.


#### Average method
Here, the forecast of all future values are equal to the average of the historical data. The residual plot of this method is not stationary. 
```{r}
f1<-meanf((LLY_ts), h=251) #mean
#summary(f1)
checkresiduals(f1)#serial correlation ; Lung Box p <0.05
```


#### Naive method
This method simply set all forecasts to be the value of the last observation. According to error measurement here, ARIMA(4,1,3) outperform the average method. 
```{r}
f2<-naive((LLY_ts), h=11) # naive method
summary(f2)
checkresiduals(f2)#serial correlation ; Lung Box p <0.05
```

#### Seasonal naive method
This method is useful for highly seasonal data, which can set each forecast to be equal to the last observed value from the same season of the year. Here seasonal naive is used to forecast the next 4 values for the LLY stock price series. 
```{r}
f3<-snaive((LLY_ts), h=4) #seasonal naive method
summary(f3)
checkresiduals(f3) #serial correlation ; Lung Box p <0.05
```


#### Drift Method

A variation on the naïve method is to allow the forecasts to increase or
decrease over time, where the amount of change over time is set to be the
average change seen in the historical data. 
```{r}
f4 <- rwf((LLY_ts),drift=TRUE, h=20) 
summary(f4)
checkresiduals(f4)
```


```{r}
autoplot(LLY_ts) +
  autolayer(meanf(LLY_ts, h=100),
            series="Mean.tr", PI=FALSE) +
  autolayer(naive((LLY_ts), h=100),
            series="Naïve.tr", PI=FALSE) +
  autolayer(rwf((LLY_ts), drift=TRUE, h=100),
            series="Drift.tr", PI=FALSE) +
  autolayer(forecast(fit1,100), 
            series="fit",PI=FALSE) +
  ggtitle("LLY Stock Price") +
  xlab("Time") + ylab("Price") +
  guides(colour=guide_legend(title="Forecast"))
```

According to the graph above, ARIMA(0,1,0) outperform most of benchmark method, though its performance is very similar to drift method. 
