[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "arimax.html",
    "href": "arimax.html",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "",
    "text": "ARIMAX\nARIMAX model assumes that future values of a variable linearly depend on its past values, as well as on the values of past (stochastic) shocks. It is an extended version of the ARIMA model, with other independent (predictor) variables. The model is also referred to as the dynamic regression model. The X added to the end stands for “exogenous”. In other words, it suggests adding a separate different outside variable to help measure our endogenous variable.\nThe ‘exogenous’ variables added here are COVID-19 case numbers and COVID-19 vaccine rates. The number of COVID-19 cases in US can affect healthcare stock prices, e.g. UNH. Higher case numbers may result in increased demand for healthcare services, such as hospitalizations, treatments, and testing, which could positively impact the stock prices of healthcare companies involved in providing those services. Conversely, lower case numbers may lead to reduced demand for healthcare services, potentially resulting in lower stock prices for healthcare companies.\nVaccine rates, specifically the rate at which a population is vaccinated against COVID-19, can also impact healthcare stock prices. Higher vaccine rates are generally seen as positive for healthcare companies, as vaccines are considered a key tool in controlling the spread of the virus and reducing the severity of illness. Higher vaccine rates may lead to decreased demand for COVID-19 treatments and testing, but increased demand for vaccines and other preventive healthcare measures, which could positively impact the stock prices of healthcare companies involved in vaccine production or distribution, as well as other preventive healthcare services.\nIn this section, we choose the UNH stock price to fit ARIMAX model with COVID-19 case numbers and COVID-19 vaccine rates.\n\nStep 1: Data Prepare\n\n\nShow the code\ngetSymbols(\"UNH\", from=\"2020-12-16\", src=\"yahoo\")\n\n\n\n\nShow the code\nUNH_df <- as.data.frame(UNH)\nUNH_df$Date <- rownames(UNH_df)\nUNH_df <- UNH_df[c('Date','UNH.Adjusted')]\nUNH_df <- UNH_df %>%\n  mutate(Date = as.Date(Date)) %>%\n  complete(Date = seq.Date(min(Date), max(Date), by=\"day\"))\n\n# fill missing values in stock \nUNH_df <- UNH_df %>% fill(UNH.Adjusted)\n\nnew_dates <- seq(as.Date('2020-12-16'), as.Date('2023-3-21'),'week')\n\nUNH_df <- UNH_df[which((UNH_df$Date) %in% new_dates),]\n\nvaccine_df <- read.csv('data/vaccine_clean.csv')\n\nnew_dates <- seq(as.Date('2020-12-16'), as.Date('2023-3-22'),'week')\n\n#vaccine_df\nvaccine_df$Date <- as.Date(vaccine_df$Date)\nvaccine_df <- vaccine_df[which((vaccine_df$Date) %in% new_dates),]\n\n#covid_df\ncovid_df <- read.csv('data/covid.csv')\n\n#covid_ts <- ts(covid_df$Weekly.Cases, start = c(2020,1,29), frequency = 54)\ncovid_df$Date <- as.Date(covid_df$Date)\ncovid_df <- covid_df[covid_df$Date >= '2020-12-16'&covid_df$Date < '2023-03-22',]\n\n# combine all data, create dataframe\ndf <- data.frame(UNH_df, vaccine_df$total_doses, covid_df$Weekly.Cases)\ncolnames(df) <- c('Date', 'stock_price', 'vaccine_dose','covid_case')\n\nknitr::kable(head(df))\n\n\n\n\n\nDate\nstock_price\nvaccine_dose\ncovid_case\n\n\n\n\n2020-12-16\n329.2309\n160010\n1526464\n\n\n2020-12-23\n327.5330\n577895\n1499703\n\n\n2020-12-30\n334.7125\n848447\n1299023\n\n\n2021-01-06\n348.5671\n1029958\n1584212\n\n\n2021-01-13\n344.4632\n1334188\n1716354\n\n\n2021-01-20\n340.3883\n1614134\n1391546\n\n\n\n\n\n\n\nStep 2: Plotting the Data\n\n\nShow the code\ndf.ts<-ts(df,star=decimal_date(as.Date(\"2020-12-16\",format = \"%Y-%m-%d\")),frequency = 52)\n\nautoplot(df.ts[,c(2:4)], facets=TRUE) +\n  xlab(\"Date\") + ylab(\"\") +\n  ggtitle(\"Variables influencing UNH Stock Price in USA\")\n\n\n\n\n\nUNH stock price, Covid, Vaccine values\n\n\nStep 3: Fitting the model using ’auto.arima()`\nHere I’m using auto.arima() function to fit the ARIMAX model. Here we are trying to predict UNH stock price using COVID vaccine dose and COVID cases. All variables are time series and the exogenous variables in this case are vaccine_dose and covid_case.\n\n\nShow the code\nxreg <- cbind(Vac = df.ts[, \"vaccine_dose\"],\n              Imp = df.ts[, \"covid_case\"])\n\nfit <- auto.arima(df.ts[, \"stock_price\"], xreg = xreg)\nsummary(fit)\n\n\nSeries: df.ts[, \"stock_price\"] \nRegression with ARIMA(0,1,0) errors \n\nCoefficients:\n        Vac    Imp\n      0e+00  0e+00\ns.e.  1e-04  1e-04\n\nsigma^2 = 177.9:  log likelihood = -468.1\nAIC=942.2   AICc=942.41   BIC=950.49\n\nTraining set error measures:\n                   ME     RMSE      MAE       MPE     MAPE      MASE       ACF1\nTraining set 1.097616 13.16639 10.30662 0.2339931 2.273227 0.1046469 0.04011562\n\n\n\n\nShow the code\ncheckresiduals(fit)\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Regression with ARIMA(0,1,0) errors\nQ* = 21.112, df = 24, p-value = 0.6321\n\nModel df: 0.   Total lags used: 24\n\n\nThis is an ARIMA model. This is a Regression model with ARIMA(0,1,0) errors.\n\n\nStep 4: Fitting the model manually\nHere we will first have to fit the linear regression model predicting stock price using Covid cases and vaccine doses.\nThen for the residuals, we will fit an ARIMA/SARIMA model.\n\n\nShow the code\ndf$stock_price <- ts(df$stock_price,star=decimal_date(as.Date(\"2020-12-16\",format = \"%Y-%m-%d\")),frequency = 52)\ndf$vaccine_dose <-ts(df$vaccine_dose,star=decimal_date(as.Date(\"2020-12-16\",format = \"%Y-%m-%d\")),frequency = 52)\ndf$covid_case<-ts(df$covid_case,star=decimal_date(as.Date(\"2020-12-16\",format = \"%Y-%m-%d\")),frequency = 52)\n\n############# First fit the linear model##########\nfit.reg <- lm(stock_price ~ vaccine_dose+covid_case, data=df)\nsummary(fit.reg)\n\n\n\nCall:\nlm(formula = stock_price ~ vaccine_dose + covid_case, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-154.24  -42.59    8.62   42.91   82.36 \n\nCoefficients:\n               Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   4.954e+02  7.932e+00  62.461  < 2e-16 ***\nvaccine_dose -4.341e-05  5.043e-06  -8.607 4.47e-14 ***\ncovid_case   -3.275e-06  5.345e-06  -0.613    0.541    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 52.02 on 115 degrees of freedom\nMultiple R-squared:  0.3944,    Adjusted R-squared:  0.3839 \nF-statistic: 37.45 on 2 and 115 DF,  p-value: 2.988e-13\n\n\n\n\nShow the code\nres.fit<-ts(residuals(fit.reg),star=decimal_date(as.Date(\"2020-12-16\",format = \"%Y-%m-%d\")),frequency = 52)\n\n############## Then look at the residuals ############\nacf(res.fit)\n\n\n\n\n\n\n\nShow the code\nPacf(res.fit)\n\n\n\n\n\n\n\nShow the code\nres.fit %>% diff() %>% ggtsdisplay()\n\n\n\n\n\n\n\nShow the code\nres.fit %>% diff() %>% diff(52) %>% ggtsdisplay()\n\n\n\n\n\nFinding the model parameters.\n\n\nShow the code\nd=1\ni=1\ntemp= data.frame()\nls=matrix(rep(NA,6*23),nrow=23) # roughly nrow = 3x4x2\n\n\nfor (p in 1:3)# p=1,2,\n{\n  for(q in 1:3)# q=1,2,\n  {\n    for(d in 0:1)# \n    {\n      \n      if(p-1+d+q-1<=8)\n      {\n        \n        model<- Arima(res.fit,order=c(p-1,d,q-1),include.drift=TRUE) \n        ls[i,]= c(p-1,d,q-1,model$aic,model$bic,model$aicc)\n        i=i+1\n        #print(i)\n        \n      }\n      \n    }\n  }\n}\n\ntemp= as.data.frame(ls)\nnames(temp)= c(\"p\",\"d\",\"q\",\"AIC\",\"BIC\",\"AICc\")\n\n#temp\nknitr::kable(temp)\n\n\n\n\n\np\nd\nq\nAIC\nBIC\nAICc\n\n\n\n\n0\n0\n0\n1217.4187\n1225.7307\n1217.6292\n\n\n0\n1\n0\n993.5902\n999.1146\n993.6955\n\n\n0\n0\n1\n1117.7725\n1128.8552\n1118.1264\n\n\n0\n1\n1\n993.1255\n1001.4121\n993.3379\n\n\n0\n0\n2\n1058.3968\n1072.2502\n1058.9325\n\n\n0\n1\n2\n994.2045\n1005.2532\n994.5617\n\n\n1\n0\n0\n1005.4074\n1016.4901\n1005.7613\n\n\n1\n1\n0\n992.7213\n1001.0078\n992.9337\n\n\n1\n0\n1\n1003.8973\n1017.7508\n1004.4331\n\n\n1\n1\n1\n994.2404\n1005.2891\n994.5976\n\n\n1\n0\n2\n1003.9562\n1020.5803\n1004.7130\n\n\n1\n1\n2\n995.7890\n1009.5998\n996.3295\n\n\n2\n0\n0\n1002.8799\n1016.7334\n1003.4157\n\n\n2\n1\n0\n994.1790\n1005.2277\n994.5361\n\n\n2\n0\n1\n999.6162\n1016.2403\n1000.3730\n\n\n2\n1\n1\n995.8971\n1009.7079\n996.4376\n\n\n2\n0\n2\n1005.7699\n1025.1647\n1006.7881\n\n\n2\n1\n2\n995.6380\n1012.2111\n996.4017\n\n\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\n\n\n\n\nShow the code\ntemp[which.min(temp$AIC),] \n\n\n  p d q      AIC      BIC     AICc\n8 1 1 0 992.7213 1001.008 992.9337\n\n\n\n\nShow the code\ntemp[which.min(temp$BIC),] \n\n\n  p d q      AIC      BIC     AICc\n2 0 1 0 993.5902 999.1146 993.6955\n\n\n\n\nShow the code\ntemp[which.min(temp$AICc),] \n\n\n  p d q      AIC      BIC     AICc\n8 1 1 0 992.7213 1001.008 992.9337\n\n\n\n\nShow the code\nset.seed(1234)\n\nmodel_output12 <- capture.output(sarima(res.fit, 0,1,0)) \n\n\n\n\n\n\n\nShow the code\ncat(model_output12[9:38], model_output12[length(model_output12)], sep = \"\\n\")\n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n      constant\n        1.0885\ns.e.    1.5357\n\nsigma^2 estimated as 275.9:  log likelihood = -494.8,  aic = 993.59\n\n$degrees_of_freedom\n[1] 116\n\n$ttable\n         Estimate     SE t.value p.value\nconstant   1.0885 1.5357  0.7088  0.4799\n\n$AIC\n[1] 8.492224\n\n$AICc\n[1] 8.492521\n\n$BIC\n[1] 8.539441\n\n\n\n\nShow the code\nset.seed(1234)\n\nmodel_output13 <- capture.output(sarima(res.fit, 1,1,0)) \n\n\n\n\n\n\n\nShow the code\ncat(model_output13[16:46], model_output13[length(model_output13)], sep = \"\\n\")\n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n         ar1  constant\n      0.1556    1.1047\ns.e.  0.0913    1.7935\n\nsigma^2 estimated as 269.2:  log likelihood = -493.36,  aic = 992.72\n\n$degrees_of_freedom\n[1] 115\n\n$ttable\n         Estimate     SE t.value p.value\nar1        0.1556 0.0913  1.7049  0.0909\nconstant   1.1047 1.7935  0.6160  0.5391\n\n$AIC\n[1] 8.484798\n\n$AICc\n[1] 8.485697\n\n$BIC\n[1] 8.555623\n\n\nARIMA(0,1,0) and ARIMA(1,1,0) both look okay.\n\n\nStep 5: Using Cross Validation\n\n\nShow the code\nk <- 36 # minimum data length for fitting a model \nn <- length(res.fit)\nn-k # rest of the observations\n\n\n[1] 82\n\n\n\n\nShow the code\ni=1\nerr1 = c()\nerr2 = c()\n\nrmse1 <- c()\nrmse2 <- c()\n\nfor(i in 1:(n-k))\n{\n  xtrain <- res.fit[1:(k-1)+i] #observations from 1 to 75\n  xtest <- res.fit[k+i] #76th observation as the test set\n  \n  fit <- Arima(xtrain, order=c(1,1,0),include.drift=FALSE, method=\"ML\")\n  fcast1 <- forecast(fit, h=1)\n  \n  fit2 <- Arima(xtrain, order=c(0,1,0),include.drift=FALSE, method=\"ML\")\n  fcast2 <- forecast(fit2, h=1)\n  \n  #capture error for each iteration\n  # This is mean absolute error\n  err1 = c(err1, abs(fcast1$mean-xtest)) \n  err2 = c(err2, abs(fcast2$mean-xtest))\n  \n  # This is mean squared error\n  err3 = c(err1, (fcast1$mean-xtest)^2)\n  err4 = c(err2, (fcast2$mean-xtest)^2)\n  \n  rmse1 <- c(rmse1, sqrt((fcast1$mean-xtest)^2))\n  rmse2 <- c(rmse2, sqrt((fcast2$mean-xtest)^2))\n  \n}\n\n(MAE1=mean(err1)) # This is mean absolute error\n\n\n[1] 13.45554\n\n\n\n\nShow the code\n(MAE2=mean(err2)) #has slightly higher error\n\n\n[1] 13.46804\n\n\n\n\nShow the code\nMSE1=mean(err1) #fit 1,1,0\nMSE2=mean(err2)#fit 0,1,0\n\nMSE1\n\n\n[1] 13.45554\n\n\n\n\nShow the code\nMSE2\n\n\n[1] 13.46804\n\n\n\n\nShow the code\nrmse_df <- data.frame(rmse1,rmse2)\nrmse_df$x <- as.numeric(rownames(rmse_df))\n\nplot(rmse_df$x, rmse_df$rmse1, type = 'l', col=2, xlab=\"horizon\", ylab=\"RMSE\")\nlines(rmse_df$x, rmse_df$rmse2, type=\"l\",col=3)\nlegend(\"topleft\",legend=c(\"fit1\",\"fit2\"),col=2:4,lty=1)\n\n\n\n\n\n\n\nStep 6: forcasting\n\n\nShow the code\nvac_fit<-auto.arima(df$vaccine_dose) #fiting an ARIMA model to the vaccine_dose variable\nsummary(vac_fit)\n\n\nSeries: df$vaccine_dose \nARIMA(1,1,1) \n\nCoefficients:\n         ar1      ma1\n      0.8341  -0.6298\ns.e.  0.0916   0.1128\n\nsigma^2 = 4.824e+10:  log likelihood = -1604.18\nAIC=3214.36   AICc=3214.57   BIC=3222.65\n\nTraining set error measures:\n                    ME     RMSE      MAE          MPE     MAPE      MASE\nTraining set -1592.486 216829.1 132002.8 0.0009291879 13.08388 0.1149613\n                    ACF1\nTraining set -0.06829849\n\n\n\n\nShow the code\nfvac<-forecast(vac_fit)\n\ncovid_fit<-auto.arima(df$covid_case) #fiting an ARIMA model to the covid_case variable\nsummary(covid_fit)\n\n\nSeries: df$covid_case \nARIMA(2,0,2) with non-zero mean \n\nCoefficients:\n         ar1      ar2     ma1     ma2      mean\n      1.3306  -0.5128  0.8165  0.4848  733469.2\ns.e.  0.1153   0.1117  0.1363  0.1076  189902.4\n\nsigma^2 = 2.904e+10:  log likelihood = -1588.94\nAIC=3189.87   AICc=3190.63   BIC=3206.5\n\nTraining set error measures:\n                    ME     RMSE      MAE       MPE     MAPE      MASE\nTraining set -424.0988 166757.6 99191.78 -6.959882 17.02194 0.1095754\n                    ACF1\nTraining set -0.00122146\n\n\n\n\nShow the code\nfcov<-forecast(covid_fit)\n\nfxreg <- cbind(Vac = fvac$mean,\n              Cov = fcov$mean)\n\nfcast <- forecast(fit, xreg=fxreg) #fimp$mean gives the forecasted values\n\n\nWarning in forecast.forecast_ARIMA(fit, xreg = fxreg): xreg not required by this\nmodel, ignoring the provided regressors\n\n\nShow the code\nautoplot(fcast) + xlab(\"Date\") +\n  ylab(\"Price\")\n\n\n\n\n\n\n\n\nVAR\nVAR models (vector autoregressive models) are used for multivariate time series. The structure is that each variable is a linear function of past lags of itself and past lags of the other variables. A Vector autoregressive (VAR) model is useful when one is interested in predicting multiple time series variables using a single model.\nThe variables we are interested here are GDP and Unemployment rate in US. The overall health of the economy, as reflected by the GDP, can influence healthcare stock prices. A strong GDP generally indicates a healthy economy with higher levels of consumer spending, business investment, and economic growth. In such an environment, healthcare companies may experience increased demand for their products and services, which could positively impact their stock prices. Conversely, a weak GDP may signal an economic slowdown or recession, which could result in reduced demand for healthcare products and services, potentially leading to lower stock prices for healthcare companies. The unemployment rate, which reflects the percentage of the labor force that is unemployed, can also affect healthcare stock prices. A low unemployment rate is generally indicative of a strong labor market, with more people employed and potentially having access to employer-sponsored healthcare benefits. This may result in increased demand for healthcare services, positively impacting healthcare stock prices. On the other hand, a high unemployment rate may indicate a weak labor market, with more people losing their jobs and potentially losing access to healthcare benefits, leading to reduced demand for healthcare services and potentially lower stock prices for healthcare companies.\nIn this section, I’m fitting a VAR model to find multivariate relationship between the series UNH stock price, GDP, and Employment.\n\nStep 1: Data Preparing\n\n\nShow the code\ngetSymbols(\"UNH\", from=\"1989-12-29\", src=\"yahoo\")\n\n\n[1] \"UNH\"\n\n\nShow the code\nUNH_df <- as.data.frame(UNH)\nUNH_df$Date <- rownames(UNH_df)\nUNH_df <- UNH_df[c('Date','UNH.Adjusted')]\nUNH_df <- UNH_df %>%\n  mutate(Date = as.Date(Date)) %>%\n  complete(Date = seq.Date(min(Date), max(Date), by=\"day\"))\n\n# fill missing values in stock \nUNH_df <- UNH_df %>% fill(UNH.Adjusted)\n#UNH_df\nnew_dates <- seq(as.Date('1990-01-01'), as.Date('2022-10-01'),'quarter')\n#new_dates\nUNH_df <- UNH_df[which((UNH_df$Date) %in% new_dates),]\n\ngdp <- read.csv('data/GDP59.CSV')\ngdp$DATE <- as.Date(gdp$DATE)\ngdp <- gdp[which((gdp$DATE) %in% new_dates),]\n\nemp <- read.csv('data/Employment59.CSV')\nemp$DATE <- as.Date(emp$DATE)\nemp <- emp[which((emp$DATE) %in% new_dates),]\n\n\n\n\nShow the code\ndd <- data.frame(UNH_df,gdp,emp)\ndd <- dd[,c(1,2,4,6)]\ncolnames(dd) <- c('DATE', 'stock_price','GDP','Employment')\nknitr::kable(head(dd))\n\n\n\n\n\n\nDATE\nstock_price\nGDP\nEmployment\n\n\n\n\n125\n1990-01-01\n0.308110\n5872.701\n109418.7\n\n\n126\n1990-04-01\n0.247759\n5960.028\n109786.7\n\n\n127\n1990-07-01\n0.438343\n6015.116\n109651.0\n\n\n128\n1990-10-01\n0.435356\n6004.733\n109255.0\n\n\n129\n1991-01-01\n0.591069\n6035.178\n108783.0\n\n\n130\n1991-04-01\n0.985114\n6126.862\n108312.3\n\n\n\n\n\n\n\nStep 2: Plotting the data\n\n\nShow the code\ndd.ts<-ts(dd,star=decimal_date(as.Date(\"1990-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\n\nautoplot(dd.ts[,c(2:4)], facets=TRUE) +\n  xlab(\"Year\") + ylab(\"\") +\n  ggtitle(\"UNH Stock Price, GDP and Employment in USA\")\n\n\n\n\n\n\n\nStep 3: Fitting a VAR model\n\n\nShow the code\nVARselect(dd[, c(2:4)], lag.max=10, type=\"both\")\n\n\n$selection\nAIC(n)  HQ(n)  SC(n) FPE(n) \n    10      9      9      9 \n\n$criteria\n                  1            2            3            4            5\nAIC(n) 2.941404e+01 2.865601e+01 2.849637e+01 2.814189e+01 2.812288e+01\nHQ(n)  2.955407e+01 2.888006e+01 2.880444e+01 2.853398e+01 2.859898e+01\nSC(n)  2.975880e+01 2.920762e+01 2.925484e+01 2.910721e+01 2.929506e+01\nFPE(n) 5.948600e+12 2.788635e+12 2.379348e+12 1.671832e+12 1.644338e+12\n                  6            7            8            9           10\nAIC(n) 2.758293e+01 2.723307e+01 2.696700e+01 2.669062e+01 2.668670e+01\nHQ(n)  2.814305e+01 2.787720e+01 2.769515e+01 2.750279e+01 2.758289e+01\nSC(n)  2.896196e+01 2.881895e+01 2.875974e+01 2.869020e+01 2.889314e+01\nFPE(n) 9.616254e+11 6.809539e+11 5.251260e+11 4.014816e+11 4.038685e+11\n\n\nIt’s clear that according to selection criteria p=10 and 9 are good.\nI’m fitting several models with p=1(for simplicity), 5, and 9.=> VAR(1), VAR(5), VAR(9)\n\n\nShow the code\nsummary(VAR(dd[, c(2:4)], p=1, type='both'))\n\n\n\nVAR Estimation Results:\n========================= \nEndogenous variables: stock_price, GDP, Employment \nDeterministic variables: both \nSample size: 131 \nLog Likelihood: -2461.36 \nRoots of the characteristic polynomial:\n1.028 0.9422  0.78\nCall:\nVAR(y = dd[, c(2:4)], p = 1, type = \"both\")\n\n\nEstimation results for equation stock_price: \n============================================ \nstock_price = stock_price.l1 + GDP.l1 + Employment.l1 + const + trend \n\n                 Estimate Std. Error t value Pr(>|t|)    \nstock_price.l1  1.0341243  0.0394781  26.195   <2e-16 ***\nGDP.l1         -0.0005115  0.0039797  -0.129   0.8979    \nEmployment.l1  -0.0006163  0.0003038  -2.029   0.0446 *  \nconst          69.2145318 34.7475948   1.992   0.0485 *  \ntrend           0.3006692  0.4555344   0.660   0.5104    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 14.62 on 126 degrees of freedom\nMultiple R-Squared: 0.9849, Adjusted R-squared: 0.9844 \nF-statistic:  2057 on 4 and 126 DF,  p-value: < 2.2e-16 \n\n\nEstimation results for equation GDP: \n==================================== \nGDP = stock_price.l1 + GDP.l1 + Employment.l1 + const + trend \n\n                 Estimate Std. Error t value Pr(>|t|)    \nstock_price.l1    2.55504    0.64584   3.956 0.000126 ***\nGDP.l1            0.79020    0.06511  12.137  < 2e-16 ***\nEmployment.l1    -0.01069    0.00497  -2.151 0.033349 *  \nconst          2355.16303  568.45520   4.143 6.24e-05 ***\ntrend            27.89291    7.45234   3.743 0.000275 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 239.2 on 126 degrees of freedom\nMultiple R-Squared: 0.998,  Adjusted R-squared: 0.998 \nF-statistic: 1.607e+04 on 4 and 126 DF,  p-value: < 2.2e-16 \n\n\nEstimation results for equation Employment: \n=========================================== \nEmployment = stock_price.l1 + GDP.l1 + Employment.l1 + const + trend \n\n                 Estimate Std. Error t value Pr(>|t|)    \nstock_price.l1  6.183e+00  4.862e+00   1.272  0.20584    \nGDP.l1         -5.610e-01  4.901e-01  -1.145  0.25458    \nEmployment.l1   9.253e-01  3.741e-02  24.733  < 2e-16 ***\nconst           1.161e+04  4.279e+03   2.712  0.00762 ** \ntrend           8.544e+01  5.610e+01   1.523  0.13028    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 1801 on 126 degrees of freedom\nMultiple R-Squared: 0.9786, Adjusted R-squared: 0.9779 \nF-statistic:  1441 on 4 and 126 DF,  p-value: < 2.2e-16 \n\n\n\nCovariance matrix of residuals:\n            stock_price    GDP Employment\nstock_price       213.8   1386      12148\nGDP              1385.7  57224     396720\nEmployment      12147.6 396720    3242998\n\nCorrelation matrix of residuals:\n            stock_price    GDP Employment\nstock_price      1.0000 0.3962     0.4613\nGDP              0.3962 1.0000     0.9209\nEmployment       0.4613 0.9209     1.0000\n\n\n\n\nShow the code\nsummary(VAR(dd[, c(2:4)], p=5, type='both'))\n\n\n\nVAR Estimation Results:\n========================= \nEndogenous variables: stock_price, GDP, Employment \nDeterministic variables: both \nSample size: 127 \nLog Likelihood: -2267.864 \nRoots of the characteristic polynomial:\n1.054 0.9435 0.9435 0.8781 0.8781 0.7952 0.7952 0.7806 0.7806 0.7709 0.7709 0.6959 0.4107 0.4107 0.3355\nCall:\nVAR(y = dd[, c(2:4)], p = 5, type = \"both\")\n\n\nEstimation results for equation stock_price: \n============================================ \nstock_price = stock_price.l1 + GDP.l1 + Employment.l1 + stock_price.l2 + GDP.l2 + Employment.l2 + stock_price.l3 + GDP.l3 + Employment.l3 + stock_price.l4 + GDP.l4 + Employment.l4 + stock_price.l5 + GDP.l5 + Employment.l5 + const + trend \n\n                 Estimate Std. Error t value Pr(>|t|)    \nstock_price.l1  5.454e-01  1.048e-01   5.204 9.13e-07 ***\nGDP.l1          6.007e-02  1.789e-02   3.358  0.00108 ** \nEmployment.l1  -7.709e-03  2.385e-03  -3.232  0.00162 ** \nstock_price.l2  2.323e-01  1.468e-01   1.583  0.11639    \nGDP.l2         -6.188e-02  2.695e-02  -2.297  0.02353 *  \nEmployment.l2   8.708e-03  4.117e-03   2.115  0.03668 *  \nstock_price.l3 -6.701e-02  1.636e-01  -0.410  0.68293    \nGDP.l3          1.256e-02  2.614e-02   0.480  0.63198    \nEmployment.l3  -3.778e-03  4.122e-03  -0.917  0.36135    \nstock_price.l4  2.159e-01  1.796e-01   1.202  0.23192    \nGDP.l4         -1.114e-02  2.275e-02  -0.490  0.62525    \nEmployment.l4   2.759e-03  3.733e-03   0.739  0.46146    \nstock_price.l5  2.480e-01  1.956e-01   1.268  0.20751    \nGDP.l5         -1.060e-02  1.379e-02  -0.769  0.44375    \nEmployment.l5  -6.394e-04  2.197e-03  -0.291  0.77155    \nconst           1.189e+02  3.723e+01   3.194  0.00183 ** \ntrend           1.493e+00  5.610e-01   2.661  0.00897 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 12.06 on 110 degrees of freedom\nMultiple R-Squared: 0.9909, Adjusted R-squared: 0.9896 \nF-statistic: 750.5 on 16 and 110 DF,  p-value: < 2.2e-16 \n\n\nEstimation results for equation GDP: \n==================================== \nGDP = stock_price.l1 + GDP.l1 + Employment.l1 + stock_price.l2 + GDP.l2 + Employment.l2 + stock_price.l3 + GDP.l3 + Employment.l3 + stock_price.l4 + GDP.l4 + Employment.l4 + stock_price.l5 + GDP.l5 + Employment.l5 + const + trend \n\n                 Estimate Std. Error t value Pr(>|t|)    \nstock_price.l1   -1.30446    1.54245  -0.846 0.399554    \nGDP.l1            1.92765    0.26327   7.322 4.28e-11 ***\nEmployment.l1    -0.18525    0.03510  -5.277 6.65e-07 ***\nstock_price.l2    7.85076    2.15990   3.635 0.000425 ***\nGDP.l2           -0.49799    0.39656  -1.256 0.211853    \nEmployment.l2     0.14315    0.06059   2.363 0.019907 *  \nstock_price.l3    1.97663    2.40806   0.821 0.413514    \nGDP.l3           -0.68441    0.38474  -1.779 0.078022 .  \nEmployment.l3     0.07846    0.06066   1.293 0.198569    \nstock_price.l4   -3.50096    2.64351  -1.324 0.188128    \nGDP.l4           -0.53338    0.33481  -1.593 0.114013    \nEmployment.l4     0.03338    0.05493   0.608 0.544646    \nstock_price.l5   -4.52830    2.87918  -1.573 0.118643    \nGDP.l5            0.62554    0.20300   3.081 0.002603 ** \nEmployment.l5    -0.07633    0.03233  -2.361 0.019978 *  \nconst          1593.63731  547.91858   2.909 0.004394 ** \ntrend            21.22795    8.25642   2.571 0.011473 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 177.5 on 110 degrees of freedom\nMultiple R-Squared: 0.999,  Adjusted R-squared: 0.9988 \nF-statistic:  6815 on 16 and 110 DF,  p-value: < 2.2e-16 \n\n\nEstimation results for equation Employment: \n=========================================== \nEmployment = stock_price.l1 + GDP.l1 + Employment.l1 + stock_price.l2 + GDP.l2 + Employment.l2 + stock_price.l3 + GDP.l3 + Employment.l3 + stock_price.l4 + GDP.l4 + Employment.l4 + stock_price.l5 + GDP.l5 + Employment.l5 + const + trend \n\n                 Estimate Std. Error t value Pr(>|t|)    \nstock_price.l1 -3.786e+01  1.190e+01  -3.182  0.00190 ** \nGDP.l1          6.787e+00  2.030e+00   3.343  0.00113 ** \nEmployment.l1  -1.385e-02  2.707e-01  -0.051  0.95928    \nstock_price.l2  7.770e+01  1.666e+01   4.664 8.77e-06 ***\nGDP.l2         -2.705e+00  3.058e+00  -0.885  0.37831    \nEmployment.l2   6.207e-01  4.673e-01   1.328  0.18687    \nstock_price.l3  4.014e+00  1.857e+01   0.216  0.82928    \nGDP.l3         -8.103e+00  2.967e+00  -2.731  0.00736 ** \nEmployment.l3   1.098e+00  4.678e-01   2.346  0.02075 *  \nstock_price.l4 -1.641e+01  2.039e+01  -0.805  0.42269    \nGDP.l4         -1.499e+00  2.582e+00  -0.581  0.56270    \nEmployment.l4  -1.900e-01  4.237e-01  -0.448  0.65470    \nstock_price.l5 -2.700e+01  2.221e+01  -1.216  0.22657    \nGDP.l5          4.674e+00  1.566e+00   2.985  0.00349 ** \nEmployment.l5  -5.865e-01  2.493e-01  -2.352  0.02044 *  \nconst           1.222e+04  4.226e+03   2.891  0.00462 ** \ntrend           1.203e+02  6.368e+01   1.890  0.06146 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 1369 on 110 degrees of freedom\nMultiple R-Squared: 0.9879, Adjusted R-squared: 0.9861 \nF-statistic: 560.4 on 16 and 110 DF,  p-value: < 2.2e-16 \n\n\n\nCovariance matrix of residuals:\n            stock_price      GDP Employment\nstock_price       145.5    455.4       5503\nGDP               455.4  31506.6     225145\nEmployment       5503.3 225144.8    1874014\n\nCorrelation matrix of residuals:\n            stock_price    GDP Employment\nstock_price      1.0000 0.2127     0.3333\nGDP              0.2127 1.0000     0.9266\nEmployment       0.3333 0.9266     1.0000\n\n\n\n\nShow the code\nsummary(VAR(dd[, c(2:4)], p=9, type='both'))\n\n\n\nVAR Estimation Results:\n========================= \nEndogenous variables: stock_price, GDP, Employment \nDeterministic variables: both \nSample size: 123 \nLog Likelihood: -2076.371 \nRoots of the characteristic polynomial:\n1.058 1.051 1.018 1.018 1.012 1.012 1.012 1.012 1.005 1.005 1.003 1.003 0.9793 0.9793 0.9726 0.9726 0.8256 0.8256 0.8098 0.8098 0.7926 0.7926 0.6662 0.6662 0.4868 0.4868 0.0237\nCall:\nVAR(y = dd[, c(2:4)], p = 9, type = \"both\")\n\n\nEstimation results for equation stock_price: \n============================================ \nstock_price = stock_price.l1 + GDP.l1 + Employment.l1 + stock_price.l2 + GDP.l2 + Employment.l2 + stock_price.l3 + GDP.l3 + Employment.l3 + stock_price.l4 + GDP.l4 + Employment.l4 + stock_price.l5 + GDP.l5 + Employment.l5 + stock_price.l6 + GDP.l6 + Employment.l6 + stock_price.l7 + GDP.l7 + Employment.l7 + stock_price.l8 + GDP.l8 + Employment.l8 + stock_price.l9 + GDP.l9 + Employment.l9 + const + trend \n\n                 Estimate Std. Error t value Pr(>|t|)    \nstock_price.l1  0.6177996  0.1140057   5.419 4.61e-07 ***\nGDP.l1          0.0415176  0.0139527   2.976 0.003717 ** \nEmployment.l1  -0.0069645  0.0020733  -3.359 0.001131 ** \nstock_price.l2  0.3603698  0.1652332   2.181 0.031680 *  \nGDP.l2         -0.0157162  0.0211664  -0.743 0.459633    \nEmployment.l2   0.0048987  0.0033424   1.466 0.146091    \nstock_price.l3  0.0177132  0.1607488   0.110 0.912492    \nGDP.l3         -0.0009782  0.0212811  -0.046 0.963436    \nEmployment.l3  -0.0018913  0.0034992  -0.540 0.590133    \nstock_price.l4 -0.1569004  0.1711382  -0.917 0.361591    \nGDP.l4         -0.0352142  0.0218848  -1.609 0.110954    \nEmployment.l4   0.0072039  0.0035652   2.021 0.046168 *  \nstock_price.l5  0.3383736  0.2044761   1.655 0.101294    \nGDP.l5         -0.0156536  0.0222246  -0.704 0.482964    \nEmployment.l5   0.0016545  0.0036673   0.451 0.652931    \nstock_price.l6  0.1416257  0.2112798   0.670 0.504295    \nGDP.l6         -0.0184066  0.0221573  -0.831 0.408234    \nEmployment.l6  -0.0012687  0.0036506  -0.348 0.728965    \nstock_price.l7 -0.9691887  0.2444901  -3.964 0.000144 ***\nGDP.l7          0.0190304  0.0219980   0.865 0.389189    \nEmployment.l7  -0.0017353  0.0035712  -0.486 0.628163    \nstock_price.l8  1.0643910  0.2599678   4.094 8.96e-05 ***\nGDP.l8          0.0123809  0.0212168   0.584 0.560926    \nEmployment.l8  -0.0058935  0.0033845  -1.741 0.084894 .  \nstock_price.l9 -0.2681903  0.2116089  -1.267 0.208148    \nGDP.l9          0.0063637  0.0130395   0.488 0.626665    \nEmployment.l9   0.0035409  0.0021429   1.652 0.101786    \nconst          79.8647917 39.9369781   2.000 0.048411 *  \ntrend           0.9524398  0.5617195   1.696 0.093276 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 8.687 on 94 degrees of freedom\nMultiple R-Squared: 0.9959, Adjusted R-squared: 0.9947 \nF-statistic:   819 on 28 and 94 DF,  p-value: < 2.2e-16 \n\n\nEstimation results for equation GDP: \n==================================== \nGDP = stock_price.l1 + GDP.l1 + Employment.l1 + stock_price.l2 + GDP.l2 + Employment.l2 + stock_price.l3 + GDP.l3 + Employment.l3 + stock_price.l4 + GDP.l4 + Employment.l4 + stock_price.l5 + GDP.l5 + Employment.l5 + stock_price.l6 + GDP.l6 + Employment.l6 + stock_price.l7 + GDP.l7 + Employment.l7 + stock_price.l8 + GDP.l8 + Employment.l8 + stock_price.l9 + GDP.l9 + Employment.l9 + const + trend \n\n                 Estimate Std. Error t value Pr(>|t|)    \nstock_price.l1 -5.605e+00  1.777e+00  -3.154  0.00217 ** \nGDP.l1          1.818e+00  2.175e-01   8.360 5.58e-13 ***\nEmployment.l1  -1.497e-01  3.232e-02  -4.631 1.17e-05 ***\nstock_price.l2  1.352e+01  2.576e+00   5.250 9.43e-07 ***\nGDP.l2         -7.098e-01  3.300e-01  -2.151  0.03403 *  \nEmployment.l2   1.262e-01  5.211e-02   2.422  0.01733 *  \nstock_price.l3 -1.116e+00  2.506e+00  -0.445  0.65723    \nGDP.l3         -2.534e-01  3.318e-01  -0.764  0.44683    \nEmployment.l3   3.585e-02  5.455e-02   0.657  0.51263    \nstock_price.l4  1.673e+00  2.668e+00   0.627  0.53209    \nGDP.l4         -8.506e-02  3.412e-01  -0.249  0.80365    \nEmployment.l4   1.384e-02  5.558e-02   0.249  0.80383    \nstock_price.l5 -7.495e-01  3.188e+00  -0.235  0.81463    \nGDP.l5          1.171e-01  3.465e-01   0.338  0.73604    \nEmployment.l5  -2.497e-02  5.717e-02  -0.437  0.66327    \nstock_price.l6 -1.389e+01  3.294e+00  -4.216 5.71e-05 ***\nGDP.l6          2.705e-02  3.454e-01   0.078  0.93775    \nEmployment.l6  -9.451e-03  5.691e-02  -0.166  0.86846    \nstock_price.l7 -1.613e+00  3.812e+00  -0.423  0.67320    \nGDP.l7         -2.916e-01  3.429e-01  -0.850  0.39727    \nEmployment.l7   7.971e-02  5.567e-02   1.432  0.15552    \nstock_price.l8  7.701e+00  4.053e+00   1.900  0.06048 .  \nGDP.l8          9.947e-02  3.308e-01   0.301  0.76428    \nEmployment.l8  -1.132e-01  5.276e-02  -2.144  0.03457 *  \nstock_price.l9  1.073e+00  3.299e+00   0.325  0.74571    \nGDP.l9          1.156e-01  2.033e-01   0.569  0.57100    \nEmployment.l9   3.477e-02  3.341e-02   1.041  0.30064    \nconst           1.603e+03  6.226e+02   2.575  0.01159 *  \ntrend           2.134e+01  8.757e+00   2.437  0.01669 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 135.4 on 94 degrees of freedom\nMultiple R-Squared: 0.9995, Adjusted R-squared: 0.9993 \nF-statistic:  6211 on 28 and 94 DF,  p-value: < 2.2e-16 \n\n\nEstimation results for equation Employment: \n=========================================== \nEmployment = stock_price.l1 + GDP.l1 + Employment.l1 + stock_price.l2 + GDP.l2 + Employment.l2 + stock_price.l3 + GDP.l3 + Employment.l3 + stock_price.l4 + GDP.l4 + Employment.l4 + stock_price.l5 + GDP.l5 + Employment.l5 + stock_price.l6 + GDP.l6 + Employment.l6 + stock_price.l7 + GDP.l7 + Employment.l7 + stock_price.l8 + GDP.l8 + Employment.l8 + stock_price.l9 + GDP.l9 + Employment.l9 + const + trend \n\n                 Estimate Std. Error t value Pr(>|t|)    \nstock_price.l1 -6.801e+01  1.216e+01  -5.594 2.17e-07 ***\nGDP.l1          6.439e+00  1.488e+00   4.328 3.75e-05 ***\nEmployment.l1   1.372e-01  2.211e-01   0.621  0.53643    \nstock_price.l2  1.137e+02  1.762e+01   6.453 4.74e-09 ***\nGDP.l2         -4.116e+00  2.257e+00  -1.824  0.07136 .  \nEmployment.l2   5.335e-01  3.564e-01   1.497  0.13777    \nstock_price.l3 -4.159e+00  1.714e+01  -0.243  0.80883    \nGDP.l3         -3.838e+00  2.269e+00  -1.691  0.09408 .  \nEmployment.l3   5.003e-01  3.731e-01   1.341  0.18318    \nstock_price.l4  1.066e+01  1.825e+01   0.584  0.56038    \nGDP.l4         -6.501e-02  2.334e+00  -0.028  0.97783    \nEmployment.l4   1.168e-01  3.802e-01   0.307  0.75925    \nstock_price.l5  2.259e+01  2.180e+01   1.036  0.30277    \nGDP.l5          8.889e-01  2.370e+00   0.375  0.70845    \nEmployment.l5  -2.821e-01  3.911e-01  -0.721  0.47245    \nstock_price.l6 -1.130e+02  2.253e+01  -5.017 2.48e-06 ***\nGDP.l6         -1.567e+00  2.363e+00  -0.663  0.50891    \nEmployment.l6   2.161e-01  3.893e-01   0.555  0.58018    \nstock_price.l7 -2.069e+01  2.607e+01  -0.794  0.42931    \nGDP.l7         -1.096e+00  2.346e+00  -0.467  0.64133    \nEmployment.l7   4.753e-01  3.808e-01   1.248  0.21512    \nstock_price.l8  8.864e+01  2.772e+01   3.198  0.00189 ** \nGDP.l8          1.933e+00  2.262e+00   0.854  0.39514    \nEmployment.l8  -1.204e+00  3.609e-01  -3.336  0.00122 ** \nstock_price.l9 -3.272e+01  2.256e+01  -1.450  0.15042    \nGDP.l9          8.076e-01  1.390e+00   0.581  0.56273    \nEmployment.l9   4.222e-01  2.285e-01   1.848  0.06777 .  \nconst           1.262e+04  4.259e+03   2.963  0.00386 ** \ntrend           9.703e+01  5.990e+01   1.620  0.10859    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 926.3 on 94 degrees of freedom\nMultiple R-Squared: 0.9945, Adjusted R-squared: 0.9928 \nF-statistic: 603.2 on 28 and 94 DF,  p-value: < 2.2e-16 \n\n\n\nCovariance matrix of residuals:\n            stock_price      GDP Employment\nstock_price       75.47    290.5       2750\nGDP              290.52  18342.3     112144\nEmployment      2749.66 112143.6     858124\n\nCorrelation matrix of residuals:\n            stock_price    GDP Employment\nstock_price      1.0000 0.2469     0.3417\nGDP              0.2469 1.0000     0.8939\nEmployment       0.3417 0.8939     1.0000\n\n\n\n\nStep 4: Using Cross Validation\n\n\nShow the code\nn=length(dd$stock_price)\nk=39\n\n#n-k=92; 92/4=23;\n\nrmse1 <- matrix(NA, 96,3)\nrmse2 <- matrix(NA, 96,3)\nrmse3 <- matrix(NA,23,4)\nyear<-c()\n\n# Convert data frame to time series object\nts_obj <- ts(dd[, c(2:4)], star=decimal_date(as.Date(\"1990-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\n\nst <- tsp(ts_obj )[1]+(k-1)/4 \n\n\nfor(i in 1:23)\n{\n  \n  xtrain <- window(ts_obj, end=st + i-1)\n  xtest <- window(ts_obj, start=st + (i-1) + 1/4, end=st + i)\n  \n  \n  fit <- VAR(ts_obj, p=5, type='both')\n  fcast <- predict(fit, n.ahead = 4)\n  \n  fgdp<-fcast$fcst$GDP\n  femp<-fcast$fcst$Employment\n  fsp<-fcast$fcst$stock_price\n  ff<-data.frame(fsp[,1],fgdp[,1],femp[,1])\n  \n  year<-st + (i-1) + 1/4\n  \n  ff<-ts(ff,start=c(year,1),frequency = 4)\n  \n  a = 4*i-3\n  b= 4*i\n  rmse1[c(a:b),]  <-sqrt((ff-xtest)^2)\n  \n  fit2 <- VAR(ts_obj, p=9, type='both')\n  fcast2 <- predict(fit2, n.ahead = 4)\n  \n  fgdp<-fcast2$fcst$GDP\n  femp<-fcast2$fcst$Employment\n  fsp<-fcast2$fcst$stock_price\n  ff2<-data.frame(fsp[,1],fgdp[,1],femp[,1])\n  \n  year<-st + (i-1) + 1/4\n  \n  ff2<-ts(ff2,start=c(year,1),frequency = 4)\n  \n  a = 4*i-3\n  b= 4*i\n  rmse2[c(a:b),]  <-sqrt((ff2-xtest)^2)\n}\n\nyr = rep(c(1999:2022),each =4)\nqr = rep(paste0(\"Q\",1:4),24)\n\nrmse1 = data.frame(yr,qr,rmse1)\nnames(rmse1) =c(\"Year\", \"Quater\",\"Stock_Price\",\"GDP\",\"Employment\")\nrmse2 = data.frame(yr,qr,rmse2)\nnames(rmse2) =c(\"Year\", \"Quater\",\"Stock_Price\",\"GDP\",\"Employment\")\n\nggplot() + \n  geom_line(data = rmse1, aes(x = Year, y = Stock_Price),color = \"blue\") +\n  geom_line(data = rmse2, aes(x = Year, y = Stock_Price),color = \"red\") +\n  labs(\n    title = \"CV RMSE for Stock_Price\",\n    x = \"Date\",\n    y = \"RMSE\",\n    guides(colour=guide_legend(title=\"Fit\")))\n\n\n\n\n\n\n\nShow the code\nggplot() + \n  geom_line(data = rmse1, aes(x = Year, y = GDP),color = \"blue\") +\n  geom_line(data = rmse2, aes(x = Year, y = GDP),color = \"red\") +\n  labs(\n    title = \"CV RMSE for GDP\",\n    x = \"Date\",\n    y = \"RMSE\",\n    guides(colour=guide_legend(title=\"Fit\")))\n\n\nWarning: Removed 4 rows containing missing values (`geom_line()`).\nRemoved 4 rows containing missing values (`geom_line()`).\n\n\n\n\n\n\n\nShow the code\nggplot() + \n  geom_line(data = rmse1, aes(x = Year, y = Employment),color = \"blue\") +\n  geom_line(data = rmse2, aes(x = Year, y = Employment),color = \"red\") +\n  labs(\n    title = \"CV RMSE for Employment\",\n    x = \"Date\",\n    y = \"RMSE\",\n    guides(colour=guide_legend(title=\"Fit\")))\n\n\nWarning: Removed 4 rows containing missing values (`geom_line()`).\nRemoved 4 rows containing missing values (`geom_line()`).\n\n\n\n\n\nfit 1 is better\n\n\nStep 5: Forecast\n\n\nShow the code\nforecasts <- predict(VAR(dd[, c(2:4)], p=5, type='both'))\n\n# visualize the iterated forecasts\nplot(forecasts)"
  },
  {
    "objectID": "data_cleaning.html",
    "href": "data_cleaning.html",
    "title": "Data Cleaning",
    "section": "",
    "text": "Show the code\nvaccine <- read.csv('data/vaccine.csv', skip = 2)[c('Date','Total.Doses.Administered.Daily')]\ncolnames(vaccine)[2] <- 'total_doses'\nvaccine$Date <- as.Date(vaccine$Date, '%d/%m/%Y')\n\nwrite.csv(vaccine, 'data/vaccine_clean.csv', row.names = FALSE)\n\n\n\n\nShow the code\ncovid <- read.csv('data/weekly_covid.csv', skip = 2)[c('Date','Weekly.Cases')]\n\ncovid <- covid[order(nrow(covid):1),]\n#head(covid_df)\n\nnew_dates <- seq(as.Date('2020-1-29'), as.Date('2023-3-22'),'week')\ncovid$Date <- new_dates\n\nwrite.csv(covid, 'data/covid.csv', row.names = FALSE)"
  },
  {
    "objectID": "dsourse.html",
    "href": "dsourse.html",
    "title": "Data Source",
    "section": "",
    "text": "Healthcare Stock Price Data\nData Download\nThis dataset was downloaded from Yahoo Finance via Rstudio. It includes the adjusted close stock price for Pfizer, AstraZeneca, UnitedHealth Group Incorporated, Johnson & Johnson, CVS, and Stryker from 2000/2/2 to 2023/2/2.\n\n\n\n  \n\n\n\n\n  \n\n\n\n\n\nShow the code\nlibrary(flipbookr)\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(forecast)\nlibrary(astsa) \nlibrary(xts)\nlibrary(tseries)\nlibrary(fpp2)\nlibrary(fma)\nlibrary(lubridate)\nlibrary(tidyverse)\nlibrary(TSstudio)\nlibrary(quantmod)\nlibrary(tidyquant)\nlibrary(plotly)\nlibrary(ggplot2)\n\noptions(\"getSymbols.warning4.0\"=FALSE)\noptions(\"getSymbols.yahoo.warning\"=FALSE)\n\ntickers = c(\"PFE\",\"UNH\",\"JNJ\", \"AZN\", \"SYK\", \"CVS\",\"LLY\")\nfor (i in tickers){\n  getSymbols(i,\n             from = \"2010-1-1\",\n             to = \"2023-1-29\")}\n\nx <- list(\n  title = \"date\"\n)\ny <- list(\n  title = \"value\"\n)\n\nstock <- data.frame(PFE$PFE.Adjusted,\n                    UNH$UNH.Adjusted,\n                    AZN$AZN.Adjusted,\n                    SYK$SYK.Adjusted,\n                    CVS$CVS.Adjusted,\n                    JNJ$JNJ.Adjusted,\n                    LLY$LLY.Adjusted)\n\n\nstock <- data.frame(stock,rownames(stock))\ncolnames(stock) <- append(tickers,'Dates')\n\nstock$date<-as.Date(stock$Dates,\"%Y-%m-%d\")\nwrite.csv(stock, 'data/stock.csv', row.names = FALSE)\n#head(stock)\n\n\n\n\n\nScreenshot of Stock Prices\n\n\n\n\n\n\n\nNational Health Expenditure (NHE) Data\nData Download\nThis dataset contains yearly summary statistics of over 50 years of NHE data from various perspective, including amount in billions, per capita amount, and average annual percent change from previous year shown. The data set is available from CMS.gov.\n\n\n\nScreenshot of NHE Dataset"
  },
  {
    "objectID": "dv.html#bitcoin-plot-using-plotly",
    "href": "dv.html#bitcoin-plot-using-plotly",
    "title": "Data Visualizations",
    "section": "Bitcoin plot using plotly",
    "text": "Bitcoin plot using plotly\nOR you can obtain a single stock price\n\n\n           BTC.Open BTC.High BTC.Low BTC.Close BTC.Volume BTC.Adjusted\n2021-09-15  99.7099  99.7099 99.6600    99.660        102       99.660\n2021-09-16  99.5301  99.5800 99.5300    99.580        877       99.580\n2021-09-17  99.3900  99.4650 99.3900    99.465        502       99.465\n2021-09-20  99.4101  99.6250 99.4101    99.625        110       99.625\n2021-09-21  99.5878  99.5900 99.5878    99.590        464       99.590\n2021-09-22  99.5500  99.5500 99.5500    99.550        122       99.550\n\n\n[1] \"2021-09-15\"\n\n\n[1] \"2023-04-14\"\n\n\n\n\n           BTC.Open BTC.High BTC.Low BTC.Close BTC.Volume BTC.Adjusted\n2021-09-15  99.7099  99.7099 99.6600    99.660        102       99.660\n2021-09-16  99.5301  99.5800 99.5300    99.580        877       99.580\n2021-09-17  99.3900  99.4650 99.3900    99.465        502       99.465\n2021-09-20  99.4101  99.6250 99.4101    99.625        110       99.625\n2021-09-21  99.5878  99.5900 99.5878    99.590        464       99.590\n2021-09-22  99.5500  99.5500 99.5500    99.550        122       99.550\n           rownames.bitc.\n2021-09-15     2021-09-15\n2021-09-16     2021-09-16\n2021-09-17     2021-09-17\n2021-09-20     2021-09-20\n2021-09-21     2021-09-21\n2021-09-22     2021-09-22\n\n\n           BTC.Open BTC.High BTC.Low BTC.Close BTC.Volume BTC.Adjusted\n2021-09-15  99.7099  99.7099 99.6600    99.660        102       99.660\n2021-09-16  99.5301  99.5800 99.5300    99.580        877       99.580\n2021-09-17  99.3900  99.4650 99.3900    99.465        502       99.465\n2021-09-20  99.4101  99.6250 99.4101    99.625        110       99.625\n2021-09-21  99.5878  99.5900 99.5878    99.590        464       99.590\n2021-09-22  99.5500  99.5500 99.5500    99.550        122       99.550\n                 date\n2021-09-15 2021-09-15\n2021-09-16 2021-09-16\n2021-09-17 2021-09-17\n2021-09-20 2021-09-20\n2021-09-21 2021-09-21\n2021-09-22 2021-09-22\n\n\n'data.frame':   398 obs. of  7 variables:\n $ BTC.Open    : num  99.7 99.5 99.4 99.4 99.6 ...\n $ BTC.High    : num  99.7 99.6 99.5 99.6 99.6 ...\n $ BTC.Low     : num  99.7 99.5 99.4 99.4 99.6 ...\n $ BTC.Close   : num  99.7 99.6 99.5 99.6 99.6 ...\n $ BTC.Volume  : num  102 877 502 110 464 ...\n $ BTC.Adjusted: num  99.7 99.6 99.5 99.6 99.6 ...\n $ date        : Date, format: \"2021-09-15\" \"2021-09-16\" ...\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlot the climate data (climate.csv) using plotly.\n\nhttps://plotly.com/r/\nThis graph shows the daily max temperature from January 1st 2021 to September 30th 2021 from national arboretum DC. It shows both seasonality and upward trend. The max temperature rises because it starts from winter and ends at fall. Another interesting finding is that the max temperature shows flucuation around every 2 weeks,\n\nMake only the plots visible in your webpage. (set echo=FALSE in your R code chunck)\nAdd interpretations to all the plots in the webpage.\n\nSo now you will only have the plots and the interpretation in the webpage. You can add titles or can be creative about the page as you want.\n\nUse a different theme than mine. More themes can be found here. https://quarto.org/docs/output-formats/html-themes.html\nAdd this to your Georgetown domain with the title “Data Vizes in TS”. And submit the URL for the Lab 0 assignment.\n\nHowever please remember to take it down at the end of the semester if you don’t need that page on your website.\n\nProfessor James will demonstrate how to push your website to GU domains from your local laptops."
  },
  {
    "objectID": "dvis.html",
    "href": "dvis.html",
    "title": "Data Visualization",
    "section": "",
    "text": "US Stock Prices of Healthcare Companies since 2010\nThe interactive plot below have shown the stock price of healthcare companies over last 12 years and Moderna (MRNA) since 2019. Overall, the stock prices of healthcare companies has been grow over the period of time. The stock prices of United Health Group (UNH), AstraZeneca (ANZ), Eli Lilly And Co (LLY), and CVS have shown more prevailingly rising trend, especially UNH, it surpassed the rest of healthcare companies and have been ranked the highest stock price since 2015. While Pfizer (PFE) has comparatively small increase in stock price over the period of time. The stock price of MRNA surged in COVID-19, peaked at Augest 2021 and then plummeted. Apart from MRNA, every company has a steep drop of stock price in March 2020, which was closely related to fears of COVID-19 spreading across the country and the global community.\n\n\nShow the code\nstock_df <- read.csv('data/stock.csv')\nstock_df$Dates <- as.Date(stock_df$Dates)\n\ng1<- ggplot(stock_df, aes(x=Dates)) +\n  geom_line(aes(y=PFE, colour=\"PFE\"))+\n  geom_line(aes(y=UNH, colour=\"UNH\"))+\n  geom_line(aes(y=AZN, colour=\"AZN\"))+\n  geom_line(aes(y=CVS, colour=\"CVS\"))+\n  geom_line(aes(y=LLY, colour=\"LLY\"))+\n  geom_line(data = mrna_df, aes(y=MRNA.Adjusted, colour=\"MRNA\"))+\n   labs(\n    title = \"Stock Prices for the Healthcare Companies Since 2010\",\n    subtitle = \"From 2010-2023\",\n    x = \"Date\",\n    y = \"Adjusted Closing Prices\")+\n    guides(colour=guide_legend(title=\"Healthcare Companies\")) \n\n\n\nggplotly(g1) %>% layout(hovermode = \"x\")\n\n\n\n\n\n\n\n\nUS Stock Prices of Healthcare Companies Since COVID-19 Pandemic\n\n\nShow the code\ncovid_stock <- filter(stock_df,Dates>\"2020-01-05\")\ncovid_mrna <- filter(mrna_df,Dates>\"2020-01-05\")\n\ng2<- ggplot(covid_stock, aes(x=Dates)) +\n  geom_line(aes(y=PFE, colour=\"PFE\"))+\n  geom_line(aes(y=UNH, colour=\"UNH\"))+\n  geom_line(aes(y=AZN, colour=\"AZN\"))+\n  geom_line(aes(y=CVS, colour=\"CVS\"))+\n  geom_line(aes(y=LLY, colour=\"LLY\"))+\n  geom_line(data = covid_mrna, aes(y=MRNA.Adjusted, colour=\"MRNA\"))+\n   labs(\n    title = \"Stock Prices for the Healthcare Companies Since COVID-19 Pandemic\",\n    subtitle = \"From 2020-2023\",\n    x = \"Date\",\n    y = \"Adjusted Closing Prices\")+\n    guides(colour=guide_legend(title=\"Healthcare Companies\")) \n\nggplotly(g2) %>% layout(hovermode = \"x\")\n\n\n\n\n\n\nLike many other stocks, healthcare stocks experienced a decline in their stock prices in March 2020, when the pandemic began to spread rapidly across the United States. This was due to investor concerns about the potential economic impact of the pandemic, as well as uncertainties around the healthcare industry’s response to the crisis. Despite the initial decline, healthcare stocks have shown resilience and have generally performed well during the pandemic. This is likely due to the essential nature of healthcare services, as well as the industry’s strong financial position and diverse business models.\nAmong these stocks, MRNA shows a different trend to rest of the stocks with a big surge in 2021 and plummeted at 2022, and followed by a slight upward trend. Moderna is a biotechnology company that specializes in developing mRNA-based therapeutics and vaccines. The COVID-19 pandemic has had a significant impact on Moderna’s business, as the company has been at the forefront of developing a vaccine to combat the virus.\nOverall, the pandemic has also led to increased demand for healthcare services and products, which has boosted the stock prices of companies that provide these services.\n\n\nCOVID-19 Weekly Case Number in US\n\n\nShow the code\ng3 <- ggplot(data = covid_df, aes(x=Dates, y = Weekly.Cases)) +\n  geom_line()+\n  labs(title = 'COVID-19 Weekly Case Number in US')\n\nggplotly(g3) %>% layout(hovermode = \"x\")\n\n\n\n\n\n\n\n\nDaily COVID-19 Vaccination Number in US\n\n\nShow the code\nvaccine_df <- read.csv('data/vaccine_clean.csv')\nvaccine_df$Date <- as.Date(vaccine_df$Date)\n#vaccine_df\ng4 <- ggplot(data = vaccine_df, aes(x=Date, y = total_doses)) +\n  geom_line()+\n  labs(title = 'Daily COVID-19 Vaccination Number in US')\n\nggplotly(g4) %>% layout(hovermode = \"x\")\n\n\n\n\n\n\nThe time series of daily COVID-19 vaccination number in US shows an increasing trend in the beginning as more people become eligible for the vaccine and supply chains are established, peaked at 2021 April and then shows a decreasing trend. The data shows a cyclic pattern weekly, it reached the bottom at weekend and bounced up during the weekday. The trend of time series data roughly matches with the trend of MRNA."
  },
  {
    "objectID": "eda.html",
    "href": "eda.html",
    "title": "EDA of United Health Group",
    "section": "",
    "text": "To go further, exploration of the components of the stock price time series is necessary.\n\nBasic Time Series Plot\nFirstly, it is important to visualize the time series at its most basic level, which is an interactive candlestick plot of daily stock price data over time. When hover over the data on the plot, it tells the information about open price, close price, highest price and the lowest price. Moving the bar at the bottom can select the desired time period.\n\n\nShow the code\n# candlestick plot\n\n#UNH_df <- as.data.frame(tail(UNH, 700))\nUNH_df <- as.data.frame(UNH)\nUNH_df$Dates <- as.Date(rownames(UNH_df))\n\nfig_UNH <- UNH_df %>% plot_ly(x = ~Dates, type=\"candlestick\",\n          open = ~UNH.Open, close = ~UNH.Close,\n          high = ~UNH.High, low = ~UNH.Low) \nfig_UNH <- fig_UNH %>% \n  layout(title = \"Basic Candlestick Chart for UnitedHealth Group\")\n\nfig_UNH\n\n\n\n\n\n\nThe plot shows the UNH stock price from 2010 to 2023, and a couple of important observations can be made from the plot. First, there is a clear upward trend in the data, and data is not stationary. This can be confirmed using the plot of the decomposition of the time series plot. The UNH stock price rose slowly until 2016, and then has accelerated. Additionally, there is no seasonality existed within the data, while the series has some cyclic movement since 2018. Finally, this time series appears relatively more multiplicative than additive. It appears to be an exponential increase in amplitudes over time. UnitedHealth Group (UNH) is one of the largest healthcare companies in the world, with a market capitalization of over $450 billion as of April 2023. The company’s stock price is also one of the highest in the healthcare industry.\n\n\nLag plot\nA lag plot is a type of scatter plot where time series are plotted in pairs against itself some time units behind or ahead, which helps evaluate whether there is randomness or an indication of autocorrelation in the data. Below are sets of lag plots for the UNH stock price data in 12 consecutive days.\n\n\nShow the code\nUNH_ts <- ts(stock_df$UNH, start = c(2010,1),end = c(2023,1),\n             frequency = 251)\n\nts_lags(UNH_ts)\n\n\n\n\n\n\nAccording to the plots, you can see high correlation among consecutive observations. The plot with smaller number of lag shows a more narrow distribution of dots near diagonal, thus, stronger autocorrelation, which indicated the data has stronger autocorrelation with the data closer to observation date. This plot suggests an Auto Regressive model will be more appropriate for future modelling fitting.\n\n\nDecomposed times series\nIn order to truly break down the time series data into its core components, decomposition must be run. When decomposing the time series, the four principal components are extracted, including observed data, seasonality, trend, and noise/randomness. The plot below presents the decomposed time series data for UNH stock price. In this plot, the seasonality, trend, and noise are seen. The noise is also known as the remainder in this plot. There is seasonality showing on the decomposed plots, while it is hard to observe from the observed data.\n\n\nShow the code\ndecompose_UNH <- decompose(UNH_ts,'multiplicative')\nautoplot(decompose_UNH)\n\n\n\n\n\n\n\nAutocorrelation in Time Series\nAn essential piece of information when it comes to analyzing time series data is to determine whether a time series is stationary or not. One way to make that determination is by viewing the ACF and PACF plots. The ACF (autocorrelation function) plot is a visualization of correlations between a time series and its lags. In contrast, the PACF (partial autocorrelation function) plot visualizes the partial correlation coefficients and its lags, specifically, it shows correlations of the residuals after removing the effects explained by earlier lags. Below are visualizations of each of the plots described above.\n\n\nShow the code\nUNH_acf <- ggAcf(UNH_ts,100)+ggtitle(\"UNH ACF Plot\")\n\nUNH_pacf <- ggPacf(UNH_ts)+ggtitle(\"PACF Plot for UHNs\")\ngrid.arrange(UNH_acf, UNH_pacf,nrow=2)\n\n\n\n\n\nThe ACF plots indicates strong positive correlation among time series data, thus, the series is not stationary. The PACF plot doesn’t show any significant value.\n\n\nAugmented Dickey-Fuller Test\n\n\nShow the code\n################ ADF Test #############\ntseries::adf.test(UNH_ts)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  UNH_ts\nDickey-Fuller = -1.0864, Lag order = 14, p-value = 0.9248\nalternative hypothesis: stationary\n\n\nApart from observing both ACF and PACF plots, Augmented Dickey-Fuller Test can also be applied to check the stationarity. The null hypothesis is: Data has unit roots; hence the series is not stationary. The result of this test has significant value 0.9248, which is much larger than the 0.05 significant value. Hence, here doesn’t have enough evidence to reject null hypothesis, and it confirms with plots that the series is non-stationary.\n\n\nDetrending & Differencing\nIn general, it is necessary for time series data to be stationary. Detrending and differencing are two methods that play down the effects of non-stationary so the stationary properties of the series.\n\n\nShow the code\nfit = lm(UNH_ts~time(UNH_ts), na.action=NULL) \n\ny= UNH_ts\nx=time(UNH_ts)\nDD<-data.frame(x,y)\nggp <- ggplot(DD, aes(x, y)) +           \n  geom_line()\n\nggp <- ggp +                                     \n  stat_smooth(method = \"lm\",\n              formula = y ~ x,\n              geom = \"smooth\") +ggtitle(\"UNH Stock Price\")+ylab(\"Price\")\n\nplot1<-ggAcf(resid(fit),30, main=\"ACF Plot for Detrended Data\") \nplot2<-ggAcf(diff(UNH_ts),30, main=\"ACF Plot for First Differenced Data\") \nplot3<-ggAcf(diff(log(UNH_ts)),30, main=\"ACF Plot for Log Transformed First differenced Data\") \n\n\ngrid.arrange(ggp, plot1, plot2,plot3,nrow=4)\n\n\n\n\n\nThe graphs above shows the procedures of converting stationary data series into non-stationary. The ACF plot for log transformed first differenced data series are closest to stationary.\n\n\nShow the code\ntseries::adf.test(diff(log(UNH_ts)))\n\n\nWarning in tseries::adf.test(diff(log(UNH_ts))): p-value smaller than printed\np-value\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  diff(log(UNH_ts))\nDickey-Fuller = -16.952, Lag order = 14, p-value = 0.01\nalternative hypothesis: stationary\n\n\n\n\nMoving Average Smoothing\nSmoothing methods are a family of forecasting methods that average values over multiple periods in order to reduce the noise and uncover patterns in the data. It is useful as a data preparation technique as it can reduce the random variation in the observations and better expose the structure of the underlying causal processes. We call this an m-MA, meaning a moving average of order m.\n\n\nShow the code\nMA_7 <- autoplot(UNH_ts, series=\"Data\") +\n        autolayer(ma(UNH_ts,7), series=\"7-MA\") +\n        xlab(\"Year\") + ylab(\"Adjusted Closing Price\") +\n        ggtitle(\"UNH Stock Price Trend in (7-days Moving Average)\") +\n        scale_colour_manual(values=c(\"UNH_ts\"=\"grey50\",\"7-MA\"=\"red\"),\n                            breaks=c(\"UNH_ts\",\"7-MA\"))\n\nMA_30 <- autoplot(UNH_ts, series=\"Data\") +\n        autolayer(ma(UNH_ts,30), series=\"30-MA\") +\n        xlab(\"Year\") + ylab(\"Adjusted Closing Price\") +\n        ggtitle(\"UNH Stock Price Trend in (30-days Moving Average)\") +\n        scale_colour_manual(values=c(\"UNH_ts\"=\"grey50\",\"30-MA\"=\"red\"),\n                            breaks=c(\"UNH_ts\",\"30-MA\"))\n\nMA_251 <- autoplot(UNH_ts, series=\"Data\") +\n        autolayer(ma(UNH_ts,251), series=\"251-MA\") +\n        xlab(\"Year\") + ylab(\"Adjusted Closing Price\") +\n        ggtitle(\"UNH Stock Price Trend in (251-days Moving Average)\") +\n        scale_colour_manual(values=c(\"UNH_ts\"=\"grey50\",\"251-MA\"=\"red\"),\n                            breaks=c(\"UNH_ts\",\"251-MA\"))\n\ngrid.arrange(MA_7, MA_30, MA_251, ncol=1)\n\n\n\n\n\nThe graph above shows the moving average of 7 days, 30 days and 251 days. 251 days was choose because there are around 251 days of stock price data per year. According to the plots, it can be observed that When MA is very large(MA=251), some parts of smoothing line(red) do not fit the real stock price line. While When MA is small(MA=7), the smoothing line(red) fits the real price line. MA-30 greatly fits the real price line. Therefore, MA-30 might be a good parameter for smoothing."
  },
  {
    "objectID": "eda_azn.html",
    "href": "eda_azn.html",
    "title": "EDA of AstraZeneca",
    "section": "",
    "text": "Refer to EDA-AZN for more detailed description for each plot.\n\nBasic Time Series Plot\nFirstly, it is important to visualize the time series at its most basic level, which is an interactive candlestick plot of daily stock price data over time. When hover over the data on the plot, it tells the information about open price, close price, highest price and the lowest price. Moving the bar at the bottom can select the desired time period.\n\n\nShow the code\n# candlestick plot\n\nAZN_df <- as.data.frame(AZN)\nAZN_df$Dates <- as.Date(rownames(AZN_df))\n\nfig_AZN <- AZN_df %>% plot_ly(x = ~Dates, type=\"candlestick\",\n          open = ~AZN.Open, close = ~AZN.Close,\n          high = ~AZN.High, low = ~AZN.Low) \nfig_AZN <- fig_AZN %>% \n  layout(title = \"Basic Candlestick Chart for AstraZeneca\")\n\nfig_AZN\n\n\n\n\n\n\n\n\nLag plot\nA lag plot is a type of scatter plot where time series are plotted in pairs against itself some time units behind or ahead, which helps evaluate whether there is randomness or an indication of autocorrelation in the data. Below are sets of lag plots for the UNH stock price data in 12 consecutive days.\n\n\nShow the code\nAZN_ts <- ts(stock_df$AZN, start = c(2010,1), end = c(2023,1), \n             frequency = 251)\n\nts_lags(AZN_ts)\n\n\n\n\n\n\nAccording to the plots, you can see high correlation among consecutive observations. The plot with smaller number of lag shows a more narrow distribution of dots near diagonal, thus, stronger autocorrelation, which indicated the data has stronger autocorrelation with the data closer to observation date. This plot suggests an Auto Regressive model will be more appropriate for future modelling fitting.\n\n\nDecomposed times series\n\n\nShow the code\ndecompose_AZN <- decompose(AZN_ts,'multiplicative')\nautoplot(decompose_AZN)\n\n\n\n\n\n\n\nAutocorrelation in Time Series\n\n\nShow the code\nAZN_acf <- ggAcf(AZN_ts,100)+ggtitle(\"ACF Plot for AZN\")\nAZN_pacf <- ggPacf(AZN_ts,100)+ggtitle(\"PACF Plot for AZN\")\n\ngrid.arrange(AZN_acf, AZN_pacf,nrow=2)\n\n\n\n\n\n\n\nAugmented Dickey-Fuller Test\n\n\nShow the code\ntseries::adf.test(AZN_ts)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  AZN_ts\nDickey-Fuller = -3.3602, Lag order = 14, p-value = 0.06026\nalternative hypothesis: stationary\n\n\n\n\nMoving Average Smoothing\nSmoothing methods are a family of forecasting methods that average values over multiple periods in order to reduce the noise and uncover patterns in the data. It is useful as a data preparation technique as it can reduce the random variation in the observations and better expose the structure of the underlying causal processes. We call this an m-MA, meaning a moving average of order m.\n\n\nShow the code\nMA_7 <- autoplot(AZN_ts, series=\"Data\") +\n        autolayer(ma(AZN_ts,7), series=\"7-MA\") +\n        xlab(\"Year\") + ylab(\"Adjusted Closing Price\") +\n        ggtitle(\"AZN Stock Price Trend in (7-days Moving Average)\") +\n        scale_colour_manual(values=c(\"AZN_ts\"=\"grey50\",\"7-MA\"=\"red\"),\n                            breaks=c(\"AZN_ts\",\"7-MA\"))\n\nMA_30 <- autoplot(AZN_ts, series=\"Data\") +\n        autolayer(ma(AZN_ts,30), series=\"30-MA\") +\n        xlab(\"Year\") + ylab(\"Adjusted Closing Price\") +\n        ggtitle(\"AZN Stock Price Trend in (30-days Moving Average)\") +\n        scale_colour_manual(values=c(\"AZN_ts\"=\"grey50\",\"30-MA\"=\"red\"),\n                            breaks=c(\"AZN_ts\",\"30-MA\"))\n\nMA_251 <- autoplot(AZN_ts, series=\"Data\") +\n        autolayer(ma(AZN_ts,251), series=\"251-MA\") +\n        xlab(\"Year\") + ylab(\"Adjusted Closing Price\") +\n        ggtitle(\"AZN Stock Price Trend in (251-days Moving Average)\") +\n        scale_colour_manual(values=c(\"AZN_ts\"=\"grey50\",\"251-MA\"=\"red\"),\n                            breaks=c(\"AZN_ts\",\"251-MA\"))\n\ngrid.arrange(MA_7, MA_30, MA_251, ncol=1)\n\n\n\n\n\nThe graph above shows the moving average of 7 days, 30 days and 251 days. 251 days was choose because there are around 251 days of stock price data per year. According to the plots, it can be observed that When MA is very large(MA=251), some parts of smoothing line(red) do not fit the real stock price line. While When MA is small(MA=7), the smoothing line(red) fits the real price line. MA-30 greatly fits the real price line. Therefore, MA-30 might be a good parameter for smoothing."
  },
  {
    "objectID": "eda_cvs.html",
    "href": "eda_cvs.html",
    "title": "EDA of CVS",
    "section": "",
    "text": "Refer to EDA-UNH for more detailed description for each plot.\n\nBasic Time Series Plot\n\n\nShow the code\n# candlestick plot\n\nCVS_df <- as.data.frame(CVS)\nCVS_df$Dates <- as.Date(rownames(CVS_df))\n\nfig_CVS <- CVS_df %>% plot_ly(x = ~Dates, type=\"candlestick\",\n          open = ~CVS.Open, close = ~CVS.Close,\n          high = ~CVS.High, low = ~CVS.Low) \nfig_CVS <- fig_CVS %>% \n  layout(title = \"Basic Candlestick Chart for CVS\")\n\nfig_CVS\n\n\n\n\n\n\n\n\nLag plot\n\n\nShow the code\nCVS_ts <- ts(stock_df$CVS, start = c(2010,1),end = c(2023,1),\n             frequency = 251)\nts_lags(CVS_ts)\n\n\n\n\n\n\n\n\nDecomposed times series\n\n\nShow the code\ndecompose_CVS <- decompose(CVS_ts,'additive')\nautoplot(decompose_CVS)\n\n\n\n\n\n\n\nAutocorrelation in Time Series\n\n\nShow the code\nCVS_acf <- ggAcf(CVS_ts,100)+ggtitle(\"ACF Plot for CVS\")\nCVS_pacf<- ggPacf(CVS_ts)+ggtitle(\"PACF Plot for CVS\")\n\ngrid.arrange(CVS_acf, CVS_pacf,nrow=2)\n\n\n\n\n\n\n\nAugmented Dickey-Fuller Test\n\n\nShow the code\ntseries::adf.test(CVS_ts)\n\n\nWarning in tseries::adf.test(CVS_ts): p-value smaller than printed p-value\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  CVS_ts\nDickey-Fuller = -4.5012, Lag order = 14, p-value = 0.01\nalternative hypothesis: stationary\n\n\n\n\nDetrending\n\n\nShow the code\nfit = lm(CVS_ts~time(CVS_ts), na.action=NULL) \n\ny= CVS_ts\nx=time(CVS_ts)\nDD<-data.frame(x,y)\nggp <- ggplot(DD, aes(x, y)) +           \n  geom_line()\n\nggp <- ggp +                                     \n  stat_smooth(method = \"lm\",\n              formula = y ~ x,\n              geom = \"smooth\") +ggtitle(\"CVS Stock Price\")+ylab(\"Price\")\n\nplot1<-autoplot(resid(fit), main=\"detrended\") \nplot2<-autoplot(diff(CVS_ts), main=\"first difference\") \n\n\ngrid.arrange(ggp, plot1, plot2,nrow=3)\n\n\nDon't know how to automatically pick scale for object of type <ts>. Defaulting\nto continuous.\nDon't know how to automatically pick scale for object of type <ts>. Defaulting\nto continuous.\n\n\n\n\n\n\n\nMoving Average Smoothing\nSmoothing methods are a family of forecasting methods that average values over multiple periods in order to reduce the noise and uncover patterns in the data. It is useful as a data preparation technique as it can reduce the random variation in the observations and better expose the structure of the underlying causal processes. We call this an m-MA, meaning a moving average of order m.\n\n\nShow the code\nMA_7 <- autoplot(CVS_ts, series=\"Data\") +\n        autolayer(ma(CVS_ts,7), series=\"7-MA\") +\n        xlab(\"Year\") + ylab(\"Adjusted Closing Price\") +\n        ggtitle(\"CVS Stock Price Trend in (7-days Moving Average)\") +\n        scale_colour_manual(values=c(\"CVS_ts\"=\"grey50\",\"7-MA\"=\"red\"),\n                            breaks=c(\"CVS_ts\",\"7-MA\"))\n\nMA_30 <- autoplot(CVS_ts, series=\"Data\") +\n        autolayer(ma(CVS_ts,30), series=\"30-MA\") +\n        xlab(\"Year\") + ylab(\"Adjusted Closing Price\") +\n        ggtitle(\"CVS Stock Price Trend in (30-days Moving Average)\") +\n        scale_colour_manual(values=c(\"CVS_ts\"=\"grey50\",\"30-MA\"=\"red\"),\n                            breaks=c(\"CVS_ts\",\"30-MA\"))\n\nMA_251 <- autoplot(CVS_ts, series=\"Data\") +\n        autolayer(ma(CVS_ts,251), series=\"251-MA\") +\n        xlab(\"Year\") + ylab(\"Adjusted Closing Price\") +\n        ggtitle(\"CVS Stock Price Trend in (251-days Moving Average)\") +\n        scale_colour_manual(values=c(\"CVS_ts\"=\"grey50\",\"251-MA\"=\"red\"),\n                            breaks=c(\"CVS_ts\",\"251-MA\"))\n\ngrid.arrange(MA_7, MA_30, MA_251, ncol=1)\n\n\n\n\n\nThe graph above shows the moving average of 7 days, 30 days and 251 days. 251 days was choose because there are around 251 days of stock price data per year. According to the plots, it can be observed that When MA is very large(MA=251), some parts of smoothing line(red) do not fit the real stock price line. While When MA is small(MA=7), the smoothing line(red) fits the real price line. MA-30 greatly fits the real price line. Therefore, MA-30 might be a good parameter for smoothing."
  },
  {
    "objectID": "eda_jnj.html",
    "href": "eda_jnj.html",
    "title": "EDA of JNJ",
    "section": "",
    "text": "Refer to EDA-UNH for more detailed description for each plot.\n\nBasic Time Series Plot\n\n\nShow the code\n# candlestick plot\n\nJNJ_df <- as.data.frame(JNJ)\nJNJ_df$Dates <- as.Date(rownames(JNJ_df))\n\nfig_JNJ <- JNJ_df %>% plot_ly(x = ~Dates, type=\"candlestick\",\n          open = ~JNJ.Open, close = ~JNJ.Close,\n          high = ~JNJ.High, low = ~JNJ.Low) \nfig_JNJ <- fig_JNJ %>% \n  layout(title = \"Basic Candlestick Chart for Johnson & Johnson\")\n\nfig_JNJ\n\n\n\n\n\n\n\n\nLag plot\n\n\nShow the code\nJNJ_ts <- ts(stock_df$JNJ, start = c(2010,1),end = c(2023,1),\n             frequency = 251)\nts_lags(JNJ_ts)\n\n\n\n\n\n\n\n\nDecomposed times series\n\n\nShow the code\ndecompose_JNJ <- decompose(JNJ_ts,'additive')\nautoplot(decompose_JNJ)\n\n\n\n\n\n\n\nAutocorrelation in Time Series\n\n\nShow the code\nJNJ_acf <- ggAcf(JNJ_ts,100)+ggtitle(\"ACF Plot for JNJ\")\nJNJ_pacf <- ggPacf(JNJ_ts)+ggtitle(\"PACF Plot for JNJ\")\n\ngrid.arrange(JNJ_acf, JNJ_pacf,nrow=2)\n\n\n\n\n\n\n\nAugmented Dickey-Fuller Test\n\n\nShow the code\ntseries::adf.test(JNJ_ts)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  JNJ_ts\nDickey-Fuller = -2.4264, Lag order = 14, p-value = 0.3978\nalternative hypothesis: stationary\n\n\n\n\nDetrending\n\n\nShow the code\nfit = lm(JNJ_ts~time(JNJ_ts), na.action=NULL) \n\ny= JNJ_ts\nx=time(JNJ_ts)\nDD<-data.frame(x,y)\nggp <- ggplot(DD, aes(x, y)) +           \n  geom_line()\n\nggp <- ggp +                                     \n  stat_smooth(method = \"lm\",\n              formula = y ~ x,\n              geom = \"smooth\") +ggtitle(\"JNJ Stock Price\")+ylab(\"Price\")\n\nplot1<-autoplot(resid(fit), main=\"detrended\") \nplot2<-autoplot(diff(JNJ_ts), main=\"first difference\") \n\n\ngrid.arrange(ggp, plot1, plot2,nrow=3)\n\n\nDon't know how to automatically pick scale for object of type <ts>. Defaulting\nto continuous.\nDon't know how to automatically pick scale for object of type <ts>. Defaulting\nto continuous.\n\n\n\n\n\n\n\nMoving Average Smoothing\nSmoothing methods are a family of forecasting methods that average values over multiple periods in order to reduce the noise and uncover patterns in the data. It is useful as a data preparation technique as it can reduce the random variation in the observations and better expose the structure of the underlying causal processes. We call this an m-MA, meaning a moving average of order m.\n\n\nShow the code\nMA_7 <- autoplot(JNJ_ts, series=\"Data\") +\n        autolayer(ma(JNJ_ts,7), series=\"7-MA\") +\n        xlab(\"Year\") + ylab(\"Adjusted Closing Price\") +\n        ggtitle(\"JNJ Stock Price Trend in (7-days Moving Average)\") +\n        scale_colour_manual(values=c(\"JNJ_ts\"=\"grey50\",\"7-MA\"=\"red\"),\n                            breaks=c(\"JNJ_ts\",\"7-MA\"))\n\nMA_30 <- autoplot(JNJ_ts, series=\"Data\") +\n        autolayer(ma(JNJ_ts,30), series=\"30-MA\") +\n        xlab(\"Year\") + ylab(\"Adjusted Closing Price\") +\n        ggtitle(\"JNJ Stock Price Trend in (30-days Moving Average)\") +\n        scale_colour_manual(values=c(\"JNJ_ts\"=\"grey50\",\"30-MA\"=\"red\"),\n                            breaks=c(\"JNJ_ts\",\"30-MA\"))\n\nMA_251 <- autoplot(JNJ_ts, series=\"Data\") +\n        autolayer(ma(JNJ_ts,251), series=\"251-MA\") +\n        xlab(\"Year\") + ylab(\"Adjusted Closing Price\") +\n        ggtitle(\"JNJ Stock Price Trend in (251-days Moving Average)\") +\n        scale_colour_manual(values=c(\"JNJ_ts\"=\"grey50\",\"251-MA\"=\"red\"),\n                            breaks=c(\"JNJ_ts\",\"251-MA\"))\n\ngrid.arrange(MA_7, MA_30, MA_251, ncol=1)\n\n\n\n\n\nThe graph above shows the moving average of 7 days, 30 days and 251 days. 251 days was choose because there are around 251 days of stock price data per year. According to the plots, it can be observed that When MA is very large(MA=251), some parts of smoothing line(red) do not fit the real stock price line. While When MA is small(MA=7), the smoothing line(red) fits the real price line. MA-30 greatly fits the real price line. Therefore, MA-30 might be a good parameter for smoothing."
  },
  {
    "objectID": "eda_lly.html",
    "href": "eda_lly.html",
    "title": "EDA of Eli Lilly And Co",
    "section": "",
    "text": "Basic Time Series Plot\n\n\nShow the code\n# candlestick plot\n\nLLY_df <- as.data.frame(LLY)\nLLY_df$Dates <- as.Date(rownames(LLY_df))\n\nfig_LLY <- LLY_df %>% plot_ly(x = ~Dates, type=\"candlestick\",\n          open = ~LLY.Open, close = ~LLY.Close,\n          high = ~LLY.High, low = ~LLY.Low) \nfig_LLY <- fig_LLY %>% \n  layout(title = \"Basic Candlestick Chart for Eli Lilly and Company\")\n\nfig_LLY\n\n\n\n\n\n\n\n\nLag plot\n\n\nShow the code\nLLY_ts <- ts(stock_df$LLY, start = c(2010,1),end = c(2023,1),\n             frequency = 251)\nts_lags(LLY_ts)\n\n\n\n\n\n\n\n\nDecomposed times series\n\n\nShow the code\ndecompose_LLY <- decompose(LLY_ts,'multiplicative')\nautoplot(decompose_LLY)\n\n\n\n\n\n\n\nAutocorrelation in Time Series\n\n\nShow the code\nLLY_acf <- ggAcf(LLY_ts,100)+ggtitle(\"ACF Plot for LLY\")\nLLY_pacf <- ggPacf(LLY_ts)+ggtitle(\"PACF Plot for LLY\")\n\ngrid.arrange(LLY_acf, LLY_pacf,nrow=2)\n\n\n\n\n\n\n\nAugmented Dickey-Fuller Test\n\n\nShow the code\ntseries::adf.test(LLY_ts)\n\n\nWarning in tseries::adf.test(LLY_ts): p-value greater than printed p-value\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  LLY_ts\nDickey-Fuller = 0.80469, Lag order = 14, p-value = 0.99\nalternative hypothesis: stationary\n\n\n\n\nDetrending\n\n\nShow the code\nfit = lm(LLY_ts~time(LLY_ts), na.action=NULL) \n\ny= LLY_ts\nx=time(LLY_ts)\nDD<-data.frame(x,y)\nggp <- ggplot(DD, aes(x, y)) +           \n  geom_line()\n\nggp <- ggp +                                     \n  stat_smooth(method = \"lm\",\n              formula = y ~ x,\n              geom = \"smooth\") +ggtitle(\"LLY Stock Price\")+ylab(\"Price\")\n\nplot1<-autoplot(resid(fit), main=\"detrended\") \nplot2<-autoplot(diff(LLY_ts), main=\"first difference\") \n\n\ngrid.arrange(ggp, plot1, plot2,nrow=3)\n\n\nDon't know how to automatically pick scale for object of type <ts>. Defaulting\nto continuous.\nDon't know how to automatically pick scale for object of type <ts>. Defaulting\nto continuous.\n\n\n\n\n\n\n\nMoving Average Smoothing\nSmoothing methods are a family of forecasting methods that average values over multiple periods in order to reduce the noise and uncover patterns in the data. It is useful as a data preparation technique as it can reduce the random variation in the observations and better expose the structure of the underlying causal processes. We call this an m-MA, meaning a moving average of order m.\n\n\nShow the code\nMA_7 <- autoplot(LLY_ts, series=\"Data\") +\n        autolayer(ma(LLY_ts,7), series=\"7-MA\") +\n        xlab(\"Year\") + ylab(\"Adjusted Closing Price\") +\n        ggtitle(\"LLY Stock Price Trend in (7-days Moving Average)\") +\n        scale_colour_manual(values=c(\"LLY_ts\"=\"grey50\",\"7-MA\"=\"red\"),\n                            breaks=c(\"LLY_ts\",\"7-MA\"))\n\nMA_30 <- autoplot(LLY_ts, series=\"Data\") +\n        autolayer(ma(LLY_ts,30), series=\"30-MA\") +\n        xlab(\"Year\") + ylab(\"Adjusted Closing Price\") +\n        ggtitle(\"LLY Stock Price Trend in (30-days Moving Average)\") +\n        scale_colour_manual(values=c(\"LLY_ts\"=\"grey50\",\"30-MA\"=\"red\"),\n                            breaks=c(\"LLY_ts\",\"30-MA\"))\n\nMA_251 <- autoplot(LLY_ts, series=\"Data\") +\n        autolayer(ma(LLY_ts,251), series=\"251-MA\") +\n        xlab(\"Year\") + ylab(\"Adjusted Closing Price\") +\n        ggtitle(\"LLY Stock Price Trend in (251-days Moving Average)\") +\n        scale_colour_manual(values=c(\"LLY_ts\"=\"grey50\",\"251-MA\"=\"red\"),\n                            breaks=c(\"LLY_ts\",\"251-MA\"))\n\ngrid.arrange(MA_7, MA_30, MA_251, ncol=1)\n\n\n\n\n\nThe graph above shows the moving average of 7 days, 30 days and 251 days. 251 days was choose because there are around 251 days of stock price data per year. According to the plots, it can be observed that When MA is very large(MA=251), some parts of smoothing line(red) do not fit the real stock price line. While When MA is small(MA=7), the smoothing line(red) fits the real price line. MA-30 greatly fits the real price line. Therefore, MA-30 might be a good parameter for smoothing."
  },
  {
    "objectID": "eda_pfe.html",
    "href": "eda_pfe.html",
    "title": "EDA of PFE",
    "section": "",
    "text": "Refer to EDA-UNH for more detailed description for each plot.\n\nBasic Time Series Plot\n\n\nShow the code\n# candlestick plot\n\nPFE_df <- as.data.frame(PFE)\nPFE_df$Dates <- as.Date(rownames(PFE_df))\n\nfig_PFE <- PFE_df %>% plot_ly(x = ~Dates, type=\"candlestick\",\n          open = ~PFE.Open, close = ~PFE.Close,\n          high = ~PFE.High, low = ~PFE.Low) \nfig_PFE <- fig_PFE %>% \n  layout(title = \"Basic Candlestick Chart for Pfizer\")\n\nfig_PFE\n\n\n\n\n\n\n\n\nLag plot\n\n\nShow the code\nPFE_ts <- ts(stock_df$PFE, start = c(2010,1),end = c(2023,1),\n             frequency = 251)\nts_lags(PFE_ts)\n\n\n\n\n\n\n\n\nDecomposed times series\n\n\nShow the code\ndecompose_PFE <- decompose(PFE_ts,'multiplicative')\nautoplot(decompose_PFE)\n\n\n\n\n\n\n\nAutocorrelation in Time Series\n\n\nShow the code\nPFE_acf <- ggAcf(PFE_ts,100)+ggtitle(\"ACF Plot for PFE\")\nPFE_pacf <- ggPacf(PFE_ts)+ggtitle(\"PACF Plot for PFE\")\n\ngrid.arrange(PFE_acf, PFE_pacf,nrow=2)\n\n\n\n\n\n\n\nAugmented Dickey-Fuller Test\n\n\nShow the code\ntseries::adf.test(PFE_ts)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  PFE_ts\nDickey-Fuller = -3.572, Lag order = 14, p-value = 0.03534\nalternative hypothesis: stationary\n\n\n\n\nDetrending\n\n\nShow the code\nfit = lm(PFE_ts~time(PFE_ts), na.action=NULL) \n\ny= PFE_ts\nx=time(PFE_ts)\nDD<-data.frame(x,y)\nggp <- ggplot(DD, aes(x, y)) +           \n  geom_line()\n\nggp <- ggp +                                     \n  stat_smooth(method = \"lm\",\n              formula = y ~ x,\n              geom = \"smooth\") +ggtitle(\"PFE Stock Price\")+ylab(\"Price\")\n\nplot1<-autoplot(resid(fit), main=\"detrended\") \nplot2<-autoplot(diff(PFE_ts), main=\"first difference\") \n\n\ngrid.arrange(ggp, plot1, plot2,nrow=3)\n\n\nDon't know how to automatically pick scale for object of type <ts>. Defaulting\nto continuous.\nDon't know how to automatically pick scale for object of type <ts>. Defaulting\nto continuous.\n\n\n\n\n\n\n\nMoving Average Smoothing\nSmoothing methods are a family of forecasting methods that average values over multiple periods in order to reduce the noise and uncover patterns in the data. It is useful as a data preparation technique as it can reduce the random variation in the observations and better expose the structure of the underlying causal processes. We call this an m-MA, meaning a moving average of order m.\n\n\nShow the code\nMA_7 <- autoplot(PFE_ts, series=\"Data\") +\n        autolayer(ma(PFE_ts,7), series=\"7-MA\") +\n        xlab(\"Year\") + ylab(\"Adjusted Closing Price\") +\n        ggtitle(\"PFE Stock Price Trend in (7-days Moving Average)\") +\n        scale_colour_manual(values=c(\"PFE_ts\"=\"grey50\",\"7-MA\"=\"red\"),\n                            breaks=c(\"PFE_ts\",\"7-MA\"))\n\nMA_30 <- autoplot(PFE_ts, series=\"Data\") +\n        autolayer(ma(PFE_ts,30), series=\"30-MA\") +\n        xlab(\"Year\") + ylab(\"Adjusted Closing Price\") +\n        ggtitle(\"PFE Stock Price Trend in (30-days Moving Average)\") +\n        scale_colour_manual(values=c(\"PFE_ts\"=\"grey50\",\"30-MA\"=\"red\"),\n                            breaks=c(\"PFE_ts\",\"30-MA\"))\n\nMA_251 <- autoplot(PFE_ts, series=\"Data\") +\n        autolayer(ma(PFE_ts,251), series=\"251-MA\") +\n        xlab(\"Year\") + ylab(\"Adjusted Closing Price\") +\n        ggtitle(\"PFE Stock Price Trend in (251-days Moving Average)\") +\n        scale_colour_manual(values=c(\"PFE_ts\"=\"grey50\",\"251-MA\"=\"red\"),\n                            breaks=c(\"PFE_ts\",\"251-MA\"))\n\ngrid.arrange(MA_7, MA_30, MA_251, ncol=1)\n\n\n\n\n\nThe graph above shows the moving average of 7 days, 30 days and 251 days. 251 days was choose because there are around 251 days of stock price data per year. According to the plots, it can be observed that When MA is very large(MA=251), some parts of smoothing line(red) do not fit the real stock price line. While When MA is small(MA=7), the smoothing line(red) fits the real price line. MA-30 greatly fits the real price line. Therefore, MA-30 might be a good parameter for smoothing."
  },
  {
    "objectID": "eda_syk.html",
    "href": "eda_syk.html",
    "title": "EDA of SYK",
    "section": "",
    "text": "Refer to EDA-UNH for more detailed description for each plot.\n\nBasic Time Series Plot\n\n\nShow the code\n# candlestick plot\n\nSYK_df <- as.data.frame(SYK)\nSYK_df$Dates <- as.Date(rownames(SYK_df))\n\nfig_SYK <- SYK_df %>% plot_ly(x = ~Dates, type=\"candlestick\",\n          open = ~SYK.Open, close = ~SYK.Close,\n          high = ~SYK.High, low = ~SYK.Low) \nfig_SYK <- fig_SYK %>% \n  layout(title = \"Basic Candlestick Chart for Pfizer\")\n\nfig_SYK\n\n\n\n\n\n\n\n\nLag plot\n\n\nShow the code\nSYK_ts <- ts(stock_df$SYK, start = c(2010,1),end = c(2023,1),\n             frequency = 251)\nts_lags(SYK_ts)\n\n\n\n\n\n\n\n\nDecomposed times series\n\n\nShow the code\ndecompose_SYK <- decompose(SYK_ts,'additive')\nautoplot(decompose_SYK)\n\n\n\n\n\n\n\nAutocorrelation in Time Series\n\n\nShow the code\nggAcf(SYK_ts,100)+ggtitle(\"ACF Plot for SYK\")\n\n\n\n\n\nShow the code\nggPacf(SYK_ts)+ggtitle(\"PACF Plot for SYK\")\n\n\n\n\n\n\n\nAugmented Dickey-Fuller Test\n\n\nShow the code\ntseries::adf.test(SYK_ts)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  SYK_ts\nDickey-Fuller = -1.734, Lag order = 14, p-value = 0.6909\nalternative hypothesis: stationary\n\n\n\n\nDetrending\n\n\nShow the code\nfit = lm(SYK_ts~time(SYK_ts), na.action=NULL) \n\ny= SYK_ts\nx=time(SYK_ts)\nDD<-data.frame(x,y)\nggp <- ggplot(DD, aes(x, y)) +           \n  geom_line()\n\nggp <- ggp +                                     \n  stat_smooth(method = \"lm\",\n              formula = y ~ x,\n              geom = \"smooth\") +ggtitle(\"SYK Stock Price\")+ylab(\"Price\")\n\nplot1<-autoplot(resid(fit), main=\"detrended\") \nplot2<-autoplot(diff(SYK_ts), main=\"first difference\") \n\n\ngrid.arrange(ggp, plot1, plot2,nrow=3)\n\n\nDon't know how to automatically pick scale for object of type <ts>. Defaulting\nto continuous.\nDon't know how to automatically pick scale for object of type <ts>. Defaulting\nto continuous.\n\n\n\n\n\n\n\nMoving Average Smoothing\nSmoothing methods are a family of forecasting methods that average values over multiple periods in order to reduce the noise and uncover patterns in the data. It is useful as a data preparation technique as it can reduce the random variation in the observations and better expose the structure of the underlying causal processes. We call this an m-MA, meaning a moving average of order m.\n\n\nShow the code\nMA_7 <- autoplot(SYK_ts, series=\"Data\") +\n        autolayer(ma(SYK_ts,7), series=\"7-MA\") +\n        xlab(\"Year\") + ylab(\"Adjusted Closing Price\") +\n        ggtitle(\"SYK Stock Price Trend in (7-days Moving Average)\") +\n        scale_colour_manual(values=c(\"SYK_ts\"=\"grey50\",\"7-MA\"=\"red\"),\n                            breaks=c(\"SYK_ts\",\"7-MA\"))\n\nMA_30 <- autoplot(SYK_ts, series=\"Data\") +\n        autolayer(ma(SYK_ts,30), series=\"30-MA\") +\n        xlab(\"Year\") + ylab(\"Adjusted Closing Price\") +\n        ggtitle(\"SYK Stock Price Trend in (30-days Moving Average)\") +\n        scale_colour_manual(values=c(\"SYK_ts\"=\"grey50\",\"30-MA\"=\"red\"),\n                            breaks=c(\"SYK_ts\",\"30-MA\"))\n\nMA_251 <- autoplot(SYK_ts, series=\"Data\") +\n        autolayer(ma(SYK_ts,251), series=\"251-MA\") +\n        xlab(\"Year\") + ylab(\"Adjusted Closing Price\") +\n        ggtitle(\"SYK Stock Price Trend in (251-days Moving Average)\") +\n        scale_colour_manual(values=c(\"SYK_ts\"=\"grey50\",\"251-MA\"=\"red\"),\n                            breaks=c(\"SYK_ts\",\"251-MA\"))\n\ngrid.arrange(MA_7, MA_30, MA_251, ncol=1)\n\n\n\n\n\nThe graph above shows the moving average of 7 days, 30 days and 251 days. 251 days was choose because there are around 251 days of stock price data per year. According to the plots, it can be observed that When MA is very large(MA=251), some parts of smoothing line(red) do not fit the real stock price line. While When MA is small(MA=7), the smoothing line(red) fits the real price line. MA-30 greatly fits the real price line. Therefore, MA-30 might be a good parameter for smoothing."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction",
    "section": "",
    "text": "How does COVID-19 affect Healthcare Industry?\nEveryone in the world virtually requires healthcare. According to Centers for Medicare & Medicaid Services, national health expenditure rose 2.7% to 4.3 trillion dollars in 2021, 12914 dollars per person, and represent 18.3% of GDP. Clearly, healthcare companies contribute a lot of sales in United States.\nThe behaviors of consumers and investor shifted dramatically as the rapid spread out of COVID-19 worldwide. Most of the stock market plummeted at the outbreak of the virus due to massive selloffs. COVID-19 has had a significant impact on the healthcare industry and healthcare stock prices. Even though the healthcare sector has been a reliable safe haven during even the most challenging markets, the novel COVID-19 panic is raising questions about its resilience. This leads to the thoughts that how stock market of healthcare industry perform during pandemic. The outbreak of disease may cause greater demand of healthcare service and higher turnover of pharmaceutical industries. Besides, the distribution of vaccine may possibly result in higher stock price of responsible companies.\nThere are several types of healthcare service, pharmaceuticals, biotechnology, medical care, medical supply, health insurance, and pharmacy benefit manager. This project has choose several of healthcare companies in United States, such as Moderna, UnitedHealth Group Incorporated, and CVS. By examining the stock price of each company, we would know how healthcare companies evolved over the past, how healthcare companies affected by pandemic, and identify the similarity and differences between the companies.\n\n\nImportant Questions\n\nWhat type of healthcare industry create biggest sales?\nHow has the stock price of healthcare companies evolved?\nHow pandemic affect healthcare industry?\nWhich companies were the biggest gainer during the pandemic?\nHow the launch of Covid-19 vaccine affect the stock price of healthcare companies?\nAre we able to predict the stock prices of these companies?\nIs there any factors affecting healthcare stock prices?\nWhat’s the differences between these companies?\nHow different type of healthcare companies varies to each other?\nWhen would be think as more risky to invest in healthcare stock market?"
  },
  {
    "objectID": "models.html",
    "href": "models.html",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "",
    "text": "ARIMA(p,d,q) stands for Auto Regressive Integrated Moving Average. It is a class of statistical models for analyzing and forecasting time series data. Specifically, it explains a given time series based on its own past values, that is, its own lags and the lagged forecast errors, so that equation can be used to forecast future values. Any ‘non-seasonal’ time series that exhibits patterns and is not a random white noise can be modeled with ARIMA models.\nThe acronym of ARIMA(p,d,q) is descriptive, capturing the key aspects of the model itself. Briefly, they are:\n\nAR: Autoregression. A model that uses the dependent relationship between an observation and some number of lagged observations.\nI: Integrated. The use of differencing of raw observations (e.g. subtracting an observation from an observation at the previous time step) in order to make the time series stationary.\nMA: Moving Average. A model that uses the dependency between an observation and a residual error. Each of these components are explicitly specified in the model as a parameter.\n\nThe basic steps to fit ARIMA models to time series data involve:\n\nPlotting the data and possibly transforming the data\nDetermine the stationality of time series and eliminate the stationality if it exist\nIdentifying the dependence orders of the model & Parameter estimation\nFit the model\nDiagnostics & model choice\n\n\n\n\nThis section is going to conduct ARIMA models on UNH, CVS, LLY, AZN and PFE stock prices, and make prediction for next 100 values. Additionally, I run the model on pre-COVID part for UNH data, and made forecasting to cover the COVID-19 time, and compared the real data with the prediction. This aims to see how stock price would act without COVID-19.\nClick links below to see the more detailed page.\n\nUNH\nUNH pre-COVID\nCVS\nAZN\nLLY\nPFE"
  },
  {
    "objectID": "model_azn.html",
    "href": "model_azn.html",
    "title": "ARMA/ARIMA/SARIMA Models for AZN",
    "section": "",
    "text": "Step 1: Determine the stationality of time series\nStationality is a pre-requirement of training ARIMA model. This is because term ‘Auto Regressive’ in ARIMA means it is a linear regression model that uses its own lags as predictors, which work best when the predictors are not correlated and are independent of each other. Stationary time series make sure the statistical properties of time series do not change over time.\nBased on information obtained from both ACF graphs and Augmented Dickey-Fuller Test, the time series data is non-stationary.\n\n\nShow the code\nAZN_acf <- ggAcf(AZN_ts,100)+ggtitle(\"AZN ACF Plot\")\n\nAZN_pacf <- ggPacf(AZN_ts)+ggtitle(\"PACF Plot for UHNs\")\ngrid.arrange(AZN_acf, AZN_pacf,nrow=2)\n\n\n\n\n\n\n\nShow the code\ntseries::adf.test(AZN_ts)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  AZN_ts\nDickey-Fuller = -3.3602, Lag order = 14, p-value = 0.06026\nalternative hypothesis: stationary\n\n\n\n\nStep 2: Eliminate Non-Stationality\nSince this data is non-stationary, it is important to necessary to convert it to stationary time series. This step employs a series of actions to eliminate non-stationality, i.e. log transformation and differencing the data. It turns out the log transformed and 1st differened data has shown good stationary property, there are no need to go further at 2nd differencing. What is more, the Augmented Dickey-Fuller Test also confirmed that the log transformed and 1st differenced data is stationary. Therefore, the log transformation and 1st differencing would be the actions taken to eliminate the non-stationality.\n\n\nShow the code\nplot1<- ggAcf(log(AZN_ts) %>%diff(), 50, main=\"ACF Plot for Log Transformed & 1st differenced Data\") \nplot2<- ggAcf(log(AZN_ts) %>%diff()%>%diff(),50, main=\"ACF Plot for Log Transformed & 2nd differenced Data\") \n\ngrid.arrange(plot1, plot2,nrow=2)\n\n\n\n\n\n\n\nShow the code\ntseries::adf.test(log(AZN_ts) %>%diff())\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  log(AZN_ts) %>% diff()\nDickey-Fuller = -15.655, Lag order = 14, p-value = 0.01\nalternative hypothesis: stationary\n\n\n\n\nStep 3: Determine p,d,q Parameters\nThe standard notation of ARIMA(p,d,q) include p,d,q 3 parameters. Here are the representations: - p: The number of lag observations included in the model, also called the lag order; order of the AR term. - d: The number of times that the raw observations are differenced, also called the degree of differencing; number of differencing required to make the time series stationary. - q: order of moving average; order of the MA term. It refers to the number of lagged forecast errors that should go into the ARIMA Model.\n\n\nShow the code\nplot3<- ggPacf(log(AZN_ts) %>%diff(),50, main=\"PACF Plot for Log Transformed & 1st differenced Data\") \n\ngrid.arrange(plot1,plot3)\n\n\n\n\n\nAccording to the PACF plot and ACF plot above, both plots have 3 significant peak at 6,7,8. To avoid over-complexity, here choose the value of p and q as 0. Since I only differenced the data once, the d would be 1.\n\n\nStep 4: Fit ARIMA(p,d,q) model\n\n\nShow the code\nfit1 <- Arima(log(AZN_ts), order=c(0, 1, 0),include.drift = TRUE) \nsummary(fit1)\n\n\nSeries: log(AZN_ts) \nARIMA(0,1,0) with drift \n\nCoefficients:\n      drift\n      5e-04\ns.e.  3e-04\n\nsigma^2 = 0.0002423:  log likelihood = 8953.58\nAIC=-17903.16   AICc=-17903.16   BIC=-17890.98\n\nTraining set error measures:\n                      ME       RMSE        MAE           MPE     MAPE\nTraining set 1.15802e-06 0.01555995 0.01046885 -0.0005720625 0.227243\n                   MASE        ACF1\nTraining set 0.05903222 -0.03420903\n\n\n\nModel Diagnostics\n\nInspection of the time plot of the standardized residuals below shows no obvious patterns.\nNotice that there may be outliers, with a few values exceeding 3 standard deviations in magnitude.\nThe ACF of the standardized residuals shows no apparent departure from the model assumptions, no significant lags shown.\nThe normal Q-Q plot of the residuals shows that the assumption of normality is reasonable, with the exception of the fat-tailed.\nThe model appears to fit well.\n\n\n\nShow the code\nmodel_output <- capture.output(sarima(log(AZN_ts), 0,1,0))\n\n\n\n\n\n\n\nShow the code\ncat(model_output[9:38], model_output[length(model_output)], sep = \"\\n\") #to get rid of the convergence status and details of the optimization algorithm used by the sarima() \n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n      constant\n         5e-04\ns.e.     3e-04\n\nsigma^2 estimated as 0.0002422:  log likelihood = 8953.58,  aic = -17903.16\n\n$degrees_of_freedom\n[1] 3262\n\n$ttable\n         Estimate    SE t.value p.value\nconstant    5e-04 3e-04  1.8978  0.0578\n\n$AIC\n[1] -5.486718\n\n$AICc\n[1] -5.486718\n\n$BIC\n[1] -5.482985\n\n\n\n\nCompare with auto.arima() function\nauto.arima() returns best ARIMA model according to either AIC, AICc or BIC value. The function conducts a search over possible model within the order constraints provided. However, this method is not reliable sometimes. It fits a different model than ACF/PACF plots suggest. This is because auto.arima() usually return models that are more complex as it prefers more parameters compared than to the for example BIC.\n\n\nShow the code\nauto.arima(log(AZN_ts))\n\n\nSeries: log(AZN_ts) \nARIMA(1,1,1) with drift \n\nCoefficients:\n         ar1      ma1  drift\n      0.9292  -0.9519  5e-04\ns.e.  0.0365   0.0305  2e-04\n\nsigma^2 = 0.0002415:  log likelihood = 8959.68\nAIC=-17911.36   AICc=-17911.35   BIC=-17887\n\n\n\n\n\nStep 5: Forecast\nThe blue part in graph below forecast the next 100 values of AZN stock price in 80% and 95% confidence level.\n\n\nShow the code\nlog(AZN_ts) %>%\n  Arima(order=c(0,1,0),include.drift = TRUE) %>%\n  forecast(100) %>%\n  autoplot() +\n  ylab(\"AZN stock prices prediction\") + xlab(\"Year\")\n\n\n\n\n\n\n\nStep 6: Compare ARIMA model with the benchmark methods\nForecasting benchmarks are very important when testing new forecasting methods, to see how well they perform against some simple alternatives.\n\nAverage method\nHere, the forecast of all future values are equal to the average of the historical data. The residual plot of this method is not stationary.\n\n\nShow the code\nf1<-meanf(log(AZN_ts), h=251) #mean\n#summary(f1)\ncheckresiduals(f1)#serial correlation ; Lung Box p <0.05\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Mean\nQ* = 1165071, df = 501, p-value < 2.2e-16\n\nModel df: 1.   Total lags used: 502\n\n\n\n\nNaive method\nThis method simply set all forecasts to be the value of the last observation. According to error measurement here, ARIMA(0,1,0) outperform the average method.\n\n\nShow the code\nf2<-naive(log(AZN_ts), h=11) # naive method\nsummary(f2)\n\n\n\nForecast method: Naive method\n\nModel Information:\nCall: naive(y = log(AZN_ts), h = 11) \n\nResidual sd: 0.0156 \n\nError measures:\n                       ME       RMSE        MAE        MPE      MAPE       MASE\nTraining set 0.0005180792 0.01557082 0.01049512 0.01076463 0.2277994 0.05918031\n                    ACF1\nTraining set -0.03425189\n\nForecasts:\n         Point Forecast    Lo 80    Hi 80    Lo 95    Hi 95\n2023.004       5.470789 5.450834 5.490744 5.440271 5.501307\n2023.008       5.470789 5.442569 5.499010 5.427630 5.513949\n2023.012       5.470789 5.436226 5.505352 5.417930 5.523648\n2023.016       5.470789 5.430880 5.510699 5.409753 5.531826\n2023.020       5.470789 5.426169 5.515410 5.402548 5.539030\n2023.024       5.470789 5.421910 5.519668 5.396035 5.545543\n2023.028       5.470789 5.417994 5.523585 5.390046 5.551533\n2023.032       5.470789 5.414349 5.527230 5.384471 5.557108\n2023.036       5.470789 5.410925 5.530654 5.379234 5.562344\n2023.040       5.470789 5.407687 5.533892 5.374282 5.567296\n2023.044       5.470789 5.404607 5.536972 5.369572 5.572007\n\n\nShow the code\ncheckresiduals(f2)#serial correlation ; Lung Box p <0.05\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Naive method\nQ* = 627.23, df = 502, p-value = 0.0001143\n\nModel df: 0.   Total lags used: 502\n\n\n\n\nSeasonal naive method\nThis method is useful for highly seasonal data, which can set each forecast to be equal to the last observed value from the same season of the year. Here seasonal naive is used to forecast the next 4 values for the AZN stock price series.\n\n\nShow the code\nf3<-snaive(log(AZN_ts), h=4) #seasonal naive method\nsummary(f3)\n\n\n\nForecast method: Seasonal naive method\n\nModel Information:\nCall: snaive(y = log(AZN_ts), h = 4) \n\nResidual sd: 0.1989 \n\nError measures:\n                    ME      RMSE       MAE      MPE     MAPE MASE      ACF1\nTraining set 0.1378916 0.1988982 0.1773414 2.933481 3.770862    1 0.9881511\n\nForecasts:\n         Point Forecast    Lo 80    Hi 80    Lo 95    Hi 95\n2023.004       5.528218 5.273320 5.783116 5.138385 5.918052\n2023.008       5.558320 5.303422 5.813219 5.168487 5.948154\n2023.012       5.574101 5.319203 5.828999 5.184268 5.963934\n2023.016       5.582158 5.327260 5.837056 5.192325 5.971991\n\n\nShow the code\ncheckresiduals(f3) #serial correlation ; Lung Box p <0.05\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Seasonal naive method\nQ* = 174670, df = 502, p-value < 2.2e-16\n\nModel df: 0.   Total lags used: 502\n\n\n\n\nDrift Method\nA variation on the naïve method is to allow the forecasts to increase or decrease over time, where the amount of change over time is set to be the average change seen in the historical data.\n\n\nShow the code\nf4 <- rwf(log(AZN_ts),drift=TRUE, h=20) \nsummary(f4)\n\n\n\nForecast method: Random walk with drift\n\nModel Information:\nCall: rwf(y = log(AZN_ts), h = 20, drift = TRUE) \n\nDrift: 5e-04  (se 3e-04)\nResidual sd: 0.0156 \n\nError measures:\n                       ME      RMSE       MAE           MPE     MAPE       MASE\nTraining set 2.353136e-16 0.0155622 0.0104709 -0.0006028803 0.227282 0.05904378\n                    ACF1\nTraining set -0.03425189\n\nForecasts:\n         Point Forecast    Lo 80    Hi 80    Lo 95    Hi 95\n2023.004       5.471307 5.451357 5.491257 5.440797 5.501818\n2023.008       5.471825 5.443608 5.500043 5.428670 5.514981\n2023.012       5.472343 5.437779 5.506908 5.419481 5.525206\n2023.016       5.472862 5.432943 5.512780 5.411812 5.533911\n2023.020       5.473380 5.428743 5.518016 5.405114 5.541645\n2023.024       5.473898 5.424993 5.522802 5.399105 5.548691\n2023.028       5.474416 5.421585 5.527247 5.393618 5.555214\n2023.032       5.474934 5.418447 5.531421 5.388544 5.561324\n2023.036       5.475452 5.415529 5.535375 5.383808 5.567096\n2023.040       5.475970 5.412796 5.539144 5.379354 5.572586\n2023.044       5.476488 5.410221 5.542756 5.375141 5.577836\n2023.048       5.477006 5.407781 5.546231 5.371136 5.582876\n2023.052       5.477524 5.405462 5.549587 5.367314 5.587734\n2023.056       5.478042 5.403248 5.552836 5.363655 5.592430\n2023.060       5.478560 5.401129 5.555991 5.360140 5.596981\n2023.064       5.479078 5.399096 5.559061 5.356756 5.601401\n2023.068       5.479597 5.397140 5.562053 5.353490 5.605703\n2023.072       5.480115 5.395254 5.564975 5.350332 5.609897\n2023.076       5.480633 5.393434 5.567832 5.347273 5.613992\n2023.080       5.481151 5.391673 5.570629 5.344306 5.617995\n\n\nShow the code\ncheckresiduals(f4)\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Random walk with drift\nQ* = 627.23, df = 501, p-value = 0.0001014\n\nModel df: 1.   Total lags used: 502\n\n\n\n\nShow the code\nautoplot(AZN_ts) +\n  autolayer(meanf(AZN_ts, h=100),\n            series=\"Mean.tr\", PI=FALSE) +\n  autolayer(naive((AZN_ts), h=100),\n            series=\"Naïve.tr\", PI=FALSE) +\n  autolayer(rwf((AZN_ts), drift=TRUE, h=100),\n            series=\"Drift.tr\", PI=FALSE) +\n  autolayer(forecast(Arima((AZN_ts), order=c(4, 1, 3),include.drift = TRUE),100), \n            series=\"fit\",PI=FALSE) +\n  ggtitle(\"AZN Stock Price\") +\n  xlab(\"Time\") + ylab(\"Log(Price)\") +\n  guides(colour=guide_legend(title=\"Forecast\"))\n\n\n\n\n\nAccording to the graph above, ARIMA(0,1,0) outperform most of benchmark method, though its performance is very similar to drift method."
  },
  {
    "objectID": "model_cvs.html",
    "href": "model_cvs.html",
    "title": "ARMA/ARIMA/SARIMA Models for CVS",
    "section": "",
    "text": "Step 1: Determine the stationality of time series\nStationality is a pre-requirement of training ARIMA model. This is because term ‘Auto Regressive’ in ARIMA means it is a linear regression model that uses its own lags as predictors, which work best when the predictors are not correlated and are independent of each other. Stationary time series make sure the statistical properties of time series do not change over time.\nBased on information obtained from ACF graphs, the time series data is non-stationary, though Augmented Dickey-Fuller Test shows the series is stationary .\n\n\nShow the code\nCVS_acf <- ggAcf(CVS_ts,100)+ggtitle(\"CVS ACF Plot\")\n\nCVS_pacf <- ggPacf(CVS_ts)+ggtitle(\"PACF Plot for UHNs\")\ngrid.arrange(CVS_acf, CVS_pacf,nrow=2)\n\n\n\n\n\n\n\nShow the code\ntseries::adf.test(CVS_ts)\n\n\nWarning in tseries::adf.test(CVS_ts): p-value smaller than printed p-value\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  CVS_ts\nDickey-Fuller = -4.5012, Lag order = 14, p-value = 0.01\nalternative hypothesis: stationary\n\n\n\n\nStep 2: Eliminate Non-Stationality\nSince this data is non-stationary, it is important to necessary to convert it to stationary time series. This step employs a series of actions to eliminate non-stationality, i.e. log transformation and differencing the data. It turns out the log transformed and 1st differened data has shown good stationary property, there are no need to go further at 2nd differencing. What is more, the Augmented Dickey-Fuller Test also confirmed that the log transformed and 1st differenced data is stationary. Therefore, the log transformation and 1st differencing would be the actions taken to eliminate the non-stationality.\n\n\nShow the code\nplot1<- ggAcf(log(CVS_ts) %>%diff(), 50, main=\"ACF Plot for Log Transformed & 1st differenced Data\") \nplot2<- ggAcf(log(CVS_ts) %>%diff()%>%diff(),50, main=\"ACF Plot for Log Transformed & 2nd differenced Data\") \n\ngrid.arrange(plot1, plot2,nrow=2)\n\n\n\n\n\n\n\nShow the code\ntseries::adf.test(log(CVS_ts) %>%diff())\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  log(CVS_ts) %>% diff()\nDickey-Fuller = -16.066, Lag order = 14, p-value = 0.01\nalternative hypothesis: stationary\n\n\n\n\nStep 3: Determine p,d,q Parameters\nThe standard notation of ARIMA(p,d,q) include p,d,q 3 parameters. Here are the representations: - p: The number of lag observations included in the model, also called the lag order; order of the AR term. - d: The number of times that the raw observations are differenced, also called the degree of differencing; number of differencing required to make the time series stationary. - q: order of moving average; order of the MA term. It refers to the number of lagged forecast errors that should go into the ARIMA Model.\n\n\nShow the code\nplot3<- ggPacf(log(CVS_ts) %>%diff(),50, main=\"PACF Plot for Log Transformed & 1st differenced Data\") \n\ngrid.arrange(plot1,plot3)\n\n\n\n\n\nAccording to the PACF plot and ACF plot above, both plots have significant peak at 1 and 4. Therefore, here choose the value of p and q as 1-4. Since I only differenced the data once, the d would be 1.\n\n\nStep 4: Fit ARIMA(p,d,q) model\nBefore fitting the data with ARIMA(p,d,q) model, we will need to choose the set of parameters based on error measurement and model diagnostics. Based on the result below, ARIMA(1,1,1) has lowest error measurement.\n\n\nShow the code\n######################## Check for different combinations ########\n\n\nd=1\ni=1\ntemp= data.frame()\nls=matrix(rep(NA,6*15),nrow=15) # roughly nrow = 3x4x2\n\n\nfor (p in 2:5)# p=1,2,3,4 : 4\n{\n  for(q in 2:5)# q=1,2,4 :4\n  {\n    for(d in 1:1)# d=1 :2\n    {\n      \n      if(p-1+d+q-1<=8)\n      {\n        \n        model<- Arima(log(CVS_ts),order=c(p-1,d,q-1),include.drift=TRUE) \n        ls[i,]= c(p-1,d,q-1,model$aic,model$bic,model$aicc)\n        i=i+1\n        #print(i)\n        \n      }\n      \n    }\n  }\n}\n\ntemp= as.data.frame(ls)\nnames(temp)= c(\"p\",\"d\",\"q\",\"AIC\",\"BIC\",\"AICc\")\n\nkable(temp) %>%\n  kable_styling(font_size = 12)\n\n\n\n\n \n  \n    p \n    d \n    q \n    AIC \n    BIC \n    AICc \n  \n \n\n  \n    1 \n    1 \n    1 \n    -20412.67 \n    -20388.31 \n    -20412.66 \n  \n  \n    1 \n    1 \n    2 \n    -20411.13 \n    -20380.68 \n    -20411.11 \n  \n  \n    1 \n    1 \n    3 \n    -20414.70 \n    -20378.16 \n    -20414.68 \n  \n  \n    1 \n    1 \n    4 \n    -20424.62 \n    -20381.99 \n    -20424.59 \n  \n  \n    2 \n    1 \n    1 \n    -20411.03 \n    -20380.57 \n    -20411.01 \n  \n  \n    2 \n    1 \n    2 \n    -20412.17 \n    -20375.63 \n    -20412.15 \n  \n  \n    2 \n    1 \n    3 \n    -20416.90 \n    -20374.27 \n    -20416.86 \n  \n  \n    2 \n    1 \n    4 \n    -20423.67 \n    -20374.95 \n    -20423.63 \n  \n  \n    3 \n    1 \n    1 \n    -20413.32 \n    -20376.78 \n    -20413.30 \n  \n  \n    3 \n    1 \n    2 \n    -20421.63 \n    -20379.00 \n    -20421.59 \n  \n  \n    3 \n    1 \n    3 \n    -20420.92 \n    -20372.20 \n    -20420.88 \n  \n  \n    3 \n    1 \n    4 \n    -20421.65 \n    -20366.84 \n    -20421.60 \n  \n  \n    4 \n    1 \n    1 \n    -20425.68 \n    -20383.04 \n    -20425.64 \n  \n  \n    4 \n    1 \n    2 \n    -20425.19 \n    -20376.47 \n    -20425.15 \n  \n  \n    4 \n    1 \n    3 \n    -20423.31 \n    -20368.50 \n    -20423.26 \n  \n\n\n\n\n\n\nError measurement\nLowest AIC:\n\n\nShow the code\ntemp[which.min(temp$AIC),] \n\n\n   p d q       AIC       BIC      AICc\n13 4 1 1 -20425.68 -20383.04 -20425.64\n\n\nLowest BIC:\n\n\n  p d q       AIC       BIC      AICc\n1 1 1 1 -20412.67 -20388.31 -20412.66\n\n\nLowest AICc:\n\n\n   p d q       AIC       BIC      AICc\n13 4 1 1 -20425.68 -20383.04 -20425.64\n\n\n\n\nShow the code\nfit1 <- Arima(log(CVS_ts), order=c(1, 1, 1),include.drift = TRUE) \nsummary(fit1)\n\n\nSeries: log(CVS_ts) \nARIMA(1,1,1) with drift \n\nCoefficients:\n          ar1     ma1  drift\n      -0.3279  0.2487  4e-04\ns.e.   0.1406  0.1440  2e-04\n\nsigma^2 = 0.0001122:  log likelihood = 10210.33\nAIC=-20412.67   AICc=-20412.66   BIC=-20388.31\n\nTraining set error measures:\n                       ME       RMSE         MAE           MPE      MAPE\nTraining set -2.07333e-06 0.01058626 0.007233617 -0.0003187543 0.1611386\n                   MASE        ACF1\nTraining set 0.05795515 0.001266904\n\n\n\n\nModel Diagnostics\n\nInspection of the time plot of the standardized residuals below shows no obvious patterns.\nNotice that there may be outliers, with a few values exceeding 3 standard deviations in magnitude.\nThe ACF of the standardized residuals shows no apparent departure from the model assumptions, no significant lags shown.\nThe normal Q-Q plot of the residuals shows that the assumption of normality is reasonable, with the exception of the fat-tailed.\nThe model appears to fit well.\n\n\n\nShow the code\nmodel_output <- capture.output(sarima(log(CVS_ts), 1,1,1))\n\n\n\n\n\n\n\nShow the code\ncat(model_output[20:51], model_output[length(model_output)], sep = \"\\n\") #to get rid of the convergence status and details of the optimization algorithm used by the sarima() \n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n          ar1     ma1  constant\n      -0.3279  0.2487     4e-04\ns.e.   0.1406  0.1440     2e-04\n\nsigma^2 estimated as 0.0001121:  log likelihood = 10210.33,  aic = -20412.67\n\n$degrees_of_freedom\n[1] 3260\n\n$ttable\n         Estimate     SE t.value p.value\nar1       -0.3279 0.1406 -2.3322  0.0197\nma1        0.2487 0.1440  1.7270  0.0843\nconstant   0.0004 0.0002  2.4264  0.0153\n\n$AIC\n[1] -6.255798\n\n$AICc\n[1] -6.255796\n\n$BIC\n[1] -6.248332\n\n\n\n\nCompare with auto.arima() function\nauto.arima() returns best ARIMA model according to either AIC, AICc or BIC value. The function conducts a search over possible model within the order constraints provided. However, this method is not reliable sometimes. It fits a different model than ACF/PACF plots suggest. This is because auto.arima() usually return models that are more complex as it prefers more parameters compared than to the for example BIC.\n\n\nShow the code\nauto.arima(log(CVS_ts))\n\n\nSeries: log(CVS_ts) \nARIMA(1,1,1) with drift \n\nCoefficients:\n          ar1     ma1  drift\n      -0.3279  0.2487  4e-04\ns.e.   0.1406  0.1440  2e-04\n\nsigma^2 = 0.0001122:  log likelihood = 10210.33\nAIC=-20412.67   AICc=-20412.66   BIC=-20388.31\n\n\n\n\n\nStep 5: Forecast\nThe blue part in graph below forecast the next 100 values of CVS stock price in 80% and 95% confidence level.\n\n\nShow the code\nlog(CVS_ts) %>%\n  Arima(order=c(1,1,1),include.drift = TRUE) %>%\n  forecast(100) %>%\n  autoplot() +\n  ylab(\"CVS stock prices prediction\") + xlab(\"Year\")\n\n\n\n\n\n\n\nStep 6: Compare ARIMA model with the benchmark methods\nForecasting benchmarks are very important when testing new forecasting methods, to see how well they perform against some simple alternatives.\n\nAverage method\nHere, the forecast of all future values are equal to the average of the historical data. The residual plot of this method is not stationary.\n\n\nShow the code\nf1<-meanf(log(CVS_ts), h=251) #mean\n#summary(f1)\ncheckresiduals(f1)#serial correlation ; Lung Box p <0.05\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Mean\nQ* = 1059576, df = 501, p-value < 2.2e-16\n\nModel df: 1.   Total lags used: 502\n\n\n\n\nNaive method\nThis method simply set all forecasts to be the value of the last observation. According to error measurement here, ARIMA(1,1,1) outperform the average method.\n\n\nShow the code\nf2<-naive(log(CVS_ts), h=11) # naive method\nsummary(f2)\n\n\n\nForecast method: Naive method\n\nModel Information:\nCall: naive(y = log(CVS_ts), h = 11) \n\nResidual sd: 0.0106 \n\nError measures:\n                      ME       RMSE         MAE         MPE      MAPE      MASE\nTraining set 0.000422012 0.01063283 0.007243782 0.009237788 0.1613226 0.0580366\n                    ACF1\nTraining set -0.07974793\n\nForecasts:\n         Point Forecast    Lo 80    Hi 80    Lo 95    Hi 95\n2023.004       5.160358 5.146732 5.173985 5.139518 5.181198\n2023.008       5.160358 5.141088 5.179629 5.130886 5.189831\n2023.012       5.160358 5.136757 5.183960 5.124263 5.196454\n2023.016       5.160358 5.133105 5.187611 5.118678 5.202038\n2023.020       5.160358 5.129889 5.190828 5.113759 5.206958\n2023.024       5.160358 5.126980 5.193736 5.109311 5.211406\n2023.028       5.160358 5.124306 5.196411 5.105221 5.215496\n2023.032       5.160358 5.121817 5.198900 5.101414 5.219303\n2023.036       5.160358 5.119479 5.201238 5.097839 5.222878\n2023.040       5.160358 5.117268 5.203449 5.094457 5.226260\n2023.044       5.160358 5.115164 5.205552 5.091240 5.229477\n\n\nShow the code\ncheckresiduals(f2)#serial correlation ; Lung Box p <0.05\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Naive method\nQ* = 624.58, df = 502, p-value = 0.0001515\n\nModel df: 0.   Total lags used: 502\n\n\n\n\nSeasonal naive method\nThis method is useful for highly seasonal data, which can set each forecast to be equal to the last observed value from the same season of the year. Here seasonal naive is used to forecast the next 4 values for the CVS stock price series.\n\n\nShow the code\nf3<-snaive(log(CVS_ts), h=4) #seasonal naive method\nsummary(f3)\n\n\n\nForecast method: Seasonal naive method\n\nModel Information:\nCall: snaive(y = log(CVS_ts), h = 4) \n\nResidual sd: 0.1493 \n\nError measures:\n                    ME      RMSE      MAE      MPE     MAPE MASE      ACF1\nTraining set 0.1139911 0.1493498 0.124814 2.528258 2.771018    1 0.9873504\n\nForecasts:\n         Point Forecast    Lo 80    Hi 80    Lo 95    Hi 95\n2023.004       5.086149 4.894750 5.277549 4.793429 5.378870\n2023.008       5.090446 4.899047 5.281846 4.797726 5.383166\n2023.012       5.092350 4.900950 5.283749 4.799630 5.385070\n2023.016       5.100754 4.909355 5.292154 4.808034 5.393474\n\n\nShow the code\ncheckresiduals(f3) #serial correlation ; Lung Box p <0.05\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Seasonal naive method\nQ* = 198350, df = 502, p-value < 2.2e-16\n\nModel df: 0.   Total lags used: 502\n\n\n\n\nDrift Method\nA variation on the naïve method is to allow the forecasts to increase or decrease over time, where the amount of change over time is set to be the average change seen in the historical data.\n\n\nShow the code\nf4 <- rwf(log(CVS_ts),drift=TRUE, h=20) \nsummary(f4)\n\n\n\nForecast method: Random walk with drift\n\nModel Information:\nCall: rwf(y = log(CVS_ts), h = 20, drift = TRUE) \n\nDrift: 4e-04  (se 2e-04)\nResidual sd: 0.0106 \n\nError measures:\n                       ME       RMSE         MAE           MPE      MAPE\nTraining set 1.597791e-16 0.01062445 0.007236045 -0.0002628975 0.1611678\n                   MASE        ACF1\nTraining set 0.05797461 -0.07974793\n\nForecasts:\n         Point Forecast    Lo 80    Hi 80    Lo 95    Hi 95\n2023.004       5.160780 5.147160 5.174400 5.139950 5.181610\n2023.008       5.161202 5.141938 5.180467 5.131740 5.190665\n2023.012       5.161624 5.138027 5.185222 5.125535 5.197714\n2023.016       5.162046 5.134794 5.189299 5.120367 5.203725\n2023.020       5.162468 5.131995 5.192942 5.115863 5.209074\n2023.024       5.162890 5.129503 5.196278 5.111829 5.213952\n2023.028       5.163312 5.127244 5.199381 5.108151 5.218474\n2023.032       5.163734 5.125170 5.202299 5.104755 5.222714\n2023.036       5.164156 5.123247 5.205066 5.101590 5.226723\n2023.040       5.164578 5.121449 5.207708 5.098618 5.230539\n2023.044       5.165001 5.119759 5.210242 5.095810 5.234191\n2023.048       5.165423 5.118162 5.212683 5.093144 5.237701\n2023.052       5.165845 5.116647 5.215042 5.090603 5.241086\n2023.056       5.166267 5.115204 5.217329 5.088173 5.244360\n2023.060       5.166689 5.113826 5.219551 5.085842 5.247535\n2023.064       5.167111 5.112506 5.221715 5.083600 5.250621\n2023.068       5.167533 5.111239 5.223827 5.081438 5.253627\n2023.072       5.167955 5.110020 5.225889 5.079351 5.256558\n2023.076       5.168377 5.108845 5.227908 5.077331 5.259422\n2023.080       5.168799 5.107711 5.229886 5.075374 5.262224\n\n\nShow the code\ncheckresiduals(f4)\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Random walk with drift\nQ* = 624.58, df = 501, p-value = 0.0001346\n\nModel df: 1.   Total lags used: 502\n\n\n\n\nShow the code\nautoplot(CVS_ts) +\n  autolayer(meanf(CVS_ts, h=100),\n            series=\"Mean.tr\", PI=FALSE) +\n  autolayer(naive((CVS_ts), h=100),\n            series=\"Naïve.tr\", PI=FALSE) +\n  autolayer(rwf((CVS_ts), drift=TRUE, h=100),\n            series=\"Drift.tr\", PI=FALSE) +\n  autolayer(forecast(Arima((CVS_ts), order=c(1, 1, 1),include.drift = TRUE),100), \n            series=\"fit\",PI=FALSE) +\n  ggtitle(\"CVS Stock Price\") +\n  xlab(\"Time\") + ylab(\"Log(Price)\") +\n  guides(colour=guide_legend(title=\"Forecast\"))\n\n\n\n\n\nAccording to the graph above, ARIMA(1,1,1) outperform most of benchmark method, though its performance is very similar to drift method."
  },
  {
    "objectID": "model_gdp.html",
    "href": "model_gdp.html",
    "title": "GPD",
    "section": "",
    "text": "EDA\n\n\nShow the code\ngdp <- read.csv('data/Employment59.csv')\ngdp$DATE <- as.Date(gdp$DATE)\ngdp_ts <- ts(gdp$PAYEMS, star= c(1959,1),frequency = 4)\nautoplot(gdp_ts)+ggtitle(\"US GDP\")\n\n\n\n\n\n\n\nShow the code\nacf(gdp_ts)\n\n\n\n\n\n\n\nShow the code\ndec2=decompose(gdp_ts,type = \"multiplicative\")\nplot(dec2)\n\n\n\n\n\n\n\nShow the code\ngdp_ts %>% diff() %>% ggtsdisplay() #first ordinary differencing\n\n\n\n\n\n\n\nShow the code\ngdp_ts %>% diff(lag = 4) %>% ggtsdisplay() #first ordinary differencing\n\n\n\n\n\n\n\nDetermine Parameters\n\n\nShow the code\ngdp_ts %>% diff(lag=4) %>% diff() %>% ggtsdisplay() #do both\n\n\n\n\n\np:0,1,3 d:2 q:1,2,3 P:1,2,3 D:2 Q:1\n\n\nShow the code\n######################## Check for different combinations ########\n\n\n#write a funtion\nSARIMA.c=function(p1,p2,q1,q2,P1,P2,Q1,Q2,data){\n  \n  #K=(p2+1)*(q2+1)*(P2+1)*(Q2+1)\n  \n  temp=c()\n  d=1\n  D=1\n  s=12\n  \n  i=1\n  temp= data.frame()\n  ls=matrix(rep(NA,9*35),nrow=35)\n  \n  \n  for (p in p1:p2)\n  {\n    for(q in q1:q2)\n    {\n      for(P in P1:P2)\n      {\n        for(Q in Q1:Q2)\n        {\n          if(p+d+q+P+D+Q<=9)\n          {\n            \n            model<- Arima(data,order=c(p-1,d,q-1),seasonal=c(P-1,D,Q-1))\n            ls[i,]= c(p-1,d,q-1,P-1,D,Q-1,model$aic,model$bic,model$aicc)\n            i=i+1\n            #print(i)\n            \n          }\n          \n        }\n      }\n    }\n    \n  }\n  \n  \n  temp= as.data.frame(ls)\n  names(temp)= c(\"p\",\"d\",\"q\",\"P\",\"D\",\"Q\",\"AIC\",\"BIC\",\"AICc\")\n  \n  temp\n  \n}\n\noutput = SARIMA.c(p1=1,p2=4,q1=1,q2=4,P1=1,P2=3,Q1=1,Q2=2, data = gdp_ts)\n#output\n\nknitr::kable(output)\n\n\n\n\n\np\nd\nq\nP\nD\nQ\nAIC\nBIC\nAICc\n\n\n\n\n0\n1\n0\n0\n1\n0\n4498.257\n4501.782\n4498.273\n\n\n0\n1\n0\n0\n1\n1\n4348.055\n4355.106\n4348.104\n\n\n0\n1\n0\n1\n1\n0\n4439.336\n4446.387\n4439.384\n\n\n0\n1\n0\n1\n1\n1\n4350.031\n4360.607\n4350.128\n\n\n0\n1\n0\n2\n1\n0\n4379.073\n4389.649\n4379.170\n\n\n0\n1\n0\n2\n1\n1\n4350.737\n4364.839\n4350.900\n\n\n0\n1\n1\n0\n1\n0\n4496.142\n4503.193\n4496.191\n\n\n0\n1\n1\n0\n1\n1\n4348.392\n4358.968\n4348.489\n\n\n0\n1\n1\n1\n1\n0\n4439.612\n4450.188\n4439.709\n\n\n0\n1\n1\n1\n1\n1\n4350.294\n4364.396\n4350.456\n\n\n0\n1\n1\n2\n1\n0\n4381.060\n4395.162\n4381.223\n\n\n0\n1\n2\n0\n1\n0\n4488.150\n4498.727\n4488.248\n\n\n0\n1\n2\n0\n1\n1\n4349.256\n4363.357\n4349.418\n\n\n0\n1\n2\n1\n1\n0\n4440.078\n4454.180\n4440.241\n\n\n0\n1\n3\n0\n1\n0\n4478.378\n4492.480\n4478.541\n\n\n1\n1\n0\n0\n1\n0\n4495.859\n4502.910\n4495.907\n\n\n1\n1\n0\n0\n1\n1\n4348.206\n4358.782\n4348.303\n\n\n1\n1\n0\n1\n1\n0\n4439.428\n4450.005\n4439.525\n\n\n1\n1\n0\n1\n1\n1\n4350.101\n4364.203\n4350.264\n\n\n1\n1\n0\n2\n1\n0\n4381.057\n4395.159\n4381.220\n\n\n1\n1\n1\n0\n1\n0\n4497.843\n4508.419\n4497.940\n\n\n1\n1\n1\n0\n1\n1\n4350.038\n4364.139\n4350.200\n\n\n1\n1\n1\n1\n1\n0\n4441.326\n4455.428\n4441.489\n\n\n1\n1\n2\n0\n1\n0\n4489.217\n4503.319\n4489.380\n\n\n2\n1\n0\n0\n1\n0\n4497.803\n4508.379\n4497.900\n\n\n2\n1\n0\n0\n1\n1\n4349.683\n4363.785\n4349.845\n\n\n2\n1\n0\n1\n1\n0\n4441.003\n4455.105\n4441.165\n\n\n2\n1\n1\n0\n1\n0\n4499.678\n4513.780\n4499.841\n\n\n3\n1\n0\n0\n1\n0\n4494.238\n4508.340\n4494.400\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\n\n\n\n\nShow the code\noutput[which.min(output$AIC),] \n\n\n  p d q P D Q      AIC      BIC     AICc\n2 0 1 0 0 1 1 4348.055 4355.106 4348.104\n\n\n\n\nShow the code\noutput[which.min(output$BIC),] \n\n\n  p d q P D Q      AIC      BIC     AICc\n2 0 1 0 0 1 1 4348.055 4355.106 4348.104\n\n\n\n\nShow the code\noutput[which.min(output$AICc),] \n\n\n  p d q P D Q      AIC      BIC     AICc\n2 0 1 0 0 1 1 4348.055 4355.106 4348.104\n\n\n\n\nShow the code\nset.seed(123)\nmodel_output <- capture.output(sarima(gdp_ts, 0,1,0,0,1,1,4))\n\n\n\n\n\n\n\nShow the code\ncat(model_output[21:50], model_output[length(model_output)], sep = \"\\n\")\n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    include.mean = !no.constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n         sma1\n      -0.9492\ns.e.   0.0345\n\nsigma^2 estimated as 1852941:  log likelihood = -2172.03,  aic = 4348.06\n\n$degrees_of_freedom\n[1] 250\n\n$ttable\n     Estimate     SE  t.value p.value\nsma1  -0.9492 0.0345 -27.4916       0\n\n$AIC\n[1] 17.32293\n\n$AICc\n[1] 17.32299\n\n$BIC\n[1] 17.35102\n\n\n\n\nModel Fitting\n\n\nShow the code\nfit <- Arima(gdp_ts, order=c(0,1,0), seasonal=c(0,1,1))\nsummary(fit)\n\n\nSeries: gdp_ts \nARIMA(0,1,0)(0,1,1)[4] \n\nCoefficients:\n         sma1\n      -0.9492\ns.e.   0.0345\n\nsigma^2 = 1860405:  log likelihood = -2172.03\nAIC=4348.06   AICc=4348.1   BIC=4355.11\n\nTraining set error measures:\n                   ME     RMSE      MAE       MPE      MAPE      MASE\nTraining set 3.482494 1347.888 492.0962 0.0137906 0.4616182 0.2031885\n                    ACF1\nTraining set -0.08526289\n\n\nforecasting\n\n\nShow the code\nfit %>% forecast(h=12) %>% autoplot() #next 3 years\n\n\n\n\n\n\n\nShow the code\nsarima.for(gdp_ts, 12, 0,1,0,0,1,1,4)\n\n\n\n\n\n$pred\n         Qtr1     Qtr2     Qtr3     Qtr4\n2023 154768.4 154437.3 155221.5 155790.7\n2024 156273.1 155942.0 156726.2 157295.5\n2025 157777.8 157446.7 158231.0 158800.2\n\n$se\n         Qtr1     Qtr2     Qtr3     Qtr4\n2023 1361.322 1925.194 2357.869 2722.631\n2024 3075.622 3392.066 3681.408 3949.611\n2025 4224.838 4483.186 4727.437 4959.674\n\n\n\n\nCompare with Benchmark methods\n\n\nShow the code\nfit <- Arima(gdp_ts, order=c(0,1,0), seasonal=c(0,1,1))\n\nautoplot(gdp_ts) +\n  autolayer(meanf(gdp_ts, h=12),\n            series=\"Mean\", PI=FALSE) +\n  autolayer(naive(gdp_ts, h=12),\n            series=\"Naïve\", PI=FALSE) +\n  autolayer(snaive(gdp_ts, h=12),\n            series=\"SNaïve\", PI=FALSE)+\n  autolayer(rwf(gdp_ts, h=12, drift=TRUE),\n            series=\"Drift\", PI=FALSE)+\n  autolayer(forecast(fit,12), \n            series=\"fit\",PI=FALSE) +\n  guides(colour=guide_legend(title=\"Forecast\"))\n\n\n\n\n\n\n\nShow the code\nf2 <- snaive(gdp_ts, h=12) \n\naccuracy(f2)\n\n\n                   ME     RMSE     MAE      MPE     MAPE MASE      ACF1\nTraining set 1575.474 3048.225 2421.87 1.628746 2.330202    1 0.7384914\n\n\n\n\nShow the code\nsummary(fit)\n\n\nSeries: gdp_ts \nARIMA(0,1,0)(0,1,1)[4] \n\nCoefficients:\n         sma1\n      -0.9492\ns.e.   0.0345\n\nsigma^2 = 1860405:  log likelihood = -2172.03\nAIC=4348.06   AICc=4348.1   BIC=4355.11\n\nTraining set error measures:\n                   ME     RMSE      MAE       MPE      MAPE      MASE\nTraining set 3.482494 1347.888 492.0962 0.0137906 0.4616182 0.2031885\n                    ACF1\nTraining set -0.08526289\n\n\n\n\nCross Validation\n\n\nShow the code\nk <- 75 # minimum data length for fitting a model  n*0.3\nn <- length(gdp_ts)\n\ni=1\nerr1 = c()\n#err2 = c()\n\nfor(i in 1:(n-k))\n{\n  xtrain <- gdp_ts[1:(k-1)+i] #observations from 1 to 75\n  xtest <- gdp_ts[k+i] #76th observation as the test set\n  \n  # Arima(gdp_ts, order=c(0,1,0), seasonal=c(0,1,1))\n  fit <- Arima(xtrain, order=c(0,1,0), seasonal=c(0,1,1),include.drift=FALSE, method=\"ML\")\n  fcast1 <- forecast(fit, h=1)\n  \n  #capture error for each iteration\n  # This is mean absolute error\n  err1 = c(err1, abs(fcast1$mean-xtest)) \n  #err2 = c(err2, abs(fcast2$mean-xtest))\n  \n  # This is mean squared error\n  err3 = c(err1, (fcast1$mean-xtest)^2)\n  #err4 = c(err2, (fcast2$mean-xtest)^2)\n  \n}\n\n(MAE1=mean(err1)) # This is mean absolute error\n\n\n[1] 771.8619\n\n\n\n\nShow the code\nMSE1=mean(err1)\nMSE1\n\n\n[1] 771.8619"
  },
  {
    "objectID": "model_lly.html",
    "href": "model_lly.html",
    "title": "ARMA/ARIMA/SARIMA Models for LLY",
    "section": "",
    "text": "Step 1: Determine the stationality of time series\nStationality is a pre-requirement of training ARIMA model. This is because term ‘Auto Regressive’ in ARIMA means it is a linear regression model that uses its own lags as predictors, which work best when the predictors are not correlated and are independent of each other. Stationary time series make sure the statistical properties of time series do not change over time.\nBased on information obtained from both ACF graphs and Augmented Dickey-Fuller Test, the time series data is non-stationary.\n\n\nShow the code\nLLY_acf <- ggAcf(LLY_ts,100)+ggtitle(\"LLY ACF Plot\")\n\nLLY_pacf <- ggPacf(LLY_ts,100)+ggtitle(\"PACF Plot for UHNs\")\ngrid.arrange(LLY_acf, LLY_pacf,nrow=2)\n\n\n\n\n\n\n\nShow the code\ntseries::adf.test(LLY_ts)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  LLY_ts\nDickey-Fuller = -2.8006, Lag order = 14, p-value = 0.2393\nalternative hypothesis: stationary\n\n\n\n\nStep 2: Eliminate Non-Stationality\nSince this data is non-stationary, it is important to necessary to convert it to stationary time series. This step employs a series of actions to eliminate non-stationality, i.e. differencing the data. It turns out the 1st differened data has shown good stationary property, there are no need to go further at 2nd differencing. What is more, the Augmented Dickey-Fuller Test also confirmed that the 1st differenced data is stationary. Therefore, the1st differencing would be the actions taken to eliminate the non-stationality.\n\n\nShow the code\nplot1<- ggAcf((LLY_ts) %>%diff(), 50, main=\"ACF Plot for 1st differenced Data\") \nplot2<- ggAcf((LLY_ts) %>%diff()%>%diff(),50, main=\"ACF Plot for 2nd differenced Data\") \n\ngrid.arrange(plot1, plot2,nrow=2)\n\n\n\n\n\n\n\nShow the code\ntseries::adf.test((LLY_ts) %>%diff())\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  (LLY_ts) %>% diff()\nDickey-Fuller = -14.381, Lag order = 14, p-value = 0.01\nalternative hypothesis: stationary\n\n\n\n\nStep 3: Determine p,d,q Parameters\nThe standard notation of ARIMA(p,d,q) include p,d,q 3 parameters. Here are the representations: - p: The number of lag observations included in the model, also called the lag order; order of the AR term. - d: The number of times that the raw observations are differenced, also called the degree of differencing; number of differencing required to make the time series stationary. - q: order of moving average; order of the MA term. It refers to the number of lagged forecast errors that should go into the ARIMA Model.\n\n\nShow the code\nplot3<- ggPacf((LLY_ts) %>%diff(),50, main=\"PACF Plot for Log Transformed & 1st differenced Data\") \n\ngrid.arrange(plot1,plot3)\n\n\n\n\n\nAccording to the PACF plot and ACF plot above, there are no significant peaks on both graph, therefore, here set both p and q as 0. Since I only differenced the data once, the d would be 1.\n\n\nStep 4: Fit ARIMA(p,d,q) model\n\n\nShow the code\nfit1 <- Arima((LLY_ts), order=c(0, 1, 0),include.drift = TRUE) \nsummary(fit1)\n\n\nSeries: (LLY_ts) \nARIMA(0,1,0) with drift \n\nCoefficients:\n       drift\n      0.0183\ns.e.  0.1283\n\nsigma^2 = 53.71:  log likelihood = -11128.73\nAIC=22261.47   AICc=22261.47   BIC=22273.65\n\nTraining set error measures:\n                       ME     RMSE      MAE        MPE     MAPE       MASE\nTraining set 2.349624e-05 7.326456 2.206738 -0.1331229 1.443189 0.02767164\n                    ACF1\nTraining set 0.004971149\n\n\n\nModel Diagnostics\n\nInspection of the time plot of the standardized residuals below shows no obvious patterns.\nNotice that there may be outliers, with a few values exceeding 3 standard deviations in magnitude.\nThe ACF of the standardized residuals shows no apparent departure from the model assumptions, no significant lags shown.\nThe normal Q-Q plot of the residuals shows that the assumption of normality is reasonable, with the exception of the fat-tailed.\nThe model appears to fit well.\n\n\n\nShow the code\nmodel_output <- capture.output(sarima((LLY_ts), 0,1,0))\n\n\n\n\n\n\n\nShow the code\ncat(model_output[9:37], model_output[length(model_output)], sep = \"\\n\") #to get rid of the convergence status and details of the optimization algorithm used by the sarima() \n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n      constant\n        0.0183\ns.e.    0.1283\n\nsigma^2 estimated as 53.69:  log likelihood = -11128.73,  aic = 22261.47\n\n$degrees_of_freedom\n[1] 3262\n\n$ttable\n         Estimate     SE t.value p.value\nconstant   0.0183 0.1283   0.143  0.8863\n\n$AIC\n[1] 6.822393\n\n$AICc\n[1] 6.822394\n\n$BIC\n[1] 6.826126\n\n\n\n\nCompare with auto.arima() function\nauto.arima() returns best ARIMA model according to either AIC, AICc or BIC value. The function conducts a search over possible model within the order constraints provided. The parameters matches with the model.\n\n\nShow the code\nauto.arima((LLY_ts))\n\n\nSeries: (LLY_ts) \nARIMA(0,1,0) \n\nsigma^2 = 53.69:  log likelihood = -11128.74\nAIC=22259.49   AICc=22259.49   BIC=22265.58\n\n\n\n\n\nStep 5: Forecast\nThe blue part in graph below forecast the next 100 values of LLY stock price in 80% and 95% confidence level.\n\n\nShow the code\nLLY_ts %>%\n  Arima(order=c(0,1,0),include.drift = TRUE) %>%\n  forecast(100) %>%\n  autoplot() +\n  ylab(\"LLY stock prices prediction\") + xlab(\"Year\")\n\n\n\n\n\n\n\nStep 6: Compare ARIMA model with the benchmark methods\nForecasting benchmarks are very important when testing new forecasting methods, to see how well they perform against some simple alternatives.\n\nAverage method\nHere, the forecast of all future values are equal to the average of the historical data. The residual plot of this method is not stationary.\n\n\nShow the code\nf1<-meanf((LLY_ts), h=251) #mean\n#summary(f1)\ncheckresiduals(f1)#serial correlation ; Lung Box p <0.05\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Mean\nQ* = 300420, df = 501, p-value < 2.2e-16\n\nModel df: 1.   Total lags used: 502\n\n\n\n\nNaive method\nThis method simply set all forecasts to be the value of the last observation. According to error measurement here, ARIMA(4,1,3) outperform the average method.\n\n\nShow the code\nf2<-naive((LLY_ts), h=11) # naive method\nsummary(f2)\n\n\n\nForecast method: Naive method\n\nModel Information:\nCall: naive(y = (LLY_ts), h = 11) \n\nResidual sd: 7.3276 \n\nError measures:\n                     ME     RMSE      MAE        MPE     MAPE       MASE\nTraining set 0.01834098 7.327602 2.208937 -0.1191376 1.444964 0.02769921\n                    ACF1\nTraining set 0.004970975\n\nForecasts:\n         Point Forecast    Lo 80    Hi 80     Lo 95    Hi 95\n2023.004       136.5567 127.1660 145.9474 122.19488 150.9186\n2023.008       136.5567 123.2763 149.8372 116.24601 156.8674\n2023.012       136.5567 120.2915 152.8219 111.68129 161.4321\n2023.016       136.5567 117.7753 155.3381 107.83305 165.2804\n2023.020       136.5567 115.5585 157.5550 104.44268 168.6708\n2023.024       136.5567 113.5543 159.5591 101.37755 171.7359\n2023.028       136.5567 111.7113 161.4022  98.55887 174.5546\n2023.032       136.5567 109.9958 163.1176  95.93531 177.1781\n2023.036       136.5567 108.3846 164.7288  93.47121 179.6422\n2023.040       136.5567 106.8607 166.2527  91.14061 181.9728\n2023.044       136.5567 105.4113 167.7021  88.92390 184.1895\n\n\nShow the code\ncheckresiduals(f2)#serial correlation ; Lung Box p <0.05\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Naive method\nQ* = 438.09, df = 502, p-value = 0.9816\n\nModel df: 0.   Total lags used: 502\n\n\n\n\nSeasonal naive method\nThis method is useful for highly seasonal data, which can set each forecast to be equal to the last observed value from the same season of the year. Here seasonal naive is used to forecast the next 4 values for the LLY stock price series.\n\n\nShow the code\nf3<-snaive((LLY_ts), h=4) #seasonal naive method\nsummary(f3)\n\n\n\nForecast method: Seasonal naive method\n\nModel Information:\nCall: snaive(y = (LLY_ts), h = 4) \n\nResidual sd: 108.2255 \n\nError measures:\n                   ME     RMSE      MAE       MPE     MAPE MASE      ACF1\nTraining set 4.249799 108.2255 79.74729 -23.12158 65.75914    1 0.9950134\n\nForecasts:\n         Point Forecast     Lo 80    Hi 80     Lo 95    Hi 95\n2023.004       105.4368 -33.25979 244.1334 -106.6813 317.5550\n2023.008       106.4282 -32.26840 245.1248 -105.6899 318.5464\n2023.012       107.5895 -31.10708 246.2861 -104.5286 319.7077\n2023.016       106.2299 -32.46667 244.9265 -105.8882 318.3481\n\n\nShow the code\ncheckresiduals(f3) #serial correlation ; Lung Box p <0.05\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Seasonal naive method\nQ* = 235237, df = 502, p-value < 2.2e-16\n\nModel df: 0.   Total lags used: 502\n\n\n\n\nDrift Method\nA variation on the naïve method is to allow the forecasts to increase or decrease over time, where the amount of change over time is set to be the average change seen in the historical data.\n\n\nShow the code\nf4 <- rwf((LLY_ts),drift=TRUE, h=20) \nsummary(f4)\n\n\n\nForecast method: Random walk with drift\n\nModel Information:\nCall: rwf(y = (LLY_ts), h = 20, drift = TRUE) \n\nDrift: 0.0183  (se 0.1283)\nResidual sd: 7.3287 \n\nError measures:\n                       ME     RMSE      MAE        MPE     MAPE       MASE\nTraining set 5.431461e-15 7.327579 2.207391 -0.1331943 1.443601 0.02767983\n                    ACF1\nTraining set 0.004970975\n\nForecasts:\n         Point Forecast     Lo 80    Hi 80     Lo 95    Hi 95\n2023.004       136.5751 127.18151 145.9686 122.20887 150.9413\n2023.008       136.5934 123.30688 149.8799 116.27342 156.9134\n2023.012       136.6117 120.33665 152.8868 111.72114 161.5023\n2023.016       136.6301 117.83435 155.4258 107.88449 165.3757\n2023.020       136.6484 115.63094 157.6659 104.50496 168.7919\n2023.024       136.6668 113.63975 159.6938 101.44998 171.8835\n2023.028       136.6851 111.80928 161.5609  98.64081 174.7294\n2023.032       136.7034 110.10600 163.3009  96.02617 177.3807\n2023.036       136.7218 108.50663 164.9369  93.57042 179.8731\n2023.040       136.7401 106.99419 166.4861  91.24765 182.2326\n2023.044       136.7585 105.55590 167.9610  89.03826 184.4787\n2023.048       136.7768 104.18182 169.3718  86.92707 186.6265\n2023.052       136.7951 102.86403 170.7263  84.90198 188.6883\n2023.056       136.8135 101.59613 172.0309  82.95318 190.6738\n2023.060       136.8318 100.37284 173.2908  81.07261 192.5911\n2023.064       136.8502  99.18974 174.5106  79.25351 194.4468\n2023.068       136.8685  98.04311 175.6939  77.49018 196.2468\n2023.072       136.8869  96.92975 176.8440  75.77774 197.9960\n2023.076       136.9052  95.84692 177.9635  74.11198 199.6984\n2023.080       136.9235  94.79222 179.0549  72.48925 201.3578\n\n\nShow the code\ncheckresiduals(f4)\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Random walk with drift\nQ* = 438.09, df = 501, p-value = 0.9801\n\nModel df: 1.   Total lags used: 502\n\n\n\n\nShow the code\nautoplot(LLY_ts) +\n  autolayer(meanf(LLY_ts, h=100),\n            series=\"Mean.tr\", PI=FALSE) +\n  autolayer(naive((LLY_ts), h=100),\n            series=\"Naïve.tr\", PI=FALSE) +\n  autolayer(rwf((LLY_ts), drift=TRUE, h=100),\n            series=\"Drift.tr\", PI=FALSE) +\n  autolayer(forecast(fit1,100), \n            series=\"fit\",PI=FALSE) +\n  ggtitle(\"LLY Stock Price\") +\n  xlab(\"Time\") + ylab(\"Price\") +\n  guides(colour=guide_legend(title=\"Forecast\"))\n\n\n\n\n\nAccording to the graph above, ARIMA(0,1,0) outperform most of benchmark method, though its performance is very similar to drift method."
  },
  {
    "objectID": "model_PFE.html",
    "href": "model_PFE.html",
    "title": "ARMA/ARIMA/SARIMA Models for PFE",
    "section": "",
    "text": "Step 1: Determine the stationality of time series\nStationality is a pre-requirement of training ARIMA model. This is because term ‘Auto Regressive’ in ARIMA means it is a linear regression model that uses its own lags as predictors, which work best when the predictors are not correlated and are independent of each other. Stationary time series make sure the statistical properties of time series do not change over time.\nBased on information obtained from ACF graphs, the time series data is non-stationary, though Augmented Dickey-Fuller Test shows the series is stationary.\n\n\nShow the code\nPFE_acf <- ggAcf(PFE_ts,100)+ggtitle(\"PFE ACF Plot\")\n\nPFE_pacf <- ggPacf(PFE_ts)+ggtitle(\"PACF Plot for UHNs\")\ngrid.arrange(PFE_acf, PFE_pacf,nrow=2)\n\n\n\n\n\n\n\nShow the code\ntseries::adf.test(PFE_ts)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  PFE_ts\nDickey-Fuller = -3.572, Lag order = 14, p-value = 0.03534\nalternative hypothesis: stationary\n\n\n\n\nStep 2: Eliminate Non-Stationality\nSince this data is non-stationary, it is important to necessary to convert it to stationary time series. This step employs a series of actions to eliminate non-stationality, i.e. log transformation and differencing the data. It turns out the log transformed and 1st differened data has shown good stationary property, there are no need to go further at 2nd differencing. What is more, the Augmented Dickey-Fuller Test also confirmed that the log transformed and 1st differenced data is stationary. Therefore, the log transformation and 1st differencing would be the actions taken to eliminate the non-stationality.\n\n\nShow the code\nplot1<- ggAcf(log(PFE_ts) %>%diff(), 50, main=\"ACF Plot for Log Transformed & 1st differenced Data\") \nplot2<- ggAcf(log(PFE_ts) %>%diff()%>%diff(),50, main=\"ACF Plot for Log Transformed & 2nd differenced Data\") \n\ngrid.arrange(plot1, plot2,nrow=2)\n\n\n\n\n\n\n\nShow the code\ntseries::adf.test(log(PFE_ts) %>%diff())\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  log(PFE_ts) %>% diff()\nDickey-Fuller = -15.533, Lag order = 14, p-value = 0.01\nalternative hypothesis: stationary\n\n\n\n\nStep 3: Determine p,d,q Parameters\nThe standard notation of ARIMA(p,d,q) include p,d,q 3 parameters. Here are the representations: - p: The number of lag observations included in the model, also called the lag order; order of the AR term. - d: The number of times that the raw observations are differenced, also called the degree of differencing; number of differencing required to make the time series stationary. - q: order of moving average; order of the MA term. It refers to the number of lagged forecast errors that should go into the ARIMA Model.\n\n\nShow the code\nplot3<- ggPacf(log(PFE_ts) %>%diff(),50, main=\"PACF Plot for Log Transformed & 1st differenced Data\") \n\ngrid.arrange(plot1,plot3)\n\n\n\n\n\nAccording to the PACF plot and ACF plot above, both plots have significant peak at 1. Therefore, here choose the value of both p and q as 1. Since I only differenced the data once, the d would be 1.\n\n\nStep 4: Fit ARIMA(p,d,q) model\n\n\nShow the code\nfit1 <- Arima(log(PFE_ts), order=c(1, 1, 1),include.drift = TRUE) \nsummary(fit1)\n\n\nSeries: log(PFE_ts) \nARIMA(1,1,1) with drift \n\nCoefficients:\n          ar1     ma1  drift\n      -0.1091  0.0608  5e-04\ns.e.   0.2342  0.2348  2e-04\n\nsigma^2 = 0.0001835:  log likelihood = 9408.16\nAIC=-18808.31   AICc=-18808.3   BIC=-18783.95\n\nTraining set error measures:\n                      ME       RMSE         MAE           MPE      MAPE\nTraining set 3.19883e-06 0.01353642 0.009508371 -0.0009318948 0.3049848\n                   MASE          ACF1\nTraining set 0.05959479 -0.0001026419\n\n\n\nModel Diagnostics\n\nInspection of the time plot of the standardized residuals below shows no obvious patterns.\nNotice that there may be outliers, with a few values exceeding 3 standard deviations in magnitude.\nThe ACF of the standardized residuals shows no apparent departure from the model assumptions, no significant lags shown.\nThe normal Q-Q plot of the residuals shows that the assumption of normality is reasonable, with the exception of the fat-tailed.\nThe model appears to fit well.\n\n\n\nShow the code\nmodel_output <- capture.output(sarima(log(PFE_ts), 1,1,1))\n\n\n\n\n\n\n\nShow the code\ncat(model_output[19:50], model_output[length(model_output)], sep = \"\\n\") #to get rid of the convergence status and details of the optimization algorithm used by the sarima() \n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n          ar1     ma1  constant\n      -0.1091  0.0608     5e-04\ns.e.   0.2342  0.2348     2e-04\n\nsigma^2 estimated as 0.0001833:  log likelihood = 9408.16,  aic = -18808.31\n\n$degrees_of_freedom\n[1] 3260\n\n$ttable\n         Estimate     SE t.value p.value\nar1       -0.1091 0.2342 -0.4660  0.6412\nma1        0.0608 0.2348  0.2588  0.7958\nconstant   0.0005 0.0002  2.0512  0.0403\n\n$AIC\n[1] -5.764117\n\n$AICc\n[1] -5.764115\n\n$BIC\n[1] -5.756651\n\n\n\n\nCompare with auto.arima() function\nauto.arima() returns best ARIMA model according to either AIC, AICc or BIC value. The function conducts a search over possible model within the order constraints provided. However, this method is not reliable sometimes. It fits a different model than ACF/PACF plots suggest. This is because auto.arima() usually return models that are more complex as it prefers more parameters compared than to the for example BIC.\n\n\nShow the code\nauto.arima(log(PFE_ts))\n\n\nSeries: log(PFE_ts) \nARIMA(1,1,0) with drift \n\nCoefficients:\n          ar1  drift\n      -0.0486  5e-04\ns.e.   0.0175  2e-04\n\nsigma^2 = 0.0001834:  log likelihood = 9408.13\nAIC=-18810.26   AICc=-18810.25   BIC=-18791.98\n\n\n\n\n\nStep 5: Forecast\nThe blue part in graph below forecast the next 100 values of PFE stock price in 80% and 95% confidence level.\n\n\nShow the code\nPFE_ts %>%\n  Arima(order=c(1,1,1),include.drift = TRUE) %>%\n  forecast(100) %>%\n  autoplot() +\n  ylab(\"PFE stock prices prediction\") + xlab(\"Year\")\n\n\n\n\n\n\n\nStep 6: Compare ARIMA model with the benchmark methods\nForecasting benchmarks are very important when testing new forecasting methods, to see how well they perform against some simple alternatives.\n\nAverage method\nHere, the forecast of all future values are equal to the average of the historical data. The residual plot of this method is not stationary.\n\n\nShow the code\nf1<-meanf(log(PFE_ts), h=251) #mean\n#summary(f1)\ncheckresiduals(f1)#serial correlation ; Lung Box p <0.05\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Mean\nQ* = 869974, df = 501, p-value < 2.2e-16\n\nModel df: 1.   Total lags used: 502\n\n\n\n\nNaive method\nThis method simply set all forecasts to be the value of the last observation. According to error measurement here, ARIMA(1,1,1) outperform the average method.\n\n\nShow the code\nf2<-naive(log(PFE_ts), h=11) # naive method\nsummary(f2)\n\n\n\nForecast method: Naive method\n\nModel Information:\nCall: naive(y = log(PFE_ts), h = 11) \n\nResidual sd: 0.0136 \n\nError measures:\n                       ME       RMSE         MAE        MPE      MAPE\nTraining set 0.0004687069 0.01356269 0.009523496 0.01416351 0.3054974\n                   MASE        ACF1\nTraining set 0.05968959 -0.04862067\n\nForecasts:\n         Point Forecast    Lo 80    Hi 80    Lo 95    Hi 95\n2023.004       3.929721 3.912340 3.947102 3.903139 3.956303\n2023.008       3.929721 3.905140 3.954302 3.892128 3.967314\n2023.012       3.929721 3.899616 3.959826 3.883679 3.975763\n2023.016       3.929721 3.894959 3.964484 3.876556 3.982886\n2023.020       3.929721 3.890855 3.968587 3.870281 3.989161\n2023.024       3.929721 3.887146 3.972296 3.864608 3.994834\n2023.028       3.929721 3.883735 3.975708 3.859391 4.000051\n2023.032       3.929721 3.880559 3.978883 3.854535 4.004907\n2023.036       3.929721 3.877577 3.981865 3.849974 4.009468\n2023.040       3.929721 3.874757 3.984686 3.845660 4.013782\n2023.044       3.929721 3.872074 3.987368 3.841557 4.017885\n\n\nShow the code\ncheckresiduals(f2)#serial correlation ; Lung Box p <0.05\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Naive method\nQ* = 599.92, df = 502, p-value = 0.001694\n\nModel df: 0.   Total lags used: 502\n\n\n\n\nSeasonal naive method\nThis method is useful for highly seasonal data, which can set each forecast to be equal to the last observed value from the same season of the year. Here seasonal naive is used to forecast the next 4 values for the PFE stock price series.\n\n\nShow the code\nf3<-snaive(log(PFE_ts), h=4) #seasonal naive method\nsummary(f3)\n\n\n\nForecast method: Seasonal naive method\n\nModel Information:\nCall: snaive(y = log(PFE_ts), h = 4) \n\nResidual sd: 0.1904 \n\nError measures:\n                    ME      RMSE       MAE      MPE   MAPE MASE      ACF1\nTraining set 0.1322592 0.1903849 0.1595504 4.188333 5.0082    1 0.9895735\n\nForecasts:\n         Point Forecast    Lo 80    Hi 80    Lo 95    Hi 95\n2023.004       4.035591 3.791603 4.279579 3.662443 4.408738\n2023.008       4.045718 3.801730 4.289706 3.672570 4.418865\n2023.012       4.031511 3.787523 4.275499 3.658364 4.404659\n2023.016       4.039823 3.795835 4.283811 3.666675 4.412970\n\n\nShow the code\ncheckresiduals(f3) #serial correlation ; Lung Box p <0.05\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Seasonal naive method\nQ* = 194683, df = 502, p-value < 2.2e-16\n\nModel df: 0.   Total lags used: 502\n\n\n\n\nDrift Method\nA variation on the naïve method is to allow the forecasts to increase or decrease over time, where the amount of change over time is set to be the average change seen in the historical data.\n\n\nShow the code\nf4 <- rwf(log(PFE_ts),drift=TRUE, h=20) \nsummary(f4)\n\n\n\nForecast method: Random walk with drift\n\nModel Information:\nCall: rwf(y = log(PFE_ts), h = 20, drift = TRUE) \n\nDrift: 5e-04  (se 2e-04)\nResidual sd: 0.0136 \n\nError measures:\n                        ME       RMSE        MAE          MPE      MAPE\nTraining set -1.352823e-16 0.01355458 0.00952708 -0.001001816 0.3056554\n                   MASE        ACF1\nTraining set 0.05971205 -0.04862067\n\nForecasts:\n         Point Forecast    Lo 80    Hi 80    Lo 95    Hi 95\n2023.004       3.930190 3.912814 3.947566 3.903615 3.956764\n2023.008       3.930658 3.906081 3.955236 3.893071 3.968246\n2023.012       3.931127 3.901021 3.961233 3.885084 3.977170\n2023.016       3.931596 3.896827 3.966364 3.878422 3.984770\n2023.020       3.932065 3.893186 3.970943 3.872606 3.991524\n2023.024       3.932533 3.889938 3.975129 3.867389 3.997677\n2023.028       3.933002 3.886987 3.979017 3.862628 4.003376\n2023.032       3.933471 3.884271 3.982671 3.858226 4.008716\n2023.036       3.933939 3.881747 3.986132 3.854118 4.013761\n2023.040       3.934408 3.879384 3.989432 3.850256 4.018560\n2023.044       3.934877 3.877158 3.992595 3.846604 4.023150\n2023.048       3.935346 3.875051 3.995640 3.843133 4.027558\n2023.052       3.935814 3.873048 3.998580 3.839822 4.031806\n2023.056       3.936283 3.871138 4.001428 3.836652 4.035914\n2023.060       3.936752 3.869310 4.004194 3.833608 4.039895\n2023.064       3.937220 3.867556 4.006885 3.830678 4.043763\n2023.068       3.937689 3.865870 4.009508 3.827851 4.047527\n2023.072       3.938158 3.864245 4.012071 3.825118 4.051198\n2023.076       3.938626 3.862677 4.014576 3.822471 4.054782\n2023.080       3.939095 3.861161 4.017030 3.819904 4.058286\n\n\nShow the code\ncheckresiduals(f4)\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Random walk with drift\nQ* = 599.92, df = 501, p-value = 0.001534\n\nModel df: 1.   Total lags used: 502\n\n\n\n\nShow the code\nautoplot(PFE_ts) +\n  autolayer(meanf(PFE_ts, h=100),\n            series=\"Mean.tr\", PI=FALSE) +\n  autolayer(naive((PFE_ts), h=100),\n            series=\"Naïve.tr\", PI=FALSE) +\n  autolayer(rwf((PFE_ts), drift=TRUE, h=100),\n            series=\"Drift.tr\", PI=FALSE) +\n  autolayer(forecast(Arima((PFE_ts), order=c(1, 1, 1),include.drift = TRUE),100), \n            series=\"fit\",PI=FALSE) +\n  ggtitle(\"PFE Stock Price\") +\n  xlab(\"Time\") + ylab(\"Log(Price)\") +\n  guides(colour=guide_legend(title=\"Forecast\"))\n\n\n\n\n\nAccording to the graph above, ARIMA(1,1,1) outperform most of benchmark method, though its performance is very similar to drift method."
  },
  {
    "objectID": "model_unh.html",
    "href": "model_unh.html",
    "title": "ARMA/ARIMA/SARIMA Models for UNH",
    "section": "",
    "text": "Step 1: Determine the stationality of time series\nStationality is a pre-requirement of training ARIMA model. This is because term ‘Auto Regressive’ in ARIMA means it is a linear regression model that uses its own lags as predictors, which work best when the predictors are not correlated and are independent of each other. Stationary time series make sure the statistical properties of time series do not change over time.\nBased on information obtained from both ACF graphs and Augmented Dickey-Fuller Test, the time series data is non-stationary.\n\n\nShow the code\nUNH_acf <- ggAcf(UNH_ts,100)+ggtitle(\"UNH ACF Plot\")\n\nUNH_pacf <- ggPacf(UNH_ts)+ggtitle(\"PACF Plot for UHNs\")\ngrid.arrange(UNH_acf, UNH_pacf,nrow=2)\n\n\n\n\n\n\n\nShow the code\ntseries::adf.test(UNH_ts)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  UNH_ts\nDickey-Fuller = -1.0864, Lag order = 14, p-value = 0.9248\nalternative hypothesis: stationary\n\n\n\n\nStep 2: Eliminate Non-Stationality\nSince this data is non-stationary, it is important to necessary to convert it to stationary time series. This step employs a series of actions to eliminate non-stationality, i.e. log transformation and differencing the data. It turns out the log transformed and 1st differened data has shown good stationary property, there are no need to go further at 2nd differencing. What is more, the Augmented Dickey-Fuller Test also confirmed that the log transformed and 1st differenced data is stationary. Therefore, the log transformation and 1st differencing would be the actions taken to eliminate the non-stationality.\n\n\nShow the code\nplot1<- ggAcf(log(UNH_ts) %>%diff(), 50, main=\"ACF Plot for Log Transformed & 1st differenced Data\") \nplot2<- ggAcf(log(UNH_ts) %>%diff()%>%diff(),50, main=\"ACF Plot for Log Transformed & 2nd differenced Data\") \n\ngrid.arrange(plot1, plot2,nrow=2)\n\n\n\n\n\n\n\nShow the code\ntseries::adf.test(log(UNH_ts) %>%diff())\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  log(UNH_ts) %>% diff()\nDickey-Fuller = -16.952, Lag order = 14, p-value = 0.01\nalternative hypothesis: stationary\n\n\n\n\nStep 3: Determine p,d,q Parameters\nThe standard notation of ARIMA(p,d,q) include p,d,q 3 parameters. Here are the representations: - p: The number of lag observations included in the model, also called the lag order; order of the AR term. - d: The number of times that the raw observations are differenced, also called the degree of differencing; number of differencing required to make the time series stationary. - q: order of moving average; order of the MA term. It refers to the number of lagged forecast errors that should go into the ARIMA Model.\n\n\nShow the code\nplot3<- ggPacf(log(UNH_ts) %>%diff(),50, main=\"PACF Plot for Log Transformed & 1st differenced Data\") \n\ngrid.arrange(plot1,plot3)\n\n\n\n\n\nAccording to the PACF plot and ACF plot above, here choose the range of p value and q value as 1-4 and 1-3, respectively. Since I only differenced the data once, the d would be 1.\n\n\nStep 4: Fit ARIMA(p,d,q) model\nBefore fitting the data with ARIMA(p,d,q) model, we will need to choose the set of parameters based on error measurement and model diagnostics. Based on the result below, ARIMA(4,1,3) has lowest error measurement.\n\n\nShow the code\n######################## Check for different combinations ########\n\n\nd=1\ni=1\ntemp= data.frame()\nls=matrix(rep(NA,6*12),nrow=12) # roughly nrow = 3x4x2\n\n\nfor (p in 2:5)# p=1,2,3,4 : 4\n{\n  for(q in 2:4)# q=1,2,3 :3\n  {\n    for(d in 1:1)# d=1,2 :2\n    {\n      \n      if(p-1+d+q-1<=8)\n      {\n        \n        model<- Arima(log(UNH_ts),order=c(p-1,d,q-1),include.drift=TRUE) \n        ls[i,]= c(p-1,d,q-1,model$aic,model$bic,model$aicc)\n        i=i+1\n        #print(i)\n        \n      }\n      \n    }\n  }\n}\n\ntemp= as.data.frame(ls)\nnames(temp)= c(\"p\",\"d\",\"q\",\"AIC\",\"BIC\",\"AICc\")\n\nkable(temp) %>%\n  kable_styling(font_size = 12)\n\n\n\n\n \n  \n    p \n    d \n    q \n    AIC \n    BIC \n    AICc \n  \n \n\n  \n    1 \n    1 \n    1 \n    -17710.12 \n    -17685.76 \n    -17710.11 \n  \n  \n    1 \n    1 \n    2 \n    -17719.11 \n    -17688.66 \n    -17719.09 \n  \n  \n    1 \n    1 \n    3 \n    -17745.61 \n    -17709.07 \n    -17745.59 \n  \n  \n    2 \n    1 \n    1 \n    -17716.76 \n    -17686.31 \n    -17716.74 \n  \n  \n    2 \n    1 \n    2 \n    -17772.64 \n    -17736.10 \n    -17772.61 \n  \n  \n    2 \n    1 \n    3 \n    -17747.50 \n    -17704.86 \n    -17747.46 \n  \n  \n    3 \n    1 \n    1 \n    -17746.22 \n    -17709.68 \n    -17746.19 \n  \n  \n    3 \n    1 \n    2 \n    -17745.07 \n    -17702.44 \n    -17745.04 \n  \n  \n    3 \n    1 \n    3 \n    -17789.34 \n    -17740.62 \n    -17789.30 \n  \n  \n    4 \n    1 \n    1 \n    -17728.60 \n    -17685.97 \n    -17728.56 \n  \n  \n    4 \n    1 \n    2 \n    -17757.45 \n    -17708.73 \n    -17757.41 \n  \n  \n    4 \n    1 \n    3 \n    -17798.95 \n    -17744.14 \n    -17798.90 \n  \n\n\n\n\n\n\nError measurement\n\n\nShow the code\ntemp[which.min(temp$AIC),] \n\n\n   p d q       AIC       BIC     AICc\n12 4 1 3 -17798.95 -17744.14 -17798.9\n\n\n\n\n   p d q       AIC       BIC     AICc\n12 4 1 3 -17798.95 -17744.14 -17798.9\n\n\n\n\n   p d q       AIC       BIC     AICc\n12 4 1 3 -17798.95 -17744.14 -17798.9\n\n\n\n\nShow the code\nfit1 <- Arima(log(UNH_ts), order=c(4, 1, 3),include.drift = TRUE) \nsummary(fit1)\n\n\nSeries: log(UNH_ts) \nARIMA(4,1,3) with drift \n\nCoefficients:\n          ar1     ar2     ar3      ar4     ma1      ma2      ma3  drift\n      -0.6673  0.7666  0.7124  -0.0768  0.6040  -0.7506  -0.7104  9e-04\ns.e.   0.0485  0.0455  0.0545   0.0218  0.0461   0.0382   0.0512  2e-04\n\nsigma^2 = 0.0002496:  log likelihood = 8908.48\nAIC=-17798.95   AICc=-17798.9   BIC=-17744.14\n\nTraining set error measures:\n                       ME       RMSE        MAE           MPE      MAPE\nTraining set 4.453197e-06 0.01577582 0.01103893 -0.0006003827 0.2395419\n                   MASE          ACF1\nTraining set 0.04432579 -0.0008705228\n\n\n\n\nModel Diagnostics\n\nInspection of the time plot of the standardized residuals below shows no obvious patterns.\nNotice that there may be outliers, with a few values exceeding 3 standard deviations in magnitude.\nThe ACF of the standardized residuals shows no apparent departure from the model assumptions, no significant lags shown.\nThe normal Q-Q plot of the residuals shows that the assumption of normality is reasonable, with the exception of the fat-tailed.\nThe model appears to fit well.\n\n\n\nShow the code\nmodel_output <- capture.output(sarima(log(UNH_ts), 4,1,3))\n\n\n\n\n\n\n\nShow the code\ncat(model_output[91:130], model_output[length(model_output)], sep = \"\\n\") #to get rid of the convergence status and details of the optimization algorithm used by the sarima() \n\n\nfinal  value -4.149087 \nconverged\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n          ar1     ar2     ar3      ar4     ma1      ma2      ma3  constant\n      -0.6673  0.7666  0.7124  -0.0768  0.6040  -0.7506  -0.7104     9e-04\ns.e.   0.0485  0.0455  0.0545   0.0218  0.0461   0.0382   0.0512     2e-04\n\nsigma^2 estimated as 0.0002489:  log likelihood = 8908.48,  aic = -17798.95\n\n$degrees_of_freedom\n[1] 3255\n\n$ttable\n         Estimate     SE  t.value p.value\nar1       -0.6673 0.0485 -13.7699   0e+00\nar2        0.7666 0.0455  16.8610   0e+00\nar3        0.7124 0.0545  13.0760   0e+00\nar4       -0.0768 0.0218  -3.5325   4e-04\nma1        0.6040 0.0461  13.1042   0e+00\nma2       -0.7506 0.0382 -19.6542   0e+00\nma3       -0.7104 0.0512 -13.8675   0e+00\nconstant   0.0009 0.0002   6.1278   0e+00\n\n$AIC\n[1] -5.454781\n\n$AICc\n[1] -5.454768\n\n$BIC\n[1] -5.437983\n\nNA\n\n\n\n\nCompare with auto.arima() function\nauto.arima() returns best ARIMA model according to either AIC, AICc or BIC value. The function conducts a search over possible model within the order constraints provided. However, this method is not reliable sometimes. It fits a different model than ACF/PACF plots suggest. This is because auto.arima() usually return models that are more complex as it prefers more parameters compared than to the for example BIC.\n\n\nShow the code\nauto.arima(log(UNH_ts))\n\n\nSeries: log(UNH_ts) \nARIMA(1,1,0) with drift \n\nCoefficients:\n          ar1  drift\n      -0.0821  9e-04\ns.e.   0.0174  3e-04\n\nsigma^2 = 0.0002582:  log likelihood = 8850.38\nAIC=-17694.76   AICc=-17694.76   BIC=-17676.49\n\n\n\n\n\nStep 5: Forecast\nThe blue part in graph below forecast the next 100 values of UNH stock price in 80% and 95% confidence level.\n\n\nShow the code\nlog(UNH_ts) %>%\n  Arima(order=c(4,1,3),include.drift = TRUE) %>%\n  forecast(100) %>%\n  autoplot() +\n  ylab(\"UNH stock prices prediction\") + xlab(\"Year\")\n\n\n\n\n\n\n\nStep 6: Compare ARIMA model with the benchmark methods\nForecasting benchmarks are very important when testing new forecasting methods, to see how well they perform against some simple alternatives.\n\nAverage method\nHere, the forecast of all future values are equal to the average of the historical data. The residual plot of this method is not stationary.\n\n\nShow the code\nf1<-meanf(log(UNH_ts), h=251) #mean\n#summary(f1)\ncheckresiduals(f1)#serial correlation ; Lung Box p <0.05\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Mean\nQ* = 1046300, df = 501, p-value < 2.2e-16\n\nModel df: 1.   Total lags used: 502\n\n\n\n\nNaive method\nThis method simply set all forecasts to be the value of the last observation. According to error measurement here, ARIMA(4,1,3) outperform the average method.\n\n\nShow the code\nf2<-naive(log(UNH_ts), h=11) # naive method\nsummary(f2)\n\n\n\nForecast method: Naive method\n\nModel Information:\nCall: naive(y = log(UNH_ts), h = 11) \n\nResidual sd: 0.0161 \n\nError measures:\n                       ME       RMSE        MAE        MPE      MAPE       MASE\nTraining set 0.0009204042 0.01614298 0.01110733 0.01941981 0.2408094 0.04460044\n                    ACF1\nTraining set -0.08216434\n\nForecasts:\n         Point Forecast    Lo 80    Hi 80    Lo 95    Hi 95\n2023.004       6.257173 6.236484 6.277861 6.225533 6.288812\n2023.008       6.257173 6.227915 6.286430 6.212427 6.301918\n2023.012       6.257173 6.221340 6.293005 6.202371 6.311974\n2023.016       6.257173 6.215796 6.298549 6.193893 6.320452\n2023.020       6.257173 6.210913 6.303432 6.186424 6.327921\n2023.024       6.257173 6.206497 6.307848 6.179671 6.334674\n2023.028       6.257173 6.202437 6.311908 6.173462 6.340883\n2023.032       6.257173 6.198658 6.315687 6.167682 6.346663\n2023.036       6.257173 6.195108 6.319237 6.162254 6.352092\n2023.040       6.257173 6.191751 6.322594 6.157119 6.357226\n2023.044       6.257173 6.188558 6.325787 6.152236 6.362109\n\n\nShow the code\ncheckresiduals(f2)#serial correlation ; Lung Box p <0.05\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Naive method\nQ* = 685.37, df = 502, p-value = 8.794e-08\n\nModel df: 0.   Total lags used: 502\n\n\n\n\nSeasonal naive method\nThis method is useful for highly seasonal data, which can set each forecast to be equal to the last observed value from the same season of the year. Here seasonal naive is used to forecast the next 4 values for the UNH stock price series.\n\n\nShow the code\nf3<-snaive(log(UNH_ts), h=4) #seasonal naive method\nsummary(f3)\n\n\n\nForecast method: Seasonal naive method\n\nModel Information:\nCall: snaive(y = log(UNH_ts), h = 4) \n\nResidual sd: 0.2754 \n\nError measures:\n                    ME      RMSE       MAE      MPE     MAPE MASE      ACF1\nTraining set 0.2432513 0.2753742 0.2490408 5.098921 5.207824    1 0.9842087\n\nForecasts:\n         Point Forecast    Lo 80    Hi 80    Lo 95    Hi 95\n2023.004       6.172414 5.819508 6.525321 5.632691 6.712138\n2023.008       6.186518 5.833612 6.539424 5.646794 6.726241\n2023.012       6.189044 5.836138 6.541951 5.649321 6.728768\n2023.016       6.197327 5.844421 6.550233 5.657603 6.737050\n\n\nShow the code\ncheckresiduals(f3) #serial correlation ; Lung Box p <0.05\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Seasonal naive method\nQ* = 226574, df = 502, p-value < 2.2e-16\n\nModel df: 0.   Total lags used: 502\n\n\n\n\nDrift Method\nA variation on the naïve method is to allow the forecasts to increase or decrease over time, where the amount of change over time is set to be the average change seen in the historical data.\n\n\nShow the code\nf4 <- rwf(log(UNH_ts),drift=TRUE, h=20) \nsummary(f4)\n\n\n\nForecast method: Random walk with drift\n\nModel Information:\nCall: rwf(y = log(UNH_ts), h = 20, drift = TRUE) \n\nDrift: 9e-04  (se 3e-04)\nResidual sd: 0.0161 \n\nError measures:\n                       ME       RMSE       MAE           MPE      MAPE\nTraining set 1.071097e-16 0.01611672 0.0110743 -0.0004837745 0.2401268\n                   MASE        ACF1\nTraining set 0.04446783 -0.08216434\n\nForecasts:\n         Point Forecast    Lo 80    Hi 80    Lo 95    Hi 95\n2023.004       6.258093 6.237432 6.278754 6.226495 6.289691\n2023.008       6.259013 6.229790 6.288237 6.214320 6.303706\n2023.012       6.259934 6.224137 6.295730 6.205188 6.314680\n2023.016       6.260854 6.219514 6.302195 6.197629 6.324079\n2023.020       6.261775 6.215547 6.308002 6.191076 6.332473\n2023.024       6.262695 6.212048 6.313342 6.185237 6.340153\n2023.028       6.263615 6.208902 6.318329 6.179938 6.347292\n2023.032       6.264536 6.206036 6.323036 6.175068 6.354004\n2023.036       6.265456 6.203398 6.327514 6.170546 6.360366\n2023.040       6.266377 6.200952 6.331802 6.166318 6.366435\n2023.044       6.267297 6.198668 6.335926 6.162338 6.372256\n2023.048       6.268217 6.196526 6.339909 6.158575 6.377860\n2023.052       6.269138 6.194508 6.343768 6.155001 6.383275\n2023.056       6.270058 6.192599 6.347517 6.151595 6.388522\n2023.060       6.270979 6.190788 6.351169 6.148338 6.393619\n2023.064       6.271899 6.189066 6.354732 6.145217 6.398581\n2023.068       6.272819 6.187424 6.358214 6.142219 6.403420\n2023.072       6.273740 6.185856 6.361624 6.139333 6.408147\n2023.076       6.274660 6.184354 6.364966 6.136549 6.412771\n2023.080       6.275581 6.182914 6.368247 6.133860 6.417301\n\n\nShow the code\ncheckresiduals(f4)\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Random walk with drift\nQ* = 685.37, df = 501, p-value = 7.477e-08\n\nModel df: 1.   Total lags used: 502\n\n\n\n\nShow the code\nautoplot(UNH_ts) +\n  autolayer(meanf(UNH_ts, h=100),\n            series=\"Mean.tr\", PI=FALSE) +\n  autolayer(naive((UNH_ts), h=100),\n            series=\"Naïve.tr\", PI=FALSE) +\n  autolayer(rwf((UNH_ts), drift=TRUE, h=100),\n            series=\"Drift.tr\", PI=FALSE) +\n  autolayer(forecast(Arima((UNH_ts), order=c(4, 1, 3),include.drift = TRUE),100), \n            series=\"fit\",PI=FALSE) +\n  ggtitle(\"UNH Stock Price\") +\n  xlab(\"Time\") + ylab(\"Log(Price)\") +\n  guides(colour=guide_legend(title=\"Forecast\"))\n\n\n\n\n\nAccording to the graph above, ARIMA(4,1,3) outperform most of benchmark method, though its performance is very similar to drift method."
  },
  {
    "objectID": "ts.html",
    "href": "ts.html",
    "title": "Financial Time Series Models: CVS Example",
    "section": "",
    "text": "Volatility is a statistical measure of the dispersion of returns for a given security or market index. In most cases, the higher the volatility, the riskier the security. It is often measured as either the standard deviation or variance between returns from the same security or market index.\nAt this section, I am going to fit financial time series models on CVS stock price.\nCVS Health Corporation is a healthcare company that operates a chain of pharmacies and retail clinics. The stock symbol for CVS is “CVS” and it is traded on the New York Stock Exchange (NYSE).\nThe stock price of CVS has shown a general trend of rising over the span of last 13 years. Since 2010, the stock price of CVS has been stably increased, with a few notable fluctuations. The price started the decade at around $30 per share and rose steadily to reach a peak of around $113 per share in July 2015. After that, the stock price went through a period of volatility and declined to around $60 per share in November 2016. Since then, the stock has been recovering and currently trading around $80 per share.\nThe COVID-19 pandemic has had a significant impact on the global economy and financial markets, including the stock price of CVS. As a healthcare company, CVS has been directly impacted by the pandemic, as demand for its pharmacy services and retail products has increased during this period. Like many other stocks, CVS experienced a decline in its stock price in March 2020, when the pandemic began to spread rapidly across the United States. The stock price fell from around $70 in February 2020 to around $50 in March 2020, as investors reacted to the uncertainty and potential economic impact of the pandemic. Despite the initial decline, the stock price of CVS recovered quickly and has been relatively resilient during the pandemic. This is likely due to the essential nature of CVS’s services, as well as the company’s strong financial position and diversified business model. During the pandemic, CVS has continued to grow and expand its business, including through acquisitions and partnerships. For example, CVS announced a partnership with Walgreens and federal and state governments to provide COVID-19 vaccines to long-term care facilities, which has helped to boost the company’s profile and reputation."
  },
  {
    "objectID": "dl.html",
    "href": "dl.html",
    "title": "Deep Learning for TS",
    "section": "",
    "text": "Deep learning models are a family of neural network models that have multiple layers between the input and output layers. These layers allow the model to learn increasingly complex features and representations of the input data, making them effective for time series modeling where the data may have complex and nonlinear relationships. In time series modeling, deep learning models are used to automatically extract and learn features from the time series data, which can then be used for prediction or classification tasks. These models can be used to model both univariate and multivariate time series data.\nSome common deep learning models used for time series modeling include recurrent neural networks (RNNs), long short-term memory networks (LSTMs), and gated recurrent unit networks (GRUs). This section is going to fit AZN stock price into these three deep learning models and explore meaningful results from the predictive modeling."
  },
  {
    "objectID": "Untitled.html",
    "href": "Untitled.html",
    "title": "Time Series for Stock Price in Healthcare",
    "section": "",
    "text": "Show the code\nimport numpy as np\nimport pandas as pd\nfrom keras.models import Sequential\nfrom keras.layers import Dense, SimpleRNN, LSTM, GRU\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\nimport math\nimport matplotlib.pyplot as plt\nnp.random.seed(1)\nimport tensorflow as tf\ntf.random.set_seed(1)\nfrom keras import models\nfrom keras import layers\nfrom keras import optimizers\nimport os\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\n\n\nShow the code\n# Step 1 : import data\ndata=pd.read_csv('data/stock.csv')\nprint(data.shape)\ndata.head()\ndata = data[['AZN']]\ndata.head()\n\n\n(3290, 9)\n\n\n\n\n\n\n  \n    \n      \n      AZN\n    \n  \n  \n    \n      0\n      43.829044\n    \n    \n      1\n      44.240784\n    \n    \n      2\n      44.921402\n    \n    \n      3\n      46.425518\n    \n    \n      4\n      46.568356\n    \n  \n\n\n\n\n\n\nShow the code\n# Step 2 : split data\ndata = np.array(data.values.astype('float32'))\nscaler = MinMaxScaler(feature_range=(0, 1))\ndata = scaler.fit_transform(data).flatten()\nn = len(data)\n# Point for splitting data into train and test\nsplit = int(n * 0.8)\ntrain_data = data[range(split)]\ntest_data = data[split:]\n\n\n\n\nShow the code\n# Step 3 : Prepare the input X and target Y\ndef get_XY(dat, time_steps):\n    # Indices of target array\n    Y_ind = np.arange(time_steps, len(dat), time_steps)\n    Y = dat[Y_ind]\n    # Prepare X\n    rows_x = len(Y)\n    X = dat[range(time_steps * rows_x)]\n    X = np.reshape(X, (rows_x, time_steps, 1))\n    return X, Y\ntime_steps = 12\ntrainX, trainY = get_XY(train_data, time_steps)\ntestX, testY = get_XY(test_data, time_steps)\nmodel = input(\"choose between 3 model types (RNN, LSTM, GRU): \")\nif model == \"RNN\":\n## This is about Keras SimpleRNN:\n    def create_model(hidden_units, dense_units, input_shape, activation):\n        model = Sequential()\n        model.add(SimpleRNN(hidden_units, input_shape=input_shape,\n                        activation=activation[0]))\n        model.add(Dense(units=dense_units, activation=activation[1]))\n        model.compile(loss='mean_squared_error', optimizer='adam')\n        return model\nelif model == \"LSTM\":\n    def create_model(hidden_units, dense_units, input_shape, activation):\n        model = Sequential()\n        model.add(LSTM(hidden_units, input_shape=input_shape,\n                        activation=activation[0]))\n        model.add(Dense(units=dense_units, activation=activation[1]))\n        model.compile(loss='mean_squared_error', optimizer='adam')\n        return model\nelse:\n    def create_model(hidden_units, dense_units, input_shape, activation):\n        model = Sequential()\n        model.add(GRU(hidden_units, input_shape=input_shape,\n                        activation=activation[0]))\n        model.add(Dense(units=dense_units, activation=activation[1]))\n        model.compile(loss='mean_squared_error', optimizer='adam')\n        return model\n\n\nchoose between 3 model types (RNN, LSTM, GRU): GRU\n\n\n\n\nShow the code\n## Step 4: Create RNN Model And Train\n## reuse the function: creat_RNN()\nmodel = create_model(hidden_units=3, dense_units=1, input_shape=(time_steps,1),\n                   activation=['tanh', 'tanh'])\nmodel.fit(trainX, trainY, epochs=20, batch_size=1, verbose=2)\n\n\nEpoch 1/20\n219/219 - 16s - loss: 0.0121 - 16s/epoch - 74ms/step\nEpoch 2/20\n219/219 - 2s - loss: 0.0024 - 2s/epoch - 9ms/step\nEpoch 3/20\n219/219 - 3s - loss: 6.3496e-04 - 3s/epoch - 13ms/step\nEpoch 4/20\n219/219 - 3s - loss: 5.0474e-04 - 3s/epoch - 13ms/step\nEpoch 5/20\n219/219 - 3s - loss: 4.2495e-04 - 3s/epoch - 14ms/step\nEpoch 6/20\n219/219 - 3s - loss: 3.6728e-04 - 3s/epoch - 13ms/step\nEpoch 7/20\n219/219 - 3s - loss: 3.3344e-04 - 3s/epoch - 15ms/step\nEpoch 8/20\n219/219 - 2s - loss: 2.7956e-04 - 2s/epoch - 10ms/step\nEpoch 9/20\n219/219 - 2s - loss: 2.5085e-04 - 2s/epoch - 9ms/step\nEpoch 10/20\n219/219 - 2s - loss: 2.0935e-04 - 2s/epoch - 9ms/step\nEpoch 11/20\n219/219 - 2s - loss: 1.8226e-04 - 2s/epoch - 10ms/step\nEpoch 12/20\n219/219 - 2s - loss: 1.4918e-04 - 2s/epoch - 10ms/step\nEpoch 13/20\n219/219 - 3s - loss: 1.2497e-04 - 3s/epoch - 12ms/step\nEpoch 14/20\n219/219 - 2s - loss: 1.2177e-04 - 2s/epoch - 8ms/step\nEpoch 15/20\n219/219 - 2s - loss: 1.0094e-04 - 2s/epoch - 8ms/step\nEpoch 16/20\n219/219 - 2s - loss: 1.1696e-04 - 2s/epoch - 10ms/step\nEpoch 17/20\n219/219 - 2s - loss: 1.0025e-04 - 2s/epoch - 11ms/step\nEpoch 18/20\n219/219 - 2s - loss: 9.4306e-05 - 2s/epoch - 9ms/step\nEpoch 19/20\n219/219 - 2s - loss: 8.3074e-05 - 2s/epoch - 10ms/step\nEpoch 20/20\n219/219 - 2s - loss: 8.7689e-05 - 2s/epoch - 9ms/step\n\n\n<keras.callbacks.History at 0x27c59e53d60>\n\n\n\n\nShow the code\n## Step 5: Compute And Print The Root Mean Square Error\n## use the function: print_error()\ndef print_error(trainY, testY, train_predict, test_predict):\n    # Error of predictions\n    train_rmse = math.sqrt(mean_squared_error(trainY, train_predict))\n    test_rmse = math.sqrt(mean_squared_error(testY, test_predict))\n    # Print RMSE\n    print('Train RMSE: %.3f RMSE' % (train_rmse))\n    print('Test RMSE: %.3f RMSE' % (test_rmse))\n\n\n\n\nShow the code\n# make predictions\ntrain_predict = model.predict(trainX)\ntest_predict = model.predict(testX)\n# Mean square error of Training dataset and Testing dataset:\nprint_error(trainY, testY, train_predict, test_predict)\n\n\n7/7 [==============================] - 2s 9ms/step\n2/2 [==============================] - 0s 12ms/step\nTrain RMSE: 0.008 RMSE\nTest RMSE: 0.078 RMSE\n\n\n\n\nShow the code\n## Step 6: View The result\n# Plot the result\ndef plot_result(trainY, testY, train_predict, test_predict): ## define the content of the plot\n    actual = np.append(trainY, testY)\n    predictions = np.append(train_predict, test_predict)\n    rows = len(actual)\n    plt.figure(figsize=(15, 6), dpi=80)\n    plt.plot(range(rows), actual)\n    plt.plot(range(rows), predictions)\n    plt.axvline(x=len(trainY), color='r')\n    plt.legend(['Actual', 'Predictions'])\n    plt.xlabel('Observation number after given time steps')\n    plt.ylabel('Adjust close Price for IXIC Index')\n    plt.title('Actual and Predicted Values. The Red Line Separates The Training And Test Examples')\nplot_result(trainY, testY, train_predict, test_predict)\nplt.show() ## show the plot\n\n\n\n\n\n\n\nShow the code\ndata=pd.read_csv('data/azn.csv')\ndata.head()\n\n\n\n\n\n\n  \n    \n      \n      Date\n      AZN.Open\n      AZN.High\n      AZN.Low\n      AZN.Close\n      AZN.Volume\n      AZN.Adjusted\n    \n  \n  \n    \n      0\n      04/01/2010\n      23.709999\n      23.764999\n      23.575001\n      23.705000\n      2606200.0\n      13.440908\n    \n    \n      1\n      05/01/2010\n      23.434999\n      23.465000\n      23.150000\n      23.225000\n      2666600.0\n      13.168744\n    \n    \n      2\n      06/01/2010\n      22.915001\n      23.045000\n      22.785000\n      23.040001\n      3020800.0\n      13.063849\n    \n    \n      3\n      07/01/2010\n      23.250000\n      23.325001\n      23.120001\n      23.290001\n      4454600.0\n      13.205604\n    \n    \n      4\n      08/01/2010\n      23.270000\n      23.415001\n      23.174999\n      23.389999\n      2675600.0\n      13.262300\n    \n  \n\n\n\n\n\n\nShow the code\n\n## set training and testing dataset\nglobal lag     #forcast time lag\nlag=12\n\nnrow=data.shape[0]\ntrain_index=list(range(int(0.7*(nrow-lag))))\nvalidation_index=list(range(int(0.7*(nrow-lag)),int(0.9*(nrow-lag))))\ntest_index=list(range(int(0.9*(nrow-lag)),(nrow-lag)))\n\ndef generate_X_y(data,ex_rate):\n    tmp=data[ex_rate]\n    print('Raw data mean:',np.mean(tmp),'\\nRaw data std:',np.std(tmp))\n    tmp=(tmp-np.mean(tmp))/np.std(tmp)\n\n    X=np.zeros((nrow-lag,lag))\n    for i in range(nrow-lag):X[i,:lag]=tmp.iloc[i:i+lag]\n    y=np.array(tmp[lag:]).reshape((-1,1))\n    return (X,y)\n\n\n\n\nShow the code\n### CHECK train data of X and Y\nX,y=generate_X_y(data,'AZN.Close')\nX_train,y_train=X[train_index,:],y[train_index,:]\nX_validation,y_validation=X[validation_index,:],y[validation_index,:]\nX_test,y_test=X[test_index,:],y[test_index,:]\n\n\nRaw data mean: 36.80589211854097 \nRaw data std: 13.013520313929487\n\n\n\n\nShow the code\n## get the valiedate and test benchmark value\n## define drift validate benchmark:\n### CHECK with another method:\ndef training_performance(model, training_history, epochs):\n    test_MAE = np.mean(np.abs(y_test - model.predict(X_test.reshape(-1, lag, 1))))\n    timestep = range(1, epochs + 1)\n    plt.figure(figsize=(10, 8), facecolor='white')\n    plt.subplot(2, 1, 1)\n    plt.plot(timestep, np.log(training_history.history['val_mae']), 'b', label='Validation Mean Absolute Error')\n    plt.plot(timestep, np.log(training_history.history['mae']), 'bo', label='Training Mean Absolute Error')\n    plt.hlines(np.log(drift_validate_benchmark), xmin=timestep[0], xmax=timestep[-1], colors='coral',\n               label='Validation Drift Benchmark')\n    plt.hlines(np.log(mean_validate_benchmark), xmin=timestep[0], xmax=timestep[-1], colors='lightblue',\n               label='Validation Mean Benchmark')\n    plt.hlines(np.log(test_MAE), xmin=timestep[0], xmax=timestep[-1], colors='purple', label='Testing Mean Absolute Error')\n    plt.ylabel('logged Mean Absolute Error')\n    plt.xlabel('Epoch Value')\n    plt.legend(loc='upper right')\n    plt.subplot(2, 1, 2)\n    plt.hlines(test_MAE, xmin=timestep[0], xmax=timestep[-1], colors='purple', label='Testing Mean Absolute Error')\n    plt.hlines(drift_test_benchmark, xmin=timestep[0], xmax=timestep[-1], colors='coral', label='Test Drift Benchmark')\n    plt.hlines(mean_test_benchmark, xmin=timestep[0], xmax=timestep[-1], colors='lightblue',\n               label='Test Mean Benchmark')\n    plt.ylabel('Mean Absolute Error')\n    plt.legend(loc='right')\n    plt.show()\n\ndef get_Mae_benchmark(X,y):\n    mean_benchmark=np.mean(np.abs(np.mean(X,0)-y))\n    drift_benchmark=np.mean(np.abs(X[:,-1]-y))\n    return(mean_benchmark,drift_benchmark)\n\nmean_validate_benchmark,drift_validate_benchmark=get_Mae_benchmark(X_validation,y_validation)\nmean_test_benchmark,drift_test_benchmark=get_Mae_benchmark(X_test,y_test)\n\n\n\ndef plot_prediction(model, ex_rate):\n    tmp = data[ex_rate]\n    plt.figure(figsize=(10, 8), facecolor='white')\n    prediction = model.predict(X_test.reshape(-1, lag, 1))\n    prediction = prediction * np.std(tmp) + np.mean(tmp)\n    y_true = y_test * np.std(tmp) + np.mean(tmp)\n    plt.plot(list(range(len(prediction))), prediction, color='coral', label='Prediction')\n    plt.plot(list(range(len(y_true))), y_true, color='purple', label='True Value')\n    xticks = np.arange(0, len(y_true), 7)\n    plt.xticks(xticks, labels=data.Date.iloc[test_index].iloc[xticks], rotation=90)\n    plt.legend()\n    plt.show()\n\n\n\n\nShow the code\n### Simple RNN:\noptimizer=optimizers.RMSprop()\nmodel_RNN=models.Sequential()\nmodel_RNN.add(layers.Dense(1))\nmodel_RNN.add(layers.SimpleRNN(1,input_shape=(lag,1),activation='relu'))\nmodel_RNN.compile(optimizer=optimizer,loss='mse',metrics=['mae'])\n\nRnn_history=model_RNN.fit(X_train.reshape(-1,lag,1),y_train.flatten(),batch_size=18,epochs=82,\n                  validation_data=(X_validation.reshape(-1,lag,1),y_validation.flatten()))\n\ntraining_performance(model_RNN,Rnn_history,82)\nplot_prediction(model_RNN,'AZN.Close')\n\n\nEpoch 1/82\n128/128 [==============================] - 12s 32ms/step - loss: 0.4987 - mae: 0.5884 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 2/82\n128/128 [==============================] - 2s 14ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 3/82\n128/128 [==============================] - 2s 19ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 4/82\n128/128 [==============================] - 3s 23ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 5/82\n128/128 [==============================] - 2s 16ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 6/82\n128/128 [==============================] - 2s 15ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 7/82\n128/128 [==============================] - 2s 17ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 8/82\n128/128 [==============================] - 2s 13ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 9/82\n128/128 [==============================] - 2s 15ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 10/82\n128/128 [==============================] - 2s 14ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 11/82\n128/128 [==============================] - 2s 15ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 12/82\n128/128 [==============================] - 2s 15ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 13/82\n128/128 [==============================] - 2s 19ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 14/82\n128/128 [==============================] - 2s 17ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 15/82\n128/128 [==============================] - 3s 20ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 16/82\n128/128 [==============================] - 3s 22ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 17/82\n128/128 [==============================] - 2s 15ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 18/82\n128/128 [==============================] - 2s 17ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 19/82\n128/128 [==============================] - 2s 15ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 20/82\n128/128 [==============================] - 2s 14ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 21/82\n128/128 [==============================] - 2s 15ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 22/82\n128/128 [==============================] - 2s 13ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 23/82\n128/128 [==============================] - 2s 16ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 24/82\n128/128 [==============================] - 3s 22ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 25/82\n128/128 [==============================] - 2s 19ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 26/82\n128/128 [==============================] - 2s 14ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 27/82\n128/128 [==============================] - 2s 12ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 28/82\n128/128 [==============================] - 2s 19ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 29/82\n128/128 [==============================] - 2s 13ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 30/82\n128/128 [==============================] - 2s 12ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 31/82\n128/128 [==============================] - 2s 15ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 32/82\n128/128 [==============================] - 2s 16ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 33/82\n128/128 [==============================] - 3s 24ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 34/82\n128/128 [==============================] - 3s 22ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 35/82\n128/128 [==============================] - 3s 20ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 36/82\n128/128 [==============================] - 2s 15ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 37/82\n128/128 [==============================] - 2s 17ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 38/82\n128/128 [==============================] - 2s 14ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 39/82\n128/128 [==============================] - 2s 18ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 40/82\n128/128 [==============================] - 2s 18ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 41/82\n128/128 [==============================] - 2s 17ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 42/82\n128/128 [==============================] - 2s 18ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 43/82\n128/128 [==============================] - 2s 15ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 44/82\n128/128 [==============================] - 2s 13ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 45/82\n128/128 [==============================] - 2s 15ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 46/82\n128/128 [==============================] - 2s 13ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 47/82\n128/128 [==============================] - 2s 14ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 48/82\n128/128 [==============================] - 2s 19ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 49/82\n128/128 [==============================] - 2s 13ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 50/82\n128/128 [==============================] - 2s 12ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 51/82\n128/128 [==============================] - 2s 15ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 52/82\n128/128 [==============================] - 2s 13ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 53/82\n128/128 [==============================] - 2s 14ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 54/82\n128/128 [==============================] - 2s 13ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 55/82\n128/128 [==============================] - 2s 13ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 56/82\n128/128 [==============================] - 2s 13ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 57/82\n128/128 [==============================] - 2s 15ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 58/82\n128/128 [==============================] - 2s 16ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 59/82\n128/128 [==============================] - 2s 14ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 60/82\n128/128 [==============================] - 2s 17ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 61/82\n128/128 [==============================] - 2s 13ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 62/82\n\n\n128/128 [==============================] - 2s 13ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 63/82\n128/128 [==============================] - 2s 14ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 64/82\n128/128 [==============================] - 2s 13ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 65/82\n128/128 [==============================] - 2s 15ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 66/82\n128/128 [==============================] - 2s 19ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 67/82\n128/128 [==============================] - 3s 23ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 68/82\n128/128 [==============================] - 2s 19ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 69/82\n128/128 [==============================] - 2s 19ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 70/82\n128/128 [==============================] - 2s 17ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 71/82\n128/128 [==============================] - 2s 19ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 72/82\n128/128 [==============================] - 2s 19ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 73/82\n128/128 [==============================] - 2s 15ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 74/82\n128/128 [==============================] - 2s 18ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 75/82\n128/128 [==============================] - 2s 16ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 76/82\n128/128 [==============================] - 5s 42ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 77/82\n128/128 [==============================] - 4s 29ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 78/82\n128/128 [==============================] - 4s 31ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 79/82\n128/128 [==============================] - 2s 18ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 80/82\n128/128 [==============================] - 4s 30ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 81/82\n128/128 [==============================] - 4s 34ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 82/82\n128/128 [==============================] - 4s 29ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\n11/11 [==============================] - 1s 7ms/step\n\n\n\n\n\n11/11 [==============================] - 0s 15ms/step\n\n\n\n\n\n\n\nShow the code\n### LSTM:\nmodel_LSTM=models.Sequential()\nmodel_LSTM.add(layers.Dense(1))\nmodel_LSTM.add(layers.LSTM(1,input_shape=(lag,1),activation='relu'))\nmodel_LSTM.compile(optimizer=optimizers.RMSprop(),loss='mse',metrics=['mae'])\n\nhistory_LSTM=model_LSTM.fit(X_train.reshape(-1,lag,1),y_train.flatten(),batch_size=18,epochs=82,\n                  validation_data=(X_validation.reshape(-1,lag,1),y_validation.flatten()))\ntraining_performance(model_LSTM,history_LSTM,82)\n\nplot_prediction(model_LSTM,'AZN.Close')\n\n\nEpoch 1/82\n128/128 [==============================] - 40s 80ms/step - loss: 5.5322 - mae: 1.6135 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 2/82\n128/128 [==============================] - 8s 65ms/step - loss: 0.8018 - mae: 0.7153 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 3/82\n128/128 [==============================] - 5s 42ms/step - loss: 0.5028 - mae: 0.5901 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 4/82\n128/128 [==============================] - 4s 35ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 5/82\n128/128 [==============================] - 5s 36ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 6/82\n128/128 [==============================] - 4s 31ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 7/82\n128/128 [==============================] - 4s 33ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 8/82\n128/128 [==============================] - 6s 49ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 9/82\n128/128 [==============================] - 6s 44ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 10/82\n128/128 [==============================] - 6s 47ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 11/82\n128/128 [==============================] - 6s 47ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 12/82\n128/128 [==============================] - 5s 41ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 13/82\n128/128 [==============================] - 5s 37ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 14/82\n128/128 [==============================] - 6s 45ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 15/82\n128/128 [==============================] - 6s 48ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 16/82\n128/128 [==============================] - 6s 43ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 17/82\n128/128 [==============================] - 7s 51ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 18/82\n128/128 [==============================] - 8s 62ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 19/82\n128/128 [==============================] - 8s 64ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 20/82\n128/128 [==============================] - 7s 56ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 21/82\n128/128 [==============================] - 6s 47ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 22/82\n128/128 [==============================] - 5s 42ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 23/82\n128/128 [==============================] - 7s 52ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 24/82\n128/128 [==============================] - 6s 50ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 25/82\n128/128 [==============================] - 7s 55ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 26/82\n128/128 [==============================] - 6s 44ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 27/82\n128/128 [==============================] - 7s 55ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 28/82\n128/128 [==============================] - 6s 47ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 29/82\n128/128 [==============================] - 6s 47ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 30/82\n128/128 [==============================] - 5s 43ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 31/82\n128/128 [==============================] - 5s 36ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 32/82\n128/128 [==============================] - 5s 39ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 33/82\n128/128 [==============================] - 5s 38ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 34/82\n128/128 [==============================] - 5s 37ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 35/82\n128/128 [==============================] - 6s 45ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 36/82\n128/128 [==============================] - 6s 47ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 37/82\n128/128 [==============================] - 5s 42ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 38/82\n128/128 [==============================] - 6s 45ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 39/82\n128/128 [==============================] - 6s 44ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 40/82\n128/128 [==============================] - 5s 42ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 41/82\n128/128 [==============================] - 5s 43ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 42/82\n128/128 [==============================] - 5s 42ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 43/82\n128/128 [==============================] - 5s 38ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 44/82\n128/128 [==============================] - 4s 35ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 45/82\n128/128 [==============================] - 5s 42ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 46/82\n128/128 [==============================] - 6s 44ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 47/82\n128/128 [==============================] - 5s 39ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 48/82\n128/128 [==============================] - 5s 42ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 49/82\n128/128 [==============================] - 6s 44ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 50/82\n128/128 [==============================] - 5s 40ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 51/82\n128/128 [==============================] - 5s 41ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 52/82\n128/128 [==============================] - 5s 38ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 53/82\n128/128 [==============================] - 4s 33ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 54/82\n128/128 [==============================] - 4s 31ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 55/82\n128/128 [==============================] - 4s 28ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 56/82\n128/128 [==============================] - 3s 27ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 57/82\n128/128 [==============================] - 4s 28ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 58/82\n128/128 [==============================] - 4s 30ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 59/82\n128/128 [==============================] - 5s 38ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 60/82\n128/128 [==============================] - 4s 31ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 61/82\n128/128 [==============================] - 4s 32ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 62/82\n\n\n128/128 [==============================] - 5s 36ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 63/82\n128/128 [==============================] - 4s 30ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 64/82\n128/128 [==============================] - 3s 27ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 65/82\n128/128 [==============================] - 4s 34ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 66/82\n128/128 [==============================] - 5s 38ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 67/82\n128/128 [==============================] - 5s 37ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 68/82\n128/128 [==============================] - 5s 37ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 69/82\n128/128 [==============================] - 6s 44ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 70/82\n128/128 [==============================] - 5s 41ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 71/82\n128/128 [==============================] - 5s 37ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 72/82\n128/128 [==============================] - 4s 35ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 73/82\n128/128 [==============================] - 5s 39ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 74/82\n128/128 [==============================] - 5s 39ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 75/82\n128/128 [==============================] - 4s 28ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 76/82\n128/128 [==============================] - 4s 29ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 77/82\n128/128 [==============================] - 5s 35ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 78/82\n128/128 [==============================] - 4s 35ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 79/82\n128/128 [==============================] - 4s 32ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 80/82\n128/128 [==============================] - 6s 50ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 81/82\n128/128 [==============================] - 6s 48ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 82/82\n128/128 [==============================] - 7s 52ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\n11/11 [==============================] - 5s 23ms/step\n\n\n\n\n\n11/11 [==============================] - 1s 17ms/step\n\n\n\n\n\n\n\nShow the code\n### GRU:\nmodel_GRU=models.Sequential()\nmodel_GRU.add(layers.LSTM(1,input_shape=(lag,1),activation='relu'))\nmodel_GRU.add(layers.Dense(1))\nmodel_GRU.compile(optimizer=optimizers.RMSprop(),loss='mse',metrics=['mae'])\n\nhistory_GRU=model_GRU.fit(X_train.reshape(-1,lag,1),y_train.flatten(),batch_size=18,epochs=82,\n                  validation_data=(X_validation.reshape(-1,lag,1),y_validation.flatten()))\n\ntraining_performance(model_GRU,history_GRU,82)\nplot_prediction(model_GRU,'AZN.Close')\n\n\nEpoch 1/82\n128/128 [==============================] - 9s 22ms/step - loss: 0.2173 - mae: 0.3515 - val_loss: 1.4073 - val_mae: 1.0980\nEpoch 2/82\n128/128 [==============================] - 2s 20ms/step - loss: 0.0156 - mae: 0.0929 - val_loss: 1.2486 - val_mae: 1.0178\nEpoch 3/82\n128/128 [==============================] - 2s 18ms/step - loss: 0.0129 - mae: 0.0870 - val_loss: 1.1973 - val_mae: 0.9933\nEpoch 4/82\n128/128 [==============================] - 3s 23ms/step - loss: 0.0116 - mae: 0.0819 - val_loss: 1.1444 - val_mae: 0.9671\nEpoch 5/82\n128/128 [==============================] - 2s 14ms/step - loss: 0.0105 - mae: 0.0778 - val_loss: 1.0962 - val_mae: 0.9420\nEpoch 6/82\n128/128 [==============================] - 2s 15ms/step - loss: 0.0095 - mae: 0.0736 - val_loss: 1.0545 - val_mae: 0.9201\nEpoch 7/82\n128/128 [==============================] - 2s 13ms/step - loss: 0.0086 - mae: 0.0695 - val_loss: 1.0150 - val_mae: 0.8981\nEpoch 8/82\n128/128 [==============================] - 3s 23ms/step - loss: 0.0080 - mae: 0.0667 - val_loss: 0.9898 - val_mae: 0.8841\nEpoch 9/82\n128/128 [==============================] - 3s 27ms/step - loss: 0.0074 - mae: 0.0637 - val_loss: 0.9631 - val_mae: 0.8686\nEpoch 10/82\n128/128 [==============================] - 2s 17ms/step - loss: 0.0067 - mae: 0.0604 - val_loss: 0.9405 - val_mae: 0.8554\nEpoch 11/82\n128/128 [==============================] - 2s 16ms/step - loss: 0.0062 - mae: 0.0570 - val_loss: 0.9183 - val_mae: 0.8421\nEpoch 12/82\n128/128 [==============================] - 2s 16ms/step - loss: 0.0057 - mae: 0.0545 - val_loss: 0.8940 - val_mae: 0.8274\nEpoch 13/82\n128/128 [==============================] - 2s 18ms/step - loss: 0.0053 - mae: 0.0520 - val_loss: 0.8782 - val_mae: 0.8184\nEpoch 14/82\n128/128 [==============================] - 2s 17ms/step - loss: 0.0050 - mae: 0.0501 - val_loss: 0.8604 - val_mae: 0.8079\nEpoch 15/82\n128/128 [==============================] - 2s 16ms/step - loss: 0.0047 - mae: 0.0487 - val_loss: 0.8425 - val_mae: 0.7973\nEpoch 16/82\n128/128 [==============================] - 2s 17ms/step - loss: 0.0046 - mae: 0.0473 - val_loss: 0.8303 - val_mae: 0.7903\nEpoch 17/82\n128/128 [==============================] - 2s 15ms/step - loss: 0.0044 - mae: 0.0463 - val_loss: 0.8194 - val_mae: 0.7842\nEpoch 18/82\n128/128 [==============================] - 2s 17ms/step - loss: 0.0042 - mae: 0.0454 - val_loss: 0.8028 - val_mae: 0.7744\nEpoch 19/82\n128/128 [==============================] - 2s 15ms/step - loss: 0.0041 - mae: 0.0449 - val_loss: 0.7949 - val_mae: 0.7699\nEpoch 20/82\n128/128 [==============================] - 2s 16ms/step - loss: 0.0040 - mae: 0.0443 - val_loss: 0.7849 - val_mae: 0.7641\nEpoch 21/82\n128/128 [==============================] - 2s 13ms/step - loss: 0.0039 - mae: 0.0435 - val_loss: 0.7761 - val_mae: 0.7592\nEpoch 22/82\n128/128 [==============================] - 2s 15ms/step - loss: 0.0038 - mae: 0.0426 - val_loss: 0.7656 - val_mae: 0.7533\nEpoch 23/82\n128/128 [==============================] - 2s 16ms/step - loss: 0.0037 - mae: 0.0424 - val_loss: 0.7515 - val_mae: 0.7447\nEpoch 24/82\n128/128 [==============================] - 2s 17ms/step - loss: 0.0036 - mae: 0.0415 - val_loss: 0.7419 - val_mae: 0.7392\nEpoch 25/82\n128/128 [==============================] - 2s 19ms/step - loss: 0.0035 - mae: 0.0412 - val_loss: 0.7333 - val_mae: 0.7343\nEpoch 26/82\n128/128 [==============================] - 2s 18ms/step - loss: 0.0034 - mae: 0.0404 - val_loss: 0.7230 - val_mae: 0.7283\nEpoch 27/82\n128/128 [==============================] - 2s 17ms/step - loss: 0.0033 - mae: 0.0397 - val_loss: 0.7077 - val_mae: 0.7190\nEpoch 28/82\n128/128 [==============================] - 2s 16ms/step - loss: 0.0032 - mae: 0.0390 - val_loss: 0.7007 - val_mae: 0.7153\nEpoch 29/82\n128/128 [==============================] - 1s 12ms/step - loss: 0.0031 - mae: 0.0382 - val_loss: 0.6873 - val_mae: 0.7071\nEpoch 30/82\n128/128 [==============================] - 1s 12ms/step - loss: 0.0030 - mae: 0.0372 - val_loss: 0.6769 - val_mae: 0.7009\nEpoch 31/82\n128/128 [==============================] - 1s 11ms/step - loss: 0.0029 - mae: 0.0368 - val_loss: 0.6653 - val_mae: 0.6939\nEpoch 32/82\n128/128 [==============================] - 2s 12ms/step - loss: 0.0028 - mae: 0.0360 - val_loss: 0.6541 - val_mae: 0.6868\nEpoch 33/82\n128/128 [==============================] - 2s 13ms/step - loss: 0.0027 - mae: 0.0352 - val_loss: 0.6474 - val_mae: 0.6832\nEpoch 34/82\n128/128 [==============================] - 2s 13ms/step - loss: 0.0026 - mae: 0.0346 - val_loss: 0.6350 - val_mae: 0.6750\nEpoch 35/82\n128/128 [==============================] - 1s 11ms/step - loss: 0.0025 - mae: 0.0338 - val_loss: 0.6281 - val_mae: 0.6713\nEpoch 36/82\n128/128 [==============================] - 1s 11ms/step - loss: 0.0024 - mae: 0.0332 - val_loss: 0.6141 - val_mae: 0.6616\nEpoch 37/82\n128/128 [==============================] - 2s 15ms/step - loss: 0.0023 - mae: 0.0327 - val_loss: 0.6107 - val_mae: 0.6602\nEpoch 38/82\n128/128 [==============================] - 2s 14ms/step - loss: 0.0023 - mae: 0.0320 - val_loss: 0.5983 - val_mae: 0.6519\nEpoch 39/82\n128/128 [==============================] - 2s 19ms/step - loss: 0.0022 - mae: 0.0317 - val_loss: 0.5912 - val_mae: 0.6476\nEpoch 40/82\n128/128 [==============================] - 2s 15ms/step - loss: 0.0021 - mae: 0.0309 - val_loss: 0.5804 - val_mae: 0.6403\nEpoch 41/82\n128/128 [==============================] - 2s 15ms/step - loss: 0.0021 - mae: 0.0306 - val_loss: 0.5749 - val_mae: 0.6370\nEpoch 42/82\n128/128 [==============================] - 2s 14ms/step - loss: 0.0020 - mae: 0.0302 - val_loss: 0.5666 - val_mae: 0.6314\nEpoch 43/82\n128/128 [==============================] - 2s 14ms/step - loss: 0.0019 - mae: 0.0296 - val_loss: 0.5547 - val_mae: 0.6230\nEpoch 44/82\n128/128 [==============================] - 2s 12ms/step - loss: 0.0019 - mae: 0.0292 - val_loss: 0.5509 - val_mae: 0.6209\nEpoch 45/82\n128/128 [==============================] - 2s 12ms/step - loss: 0.0019 - mae: 0.0289 - val_loss: 0.5451 - val_mae: 0.6173\nEpoch 46/82\n128/128 [==============================] - 2s 12ms/step - loss: 0.0018 - mae: 0.0284 - val_loss: 0.5354 - val_mae: 0.6103\nEpoch 47/82\n128/128 [==============================] - 2s 12ms/step - loss: 0.0018 - mae: 0.0284 - val_loss: 0.5314 - val_mae: 0.6079\nEpoch 48/82\n128/128 [==============================] - 2s 12ms/step - loss: 0.0018 - mae: 0.0282 - val_loss: 0.5242 - val_mae: 0.6029\nEpoch 49/82\n128/128 [==============================] - 2s 12ms/step - loss: 0.0017 - mae: 0.0277 - val_loss: 0.5179 - val_mae: 0.5985\nEpoch 50/82\n128/128 [==============================] - 2s 12ms/step - loss: 0.0017 - mae: 0.0275 - val_loss: 0.5107 - val_mae: 0.5932\nEpoch 51/82\n128/128 [==============================] - 2s 12ms/step - loss: 0.0017 - mae: 0.0274 - val_loss: 0.5081 - val_mae: 0.5920\nEpoch 52/82\n128/128 [==============================] - 2s 12ms/step - loss: 0.0016 - mae: 0.0271 - val_loss: 0.5054 - val_mae: 0.5904\nEpoch 53/82\n128/128 [==============================] - 2s 12ms/step - loss: 0.0016 - mae: 0.0269 - val_loss: 0.4996 - val_mae: 0.5861\nEpoch 54/82\n128/128 [==============================] - 1s 12ms/step - loss: 0.0016 - mae: 0.0268 - val_loss: 0.4971 - val_mae: 0.5847\nEpoch 55/82\n128/128 [==============================] - 1s 12ms/step - loss: 0.0016 - mae: 0.0268 - val_loss: 0.4911 - val_mae: 0.5804\nEpoch 56/82\n128/128 [==============================] - 1s 11ms/step - loss: 0.0016 - mae: 0.0263 - val_loss: 0.4868 - val_mae: 0.5774\nEpoch 57/82\n128/128 [==============================] - 2s 12ms/step - loss: 0.0015 - mae: 0.0264 - val_loss: 0.4824 - val_mae: 0.5741\nEpoch 58/82\n128/128 [==============================] - 2s 12ms/step - loss: 0.0015 - mae: 0.0262 - val_loss: 0.4807 - val_mae: 0.5732\nEpoch 59/82\n128/128 [==============================] - 1s 12ms/step - loss: 0.0015 - mae: 0.0262 - val_loss: 0.4735 - val_mae: 0.5677\nEpoch 60/82\n128/128 [==============================] - 1s 11ms/step - loss: 0.0015 - mae: 0.0259 - val_loss: 0.4705 - val_mae: 0.5657\nEpoch 61/82\n128/128 [==============================] - 2s 12ms/step - loss: 0.0015 - mae: 0.0258 - val_loss: 0.4695 - val_mae: 0.5652\nEpoch 62/82\n\n\n128/128 [==============================] - 2s 12ms/step - loss: 0.0015 - mae: 0.0256 - val_loss: 0.4633 - val_mae: 0.5605\nEpoch 63/82\n128/128 [==============================] - 2s 14ms/step - loss: 0.0015 - mae: 0.0257 - val_loss: 0.4611 - val_mae: 0.5592\nEpoch 64/82\n128/128 [==============================] - 2s 12ms/step - loss: 0.0015 - mae: 0.0254 - val_loss: 0.4583 - val_mae: 0.5573\nEpoch 65/82\n128/128 [==============================] - 1s 11ms/step - loss: 0.0015 - mae: 0.0256 - val_loss: 0.4558 - val_mae: 0.5556\nEpoch 66/82\n128/128 [==============================] - 1s 11ms/step - loss: 0.0014 - mae: 0.0253 - val_loss: 0.4523 - val_mae: 0.5529\nEpoch 67/82\n128/128 [==============================] - 1s 11ms/step - loss: 0.0014 - mae: 0.0254 - val_loss: 0.4489 - val_mae: 0.5504\nEpoch 68/82\n128/128 [==============================] - 1s 11ms/step - loss: 0.0014 - mae: 0.0253 - val_loss: 0.4440 - val_mae: 0.5466\nEpoch 69/82\n128/128 [==============================] - 2s 12ms/step - loss: 0.0014 - mae: 0.0253 - val_loss: 0.4435 - val_mae: 0.5466\nEpoch 70/82\n128/128 [==============================] - 2s 12ms/step - loss: 0.0014 - mae: 0.0252 - val_loss: 0.4389 - val_mae: 0.5429\nEpoch 71/82\n128/128 [==============================] - 1s 12ms/step - loss: 0.0014 - mae: 0.0251 - val_loss: 0.4382 - val_mae: 0.5427\nEpoch 72/82\n128/128 [==============================] - 1s 12ms/step - loss: 0.0014 - mae: 0.0252 - val_loss: 0.4376 - val_mae: 0.5427\nEpoch 73/82\n128/128 [==============================] - 1s 12ms/step - loss: 0.0014 - mae: 0.0252 - val_loss: 0.4344 - val_mae: 0.5402\nEpoch 74/82\n128/128 [==============================] - 1s 11ms/step - loss: 0.0014 - mae: 0.0250 - val_loss: 0.4295 - val_mae: 0.5361\nEpoch 75/82\n128/128 [==============================] - 2s 12ms/step - loss: 0.0014 - mae: 0.0250 - val_loss: 0.4266 - val_mae: 0.5339\nEpoch 76/82\n128/128 [==============================] - 1s 11ms/step - loss: 0.0014 - mae: 0.0248 - val_loss: 0.4238 - val_mae: 0.5317\nEpoch 77/82\n128/128 [==============================] - 2s 12ms/step - loss: 0.0014 - mae: 0.0248 - val_loss: 0.4261 - val_mae: 0.5343\nEpoch 78/82\n128/128 [==============================] - 2s 12ms/step - loss: 0.0014 - mae: 0.0249 - val_loss: 0.4232 - val_mae: 0.5320\nEpoch 79/82\n128/128 [==============================] - 1s 11ms/step - loss: 0.0014 - mae: 0.0247 - val_loss: 0.4193 - val_mae: 0.5288\nEpoch 80/82\n128/128 [==============================] - 2s 12ms/step - loss: 0.0014 - mae: 0.0248 - val_loss: 0.4189 - val_mae: 0.5288\nEpoch 81/82\n128/128 [==============================] - 1s 11ms/step - loss: 0.0014 - mae: 0.0246 - val_loss: 0.4164 - val_mae: 0.5269\nEpoch 82/82\n128/128 [==============================] - 1s 11ms/step - loss: 0.0013 - mae: 0.0246 - val_loss: 0.4128 - val_mae: 0.5237\n11/11 [==============================] - 1s 4ms/step\n\n\n\n\n\n11/11 [==============================] - 0s 5ms/step\n\n\n\n\n\n\n\nShow the code\n### apply regulation on RNN:\n### L1 regulation:\noptimizer=optimizers.RMSprop()\nmodel_RNN_L1=models.Sequential()\nmodel_RNN_L1.add(layers.Dense(1, activation='linear', kernel_regularizer='l1'))\nmodel_RNN_L1.add(layers.SimpleRNN(32, activation='relu', input_shape=(lag,1)))\nmodel_RNN_L1.compile(optimizer=optimizer,loss='mse',metrics=['mae'])\n\nhistory_RNN_L1=model_RNN_L1.fit(X_train.reshape(-1,lag,1),y_train.flatten(),batch_size=18,epochs=82,\n                  validation_data=(X_validation.reshape(-1,lag,1),y_validation.flatten()))\n\ntraining_performance(model_RNN_L1,history_RNN_L1,82)\nplot_prediction(model_RNN_L1,'AZN.Close')\n\n\nEpoch 1/82\n128/128 [==============================] - 6s 18ms/step - loss: 0.5861 - mae: 0.6218 - val_loss: 0.7342 - val_mae: 0.6803\nEpoch 2/82\n128/128 [==============================] - 1s 11ms/step - loss: 0.5131 - mae: 0.5865 - val_loss: 0.5484 - val_mae: 0.5216\nEpoch 3/82\n128/128 [==============================] - 1s 10ms/step - loss: 0.5111 - mae: 0.5850 - val_loss: 0.5387 - val_mae: 0.5616\nEpoch 4/82\n128/128 [==============================] - 1s 10ms/step - loss: 0.5096 - mae: 0.5845 - val_loss: 0.4740 - val_mae: 0.4704\nEpoch 5/82\n128/128 [==============================] - 1s 10ms/step - loss: 0.5082 - mae: 0.5843 - val_loss: 0.5393 - val_mae: 0.5726\nEpoch 6/82\n128/128 [==============================] - 1s 10ms/step - loss: 0.5069 - mae: 0.5841 - val_loss: 0.4889 - val_mae: 0.5025\nEpoch 7/82\n128/128 [==============================] - 1s 10ms/step - loss: 0.5056 - mae: 0.5842 - val_loss: 0.4543 - val_mae: 0.4409\nEpoch 8/82\n128/128 [==============================] - 1s 10ms/step - loss: 0.5043 - mae: 0.5842 - val_loss: 0.5623 - val_mae: 0.6038\nEpoch 9/82\n128/128 [==============================] - 1s 10ms/step - loss: 0.5031 - mae: 0.5843 - val_loss: 0.4585 - val_mae: 0.4550\nEpoch 10/82\n128/128 [==============================] - 1s 10ms/step - loss: 0.5019 - mae: 0.5845 - val_loss: 0.4899 - val_mae: 0.5080\nEpoch 11/82\n128/128 [==============================] - 1s 10ms/step - loss: 0.5008 - mae: 0.5848 - val_loss: 0.4990 - val_mae: 0.5230\nEpoch 12/82\n128/128 [==============================] - 1s 10ms/step - loss: 0.4999 - mae: 0.5851 - val_loss: 0.5719 - val_mae: 0.6131\nEpoch 13/82\n128/128 [==============================] - 1s 11ms/step - loss: 0.4990 - mae: 0.5856 - val_loss: 0.7777 - val_mae: 0.7761\nEpoch 14/82\n128/128 [==============================] - 1s 11ms/step - loss: 0.4986 - mae: 0.5863 - val_loss: 0.8035 - val_mae: 0.7913\nEpoch 15/82\n128/128 [==============================] - 1s 11ms/step - loss: 0.4986 - mae: 0.5868 - val_loss: 0.9689 - val_mae: 0.8841\nEpoch 16/82\n128/128 [==============================] - 1s 10ms/step - loss: 0.4985 - mae: 0.5866 - val_loss: 0.5487 - val_mae: 0.5856\nEpoch 17/82\n128/128 [==============================] - 1s 10ms/step - loss: 0.4985 - mae: 0.5868 - val_loss: 0.9329 - val_mae: 0.8646\nEpoch 18/82\n128/128 [==============================] - 1s 10ms/step - loss: 0.4985 - mae: 0.5868 - val_loss: 0.6800 - val_mae: 0.6994\nEpoch 19/82\n128/128 [==============================] - 1s 10ms/step - loss: 0.4986 - mae: 0.5868 - val_loss: 0.6020 - val_mae: 0.6356\nEpoch 20/82\n128/128 [==============================] - 1s 10ms/step - loss: 0.4985 - mae: 0.5867 - val_loss: 0.7942 - val_mae: 0.7849\nEpoch 21/82\n128/128 [==============================] - 1s 10ms/step - loss: 0.4985 - mae: 0.5867 - val_loss: 0.7778 - val_mae: 0.7662\nEpoch 22/82\n128/128 [==============================] - 1s 9ms/step - loss: 0.4985 - mae: 0.5869 - val_loss: 0.7034 - val_mae: 0.7211\nEpoch 23/82\n128/128 [==============================] - 1s 11ms/step - loss: 0.4986 - mae: 0.5870 - val_loss: 0.5581 - val_mae: 0.5924\nEpoch 24/82\n128/128 [==============================] - 2s 14ms/step - loss: 0.4985 - mae: 0.5866 - val_loss: 0.7923 - val_mae: 0.7802\nEpoch 25/82\n128/128 [==============================] - 2s 14ms/step - loss: 0.4985 - mae: 0.5867 - val_loss: 0.8088 - val_mae: 0.7852\nEpoch 26/82\n128/128 [==============================] - 1s 11ms/step - loss: 0.4985 - mae: 0.5868 - val_loss: 0.9231 - val_mae: 0.8576\nEpoch 27/82\n128/128 [==============================] - 1s 10ms/step - loss: 0.4984 - mae: 0.5866 - val_loss: 0.6950 - val_mae: 0.7103\nEpoch 28/82\n128/128 [==============================] - 1s 10ms/step - loss: 0.4985 - mae: 0.5870 - val_loss: 0.8908 - val_mae: 0.8403\nEpoch 29/82\n128/128 [==============================] - 1s 10ms/step - loss: 0.4986 - mae: 0.5871 - val_loss: 0.8209 - val_mae: 0.7948\nEpoch 30/82\n128/128 [==============================] - 1s 10ms/step - loss: 0.4985 - mae: 0.5866 - val_loss: 0.8684 - val_mae: 0.8248\nEpoch 31/82\n128/128 [==============================] - 1s 10ms/step - loss: 0.4986 - mae: 0.5871 - val_loss: 0.9038 - val_mae: 0.8450\nEpoch 32/82\n128/128 [==============================] - 1s 12ms/step - loss: 0.4985 - mae: 0.5867 - val_loss: 0.7245 - val_mae: 0.7337\nEpoch 33/82\n128/128 [==============================] - 2s 13ms/step - loss: 0.4985 - mae: 0.5866 - val_loss: 0.6747 - val_mae: 0.6971\nEpoch 34/82\n128/128 [==============================] - 1s 10ms/step - loss: 0.4985 - mae: 0.5868 - val_loss: 0.6232 - val_mae: 0.6619\nEpoch 35/82\n128/128 [==============================] - 1s 11ms/step - loss: 0.4985 - mae: 0.5868 - val_loss: 0.9664 - val_mae: 0.8804\nEpoch 36/82\n128/128 [==============================] - 2s 13ms/step - loss: 0.4985 - mae: 0.5866 - val_loss: 0.8829 - val_mae: 0.8314\nEpoch 37/82\n128/128 [==============================] - 1s 10ms/step - loss: 0.4985 - mae: 0.5867 - val_loss: 0.7500 - val_mae: 0.7530\nEpoch 38/82\n128/128 [==============================] - 1s 10ms/step - loss: 0.4984 - mae: 0.5866 - val_loss: 0.8512 - val_mae: 0.8214\nEpoch 39/82\n128/128 [==============================] - 2s 12ms/step - loss: 0.4985 - mae: 0.5870 - val_loss: 0.7551 - val_mae: 0.7535\nEpoch 40/82\n128/128 [==============================] - 2s 13ms/step - loss: 0.4985 - mae: 0.5867 - val_loss: 0.8826 - val_mae: 0.8317\nEpoch 41/82\n128/128 [==============================] - 2s 12ms/step - loss: 0.4986 - mae: 0.5871 - val_loss: 1.1124 - val_mae: 0.9553\nEpoch 42/82\n128/128 [==============================] - 1s 10ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 43/82\n128/128 [==============================] - 2s 13ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 44/82\n128/128 [==============================] - 1s 11ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 45/82\n128/128 [==============================] - 2s 12ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 46/82\n128/128 [==============================] - 1s 10ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 47/82\n128/128 [==============================] - 1s 11ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 48/82\n128/128 [==============================] - 1s 10ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 49/82\n128/128 [==============================] - 1s 10ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 50/82\n128/128 [==============================] - 1s 10ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 51/82\n128/128 [==============================] - 1s 10ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 52/82\n128/128 [==============================] - 2s 12ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 53/82\n128/128 [==============================] - 1s 10ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 54/82\n128/128 [==============================] - 1s 10ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 55/82\n128/128 [==============================] - 1s 9ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 56/82\n128/128 [==============================] - 1s 11ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 57/82\n128/128 [==============================] - 2s 12ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 58/82\n128/128 [==============================] - 1s 10ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 59/82\n128/128 [==============================] - 1s 9ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 60/82\n128/128 [==============================] - 1s 10ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 61/82\n128/128 [==============================] - 1s 11ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 62/82\n\n\n128/128 [==============================] - 1s 11ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 63/82\n128/128 [==============================] - 2s 12ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 64/82\n128/128 [==============================] - 1s 12ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 65/82\n128/128 [==============================] - 1s 11ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 66/82\n128/128 [==============================] - 1s 10ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 67/82\n128/128 [==============================] - 1s 9ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 68/82\n128/128 [==============================] - 1s 11ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 69/82\n128/128 [==============================] - 1s 9ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 70/82\n128/128 [==============================] - 1s 10ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 71/82\n128/128 [==============================] - 1s 10ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 72/82\n128/128 [==============================] - 1s 10ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 73/82\n128/128 [==============================] - 1s 11ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 74/82\n128/128 [==============================] - 1s 10ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 75/82\n128/128 [==============================] - 1s 10ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 76/82\n128/128 [==============================] - 1s 11ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 77/82\n128/128 [==============================] - 1s 11ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 78/82\n128/128 [==============================] - 1s 10ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 79/82\n128/128 [==============================] - 1s 9ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 80/82\n128/128 [==============================] - 1s 11ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 81/82\n128/128 [==============================] - 1s 12ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\nEpoch 82/82\n128/128 [==============================] - 1s 11ms/step - loss: 0.4987 - mae: 0.5883 - val_loss: 1.2357 - val_mae: 1.0056\n11/11 [==============================] - 1s 5ms/step\n\n\n\n\n\n11/11 [==============================] - 0s 5ms/step\n\n\n\n\n\n\n\nShow the code\n## apply L2 to RNN:\noptimizer=optimizers.RMSprop()\nmodel_RNN_L2=models.Sequential()\nmodel_RNN_L2.add(layers.SimpleRNN(32, activation='relu', input_shape=(lag,1)))\nmodel_RNN_L2.add(layers.Dense(1, activation='linear', kernel_regularizer='l2'))\nmodel_RNN_L2.compile(optimizer=optimizer,loss='mse',metrics=['mae'])\n\nhistory_RNN_L2=model_RNN_L2.fit(X_train.reshape(-1,lag,1),y_train.flatten(),batch_size=18,epochs=82,\n                  validation_data=(X_validation.reshape(-1,lag,1),y_validation.flatten()))\n\ntraining_performance(model_RNN_L1,history_RNN_L2,82)\nplot_prediction(model_RNN_L2,'AZN.Close')\n\n\nEpoch 1/82\n128/128 [==============================] - 6s 17ms/step - loss: 0.2918 - mae: 0.2824 - val_loss: 0.0887 - val_mae: 0.2388\nEpoch 2/82\n128/128 [==============================] - 1s 11ms/step - loss: 0.0150 - mae: 0.0408 - val_loss: 0.0695 - val_mae: 0.2123\nEpoch 3/82\n128/128 [==============================] - 1s 9ms/step - loss: 0.0105 - mae: 0.0376 - val_loss: 0.1283 - val_mae: 0.3092\nEpoch 4/82\n128/128 [==============================] - 1s 9ms/step - loss: 0.0085 - mae: 0.0374 - val_loss: 0.0439 - val_mae: 0.1648\nEpoch 5/82\n128/128 [==============================] - 1s 11ms/step - loss: 0.0073 - mae: 0.0360 - val_loss: 0.0943 - val_mae: 0.2614\nEpoch 6/82\n128/128 [==============================] - 1s 10ms/step - loss: 0.0064 - mae: 0.0358 - val_loss: 0.0420 - val_mae: 0.1675\nEpoch 7/82\n128/128 [==============================] - 1s 10ms/step - loss: 0.0058 - mae: 0.0351 - val_loss: 0.0240 - val_mae: 0.1182\nEpoch 8/82\n128/128 [==============================] - 1s 9ms/step - loss: 0.0052 - mae: 0.0343 - val_loss: 0.1029 - val_mae: 0.2775\nEpoch 9/82\n128/128 [==============================] - 1s 9ms/step - loss: 0.0047 - mae: 0.0333 - val_loss: 0.0340 - val_mae: 0.1484\nEpoch 10/82\n128/128 [==============================] - 2s 12ms/step - loss: 0.0043 - mae: 0.0332 - val_loss: 0.0501 - val_mae: 0.1921\nEpoch 11/82\n128/128 [==============================] - 1s 10ms/step - loss: 0.0041 - mae: 0.0325 - val_loss: 0.0199 - val_mae: 0.1072\nEpoch 12/82\n128/128 [==============================] - 1s 9ms/step - loss: 0.0038 - mae: 0.0323 - val_loss: 0.0171 - val_mae: 0.0996\nEpoch 13/82\n128/128 [==============================] - 2s 13ms/step - loss: 0.0036 - mae: 0.0323 - val_loss: 0.0390 - val_mae: 0.1643\nEpoch 14/82\n128/128 [==============================] - 2s 14ms/step - loss: 0.0035 - mae: 0.0326 - val_loss: 0.0096 - val_mae: 0.0667\nEpoch 15/82\n128/128 [==============================] - 1s 10ms/step - loss: 0.0034 - mae: 0.0318 - val_loss: 0.0395 - val_mae: 0.1671\nEpoch 16/82\n128/128 [==============================] - 1s 11ms/step - loss: 0.0033 - mae: 0.0325 - val_loss: 0.0592 - val_mae: 0.2082\nEpoch 17/82\n128/128 [==============================] - 1s 10ms/step - loss: 0.0032 - mae: 0.0318 - val_loss: 0.0678 - val_mae: 0.2253\nEpoch 18/82\n128/128 [==============================] - 1s 10ms/step - loss: 0.0031 - mae: 0.0320 - val_loss: 0.0133 - val_mae: 0.0868\nEpoch 19/82\n128/128 [==============================] - 1s 10ms/step - loss: 0.0030 - mae: 0.0317 - val_loss: 0.0224 - val_mae: 0.1177\nEpoch 20/82\n128/128 [==============================] - 1s 11ms/step - loss: 0.0030 - mae: 0.0318 - val_loss: 0.0628 - val_mae: 0.2204\nEpoch 21/82\n128/128 [==============================] - 1s 12ms/step - loss: 0.0029 - mae: 0.0317 - val_loss: 0.0408 - val_mae: 0.1686\nEpoch 22/82\n128/128 [==============================] - 2s 12ms/step - loss: 0.0028 - mae: 0.0313 - val_loss: 0.0210 - val_mae: 0.1136\nEpoch 23/82\n128/128 [==============================] - 1s 12ms/step - loss: 0.0028 - mae: 0.0318 - val_loss: 0.0079 - val_mae: 0.0597\nEpoch 24/82\n128/128 [==============================] - 2s 13ms/step - loss: 0.0027 - mae: 0.0312 - val_loss: 0.0145 - val_mae: 0.0927\nEpoch 25/82\n128/128 [==============================] - 1s 10ms/step - loss: 0.0028 - mae: 0.0317 - val_loss: 0.0179 - val_mae: 0.1035\nEpoch 26/82\n128/128 [==============================] - 1s 11ms/step - loss: 0.0027 - mae: 0.0311 - val_loss: 0.0876 - val_mae: 0.2586\nEpoch 27/82\n128/128 [==============================] - 1s 12ms/step - loss: 0.0027 - mae: 0.0311 - val_loss: 0.0259 - val_mae: 0.1294\nEpoch 28/82\n128/128 [==============================] - 1s 11ms/step - loss: 0.0026 - mae: 0.0310 - val_loss: 0.0332 - val_mae: 0.1505\nEpoch 29/82\n128/128 [==============================] - 1s 9ms/step - loss: 0.0026 - mae: 0.0314 - val_loss: 0.0744 - val_mae: 0.2362\nEpoch 30/82\n128/128 [==============================] - 1s 9ms/step - loss: 0.0026 - mae: 0.0306 - val_loss: 0.0683 - val_mae: 0.2261\nEpoch 31/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0026 - mae: 0.0308 - val_loss: 0.0205 - val_mae: 0.1134\nEpoch 32/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0025 - mae: 0.0305 - val_loss: 0.0084 - val_mae: 0.0639\nEpoch 33/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0025 - mae: 0.0302 - val_loss: 0.0623 - val_mae: 0.2169\nEpoch 34/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0025 - mae: 0.0306 - val_loss: 0.0178 - val_mae: 0.1041\nEpoch 35/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0025 - mae: 0.0306 - val_loss: 0.1061 - val_mae: 0.2884\nEpoch 36/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0025 - mae: 0.0307 - val_loss: 0.0237 - val_mae: 0.1234\nEpoch 37/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0024 - mae: 0.0303 - val_loss: 0.0349 - val_mae: 0.1564\nEpoch 38/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0024 - mae: 0.0304 - val_loss: 0.0393 - val_mae: 0.1681\nEpoch 39/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0024 - mae: 0.0304 - val_loss: 0.0515 - val_mae: 0.1947\nEpoch 40/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0024 - mae: 0.0303 - val_loss: 0.0355 - val_mae: 0.1567\nEpoch 41/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0024 - mae: 0.0300 - val_loss: 0.0545 - val_mae: 0.2009\nEpoch 42/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0024 - mae: 0.0306 - val_loss: 0.0572 - val_mae: 0.2077\nEpoch 43/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0023 - mae: 0.0297 - val_loss: 0.0230 - val_mae: 0.1220\nEpoch 44/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0023 - mae: 0.0294 - val_loss: 0.0171 - val_mae: 0.1026\nEpoch 45/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0024 - mae: 0.0300 - val_loss: 0.0105 - val_mae: 0.0747\nEpoch 46/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0023 - mae: 0.0297 - val_loss: 0.0147 - val_mae: 0.0950\nEpoch 47/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0023 - mae: 0.0301 - val_loss: 0.0288 - val_mae: 0.1404\nEpoch 48/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0023 - mae: 0.0298 - val_loss: 0.0291 - val_mae: 0.1404\nEpoch 49/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0022 - mae: 0.0291 - val_loss: 0.0081 - val_mae: 0.0628\nEpoch 50/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0022 - mae: 0.0293 - val_loss: 0.0112 - val_mae: 0.0786\nEpoch 51/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0022 - mae: 0.0297 - val_loss: 0.0083 - val_mae: 0.0634\nEpoch 52/82\n128/128 [==============================] - 1s 9ms/step - loss: 0.0022 - mae: 0.0290 - val_loss: 0.0784 - val_mae: 0.2437\nEpoch 53/82\n128/128 [==============================] - 1s 9ms/step - loss: 0.0022 - mae: 0.0295 - val_loss: 0.0665 - val_mae: 0.2224\nEpoch 54/82\n128/128 [==============================] - 1s 9ms/step - loss: 0.0022 - mae: 0.0293 - val_loss: 0.0586 - val_mae: 0.2105\nEpoch 55/82\n128/128 [==============================] - 1s 9ms/step - loss: 0.0022 - mae: 0.0292 - val_loss: 0.0986 - val_mae: 0.2756\nEpoch 56/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0022 - mae: 0.0291 - val_loss: 0.0336 - val_mae: 0.1527\nEpoch 57/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0022 - mae: 0.0290 - val_loss: 0.0399 - val_mae: 0.1667\nEpoch 58/82\n128/128 [==============================] - 1s 9ms/step - loss: 0.0022 - mae: 0.0297 - val_loss: 0.0444 - val_mae: 0.1788\nEpoch 59/82\n128/128 [==============================] - 1s 9ms/step - loss: 0.0021 - mae: 0.0290 - val_loss: 0.0114 - val_mae: 0.0822\nEpoch 60/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0022 - mae: 0.0290 - val_loss: 0.0342 - val_mae: 0.1558\nEpoch 61/82\n128/128 [==============================] - 1s 9ms/step - loss: 0.0021 - mae: 0.0287 - val_loss: 0.0104 - val_mae: 0.0751\nEpoch 62/82\n\n\n128/128 [==============================] - 1s 8ms/step - loss: 0.0021 - mae: 0.0290 - val_loss: 0.0171 - val_mae: 0.1016\nEpoch 63/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0021 - mae: 0.0289 - val_loss: 0.0323 - val_mae: 0.1484\nEpoch 64/82\n128/128 [==============================] - 1s 7ms/step - loss: 0.0021 - mae: 0.0286 - val_loss: 0.0326 - val_mae: 0.1493\nEpoch 65/82\n128/128 [==============================] - 1s 7ms/step - loss: 0.0021 - mae: 0.0286 - val_loss: 0.0390 - val_mae: 0.1677\nEpoch 66/82\n128/128 [==============================] - 1s 7ms/step - loss: 0.0021 - mae: 0.0283 - val_loss: 0.0102 - val_mae: 0.0731\nEpoch 67/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0021 - mae: 0.0289 - val_loss: 0.0274 - val_mae: 0.1375\nEpoch 68/82\n128/128 [==============================] - 1s 9ms/step - loss: 0.0021 - mae: 0.0287 - val_loss: 0.0375 - val_mae: 0.1627\nEpoch 69/82\n128/128 [==============================] - 1s 9ms/step - loss: 0.0020 - mae: 0.0282 - val_loss: 0.0843 - val_mae: 0.2536\nEpoch 70/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0020 - mae: 0.0278 - val_loss: 0.0105 - val_mae: 0.0749\nEpoch 71/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0020 - mae: 0.0281 - val_loss: 0.0207 - val_mae: 0.1145\nEpoch 72/82\n128/128 [==============================] - 1s 7ms/step - loss: 0.0020 - mae: 0.0287 - val_loss: 0.0145 - val_mae: 0.0932\nEpoch 73/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0020 - mae: 0.0287 - val_loss: 0.0455 - val_mae: 0.1829\nEpoch 74/82\n128/128 [==============================] - 1s 7ms/step - loss: 0.0020 - mae: 0.0283 - val_loss: 0.0137 - val_mae: 0.0896\nEpoch 75/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0020 - mae: 0.0279 - val_loss: 0.0073 - val_mae: 0.0613\nEpoch 76/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0020 - mae: 0.0283 - val_loss: 0.0076 - val_mae: 0.0605\nEpoch 77/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0019 - mae: 0.0275 - val_loss: 0.0502 - val_mae: 0.1958\nEpoch 78/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0019 - mae: 0.0278 - val_loss: 0.0250 - val_mae: 0.1327\nEpoch 79/82\n128/128 [==============================] - 1s 7ms/step - loss: 0.0020 - mae: 0.0281 - val_loss: 0.0223 - val_mae: 0.1215\nEpoch 80/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0020 - mae: 0.0282 - val_loss: 0.0105 - val_mae: 0.0804\nEpoch 81/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0020 - mae: 0.0279 - val_loss: 0.0071 - val_mae: 0.0588\nEpoch 82/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0020 - mae: 0.0284 - val_loss: 0.0165 - val_mae: 0.1003\n11/11 [==============================] - 0s 3ms/step\n\n\n\n\n\n11/11 [==============================] - 0s 4ms/step"
  },
  {
    "objectID": "DLmodels.html",
    "href": "DLmodels.html",
    "title": "Time Series for Stock Price in Healthcare",
    "section": "",
    "text": "Show the code\nimport numpy as np\nimport pandas as pd\nfrom keras.models import Sequential\nfrom keras.layers import Dense, SimpleRNN, LSTM, GRU\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\nimport math\nimport matplotlib.pyplot as plt\nnp.random.seed(1)\nimport tensorflow as tf\ntf.random.set_seed(1)\nfrom keras import models\nfrom keras import layers\nfrom keras import optimizers\nimport os\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\n\n\nShow the code\n# Step 1 : import data\ndata=pd.read_csv('data/stock.csv')\nprint(data.shape)\ndata.head()\ndata = data[['AZN']]\ndata.head()\n\n\n(3290, 9)\n\n\n\n\n\n\n  \n    \n      \n      AZN\n    \n  \n  \n    \n      0\n      43.829044\n    \n    \n      1\n      44.240784\n    \n    \n      2\n      44.921402\n    \n    \n      3\n      46.425518\n    \n    \n      4\n      46.568356\n    \n  \n\n\n\n\n\n\nShow the code\n# Step 2 : split data\ndata = np.array(data.values.astype('float32'))\nscaler = MinMaxScaler(feature_range=(0, 1))\ndata = scaler.fit_transform(data).flatten()\nn = len(data)\n# Point for splitting data into train and test\nsplit = int(n * 0.8)\ntrain_data = data[range(split)]\ntest_data = data[split:]\n\n\n\n\nShow the code\n# Step 3 : Prepare the input X and target Y\ndef get_XY(dat, time_steps):\n    # Indices of target array\n    Y_ind = np.arange(time_steps, len(dat), time_steps)\n    Y = dat[Y_ind]\n    # Prepare X\n    rows_x = len(Y)\n    X = dat[range(time_steps * rows_x)]\n    X = np.reshape(X, (rows_x, time_steps, 1))\n    return X, Y\ntime_steps = 12\ntrainX, trainY = get_XY(train_data, time_steps)\ntestX, testY = get_XY(test_data, time_steps)\nmodel = input(\"choose between 3 model types (RNN, LSTM, GRU): \")\nif model == \"RNN\":\n## This is about Keras SimpleRNN:\n    def create_model(hidden_units, dense_units, input_shape, activation):\n        model = Sequential()\n        model.add(SimpleRNN(hidden_units, input_shape=input_shape,\n                        activation=activation[0]))\n        model.add(Dense(units=dense_units, activation=activation[1]))\n        model.compile(loss='mean_squared_error', optimizer='adam')\n        return model\nelif model == \"LSTM\":\n    def create_model(hidden_units, dense_units, input_shape, activation):\n        model = Sequential()\n        model.add(LSTM(hidden_units, input_shape=input_shape,\n                        activation=activation[0]))\n        model.add(Dense(units=dense_units, activation=activation[1]))\n        model.compile(loss='mean_squared_error', optimizer='adam')\n        return model\nelse:\n    def create_model(hidden_units, dense_units, input_shape, activation):\n        model = Sequential()\n        model.add(GRU(hidden_units, input_shape=input_shape,\n                        activation=activation[0]))\n        model.add(Dense(units=dense_units, activation=activation[1]))\n        model.compile(loss='mean_squared_error', optimizer='adam')\n        return model\n\n\nchoose between 3 model types (RNN, LSTM, GRU): GRU\n\n\n\n\nShow the code\n## Step 4: Create RNN Model And Train\n## reuse the function: creat_RNN()\nmodel = create_model(hidden_units=3, dense_units=1, input_shape=(time_steps,1),\n                   activation=['tanh', 'tanh'])\nmodel.fit(trainX, trainY, epochs=20, batch_size=1, verbose=2)\n\n\nEpoch 1/20\n219/219 - 10s - loss: 0.0069 - 10s/epoch - 47ms/step\nEpoch 2/20\n219/219 - 1s - loss: 0.0014 - 1s/epoch - 6ms/step\nEpoch 3/20\n219/219 - 1s - loss: 0.0010 - 1s/epoch - 6ms/step\nEpoch 4/20\n219/219 - 1s - loss: 9.8429e-04 - 1s/epoch - 5ms/step\nEpoch 5/20\n219/219 - 1s - loss: 8.5398e-04 - 1s/epoch - 6ms/step\nEpoch 6/20\n219/219 - 1s - loss: 7.8150e-04 - 1s/epoch - 6ms/step\nEpoch 7/20\n219/219 - 1s - loss: 7.4900e-04 - 1s/epoch - 6ms/step\nEpoch 8/20\n219/219 - 1s - loss: 6.6497e-04 - 1s/epoch - 6ms/step\nEpoch 9/20\n219/219 - 1s - loss: 6.3397e-04 - 1s/epoch - 6ms/step\nEpoch 10/20\n219/219 - 1s - loss: 5.5917e-04 - 1s/epoch - 6ms/step\nEpoch 11/20\n219/219 - 1s - loss: 5.1979e-04 - 1s/epoch - 6ms/step\nEpoch 12/20\n219/219 - 1s - loss: 4.6718e-04 - 1s/epoch - 6ms/step\nEpoch 13/20\n219/219 - 1s - loss: 4.3562e-04 - 1s/epoch - 6ms/step\nEpoch 14/20\n219/219 - 1s - loss: 3.9828e-04 - 1s/epoch - 6ms/step\nEpoch 15/20\n219/219 - 1s - loss: 3.5342e-04 - 1s/epoch - 6ms/step\nEpoch 16/20\n219/219 - 1s - loss: 3.7704e-04 - 1s/epoch - 6ms/step\nEpoch 17/20\n219/219 - 1s - loss: 3.2615e-04 - 1s/epoch - 6ms/step\nEpoch 18/20\n219/219 - 1s - loss: 3.1109e-04 - 1s/epoch - 6ms/step\nEpoch 19/20\n219/219 - 1s - loss: 2.7350e-04 - 1s/epoch - 6ms/step\nEpoch 20/20\n219/219 - 1s - loss: 2.7090e-04 - 1s/epoch - 6ms/step\n\n\n<keras.callbacks.History at 0x27c6fd234c0>\n\n\n\n\nShow the code\n## Step 5: Compute And Print The Root Mean Square Error\n## use the function: print_error()\ndef print_error(trainY, testY, train_predict, test_predict):\n    # Error of predictions\n    train_rmse = math.sqrt(mean_squared_error(trainY, train_predict))\n    test_rmse = math.sqrt(mean_squared_error(testY, test_predict))\n    # Print RMSE\n    print('Train RMSE: %.3f RMSE' % (train_rmse))\n    print('Test RMSE: %.3f RMSE' % (test_rmse))\n\n\n\n\nShow the code\n# make predictions\ntrain_predict = model.predict(trainX)\ntest_predict = model.predict(testX)\n# Mean square error of Training dataset and Testing dataset:\nprint_error(trainY, testY, train_predict, test_predict)\n\n\n7/7 [==============================] - 1s 4ms/step\n2/2 [==============================] - 0s 4ms/step\nTrain RMSE: 0.015 RMSE\nTest RMSE: 0.111 RMSE\n\n\n\n\nShow the code\n## Step 6: View The result\n# Plot the result\ndef plot_result(trainY, testY, train_predict, test_predict): ## define the content of the plot\n    actual = np.append(trainY, testY)\n    predictions = np.append(train_predict, test_predict)\n    rows = len(actual)\n    plt.figure(figsize=(15, 6), dpi=80)\n    plt.plot(range(rows), actual)\n    plt.plot(range(rows), predictions)\n    plt.axvline(x=len(trainY), color='r')\n    plt.legend(['Actual', 'Predictions'])\n    plt.xlabel('Observation number after given time steps')\n    plt.ylabel('Adjust close Price for AZN Stock Price')\n    plt.title('Actual and Predicted Values. The Red Line Separates The Training And Test Examples')\nplot_result(trainY, testY, train_predict, test_predict)\nplt.show() ## show the plot\n\n\n\n\n\n\n\nShow the code\ndata=pd.read_csv('data/azn.csv')\ndata.head()\n\n\n\n\n\n\n  \n    \n      \n      Date\n      AZN.Open\n      AZN.High\n      AZN.Low\n      AZN.Close\n      AZN.Volume\n      AZN.Adjusted\n    \n  \n  \n    \n      0\n      04/01/2010\n      23.709999\n      23.764999\n      23.575001\n      23.705000\n      2606200.0\n      13.440908\n    \n    \n      1\n      05/01/2010\n      23.434999\n      23.465000\n      23.150000\n      23.225000\n      2666600.0\n      13.168744\n    \n    \n      2\n      06/01/2010\n      22.915001\n      23.045000\n      22.785000\n      23.040001\n      3020800.0\n      13.063849\n    \n    \n      3\n      07/01/2010\n      23.250000\n      23.325001\n      23.120001\n      23.290001\n      4454600.0\n      13.205604\n    \n    \n      4\n      08/01/2010\n      23.270000\n      23.415001\n      23.174999\n      23.389999\n      2675600.0\n      13.262300\n    \n  \n\n\n\n\n\n\nShow the code\n\n## set training and testing dataset\nglobal lag     #forcast time lag\nlag=12\n\nnrow=data.shape[0]\ntrain_index=list(range(int(0.7*(nrow-lag))))\nvalidation_index=list(range(int(0.7*(nrow-lag)),int(0.9*(nrow-lag))))\ntest_index=list(range(int(0.9*(nrow-lag)),(nrow-lag)))\n\ndef generate_X_y(data,ex_rate):\n    tmp=data[ex_rate]\n    print('Raw data mean:',np.mean(tmp),'\\nRaw data std:',np.std(tmp))\n    tmp=(tmp-np.mean(tmp))/np.std(tmp)\n\n    X=np.zeros((nrow-lag,lag))\n    for i in range(nrow-lag):X[i,:lag]=tmp.iloc[i:i+lag]\n    y=np.array(tmp[lag:]).reshape((-1,1))\n    return (X,y)\n\n\n\n\nShow the code\n### CHECK train data of X and Y\nX,y=generate_X_y(data,'AZN.Close')\nX_train,y_train=X[train_index,:],y[train_index,:]\nX_validation,y_validation=X[validation_index,:],y[validation_index,:]\nX_test,y_test=X[test_index,:],y[test_index,:]\n\n\nRaw data mean: 36.80589211854097 \nRaw data std: 13.013520313929487\n\n\n\n\nShow the code\n## get the valiedate and test benchmark value\n## define drift validate benchmark:\n### CHECK with another method:\ndef training_performance(model, training_history, epochs):\n    test_MAE = np.mean(np.abs(y_test - model.predict(X_test.reshape(-1, lag, 1))))\n    timestep = range(1, epochs + 1)\n    plt.figure(figsize=(10, 8), facecolor='white')\n    plt.subplot(2, 1, 1)\n    plt.plot(timestep, np.log(training_history.history['val_mae']), 'b', label='Validation Mean Absolute Error')\n    plt.plot(timestep, np.log(training_history.history['mae']), 'bo', label='Training Mean Absolute Error')\n    plt.hlines(np.log(drift_validate_benchmark), xmin=timestep[0], xmax=timestep[-1], colors='coral',\n               label='Validation Drift Benchmark')\n    plt.hlines(np.log(mean_validate_benchmark), xmin=timestep[0], xmax=timestep[-1], colors='lightblue',\n               label='Validation Mean Benchmark')\n    plt.hlines(np.log(test_MAE), xmin=timestep[0], xmax=timestep[-1], colors='purple', label='Testing Mean Absolute Error')\n    plt.ylabel('logged Mean Absolute Error')\n    plt.xlabel('Epoch Value')\n    plt.legend(loc='upper right')\n    plt.subplot(2, 1, 2)\n    plt.hlines(test_MAE, xmin=timestep[0], xmax=timestep[-1], colors='purple', label='Testing Mean Absolute Error')\n    plt.hlines(drift_test_benchmark, xmin=timestep[0], xmax=timestep[-1], colors='coral', label='Test Drift Benchmark')\n    plt.hlines(mean_test_benchmark, xmin=timestep[0], xmax=timestep[-1], colors='lightblue',\n               label='Test Mean Benchmark')\n    plt.ylabel('Mean Absolute Error')\n    plt.legend(loc='right')\n    plt.show()\n\ndef get_Mae_benchmark(X,y):\n    mean_benchmark=np.mean(np.abs(np.mean(X,0)-y))\n    drift_benchmark=np.mean(np.abs(X[:,-1]-y))\n    return(mean_benchmark,drift_benchmark)\n\nmean_validate_benchmark,drift_validate_benchmark=get_Mae_benchmark(X_validation,y_validation)\nmean_test_benchmark,drift_test_benchmark=get_Mae_benchmark(X_test,y_test)\n\n\n\ndef plot_prediction(model, ex_rate):\n    tmp = data[ex_rate]\n    plt.figure(figsize=(10, 8), facecolor='white')\n    prediction = model.predict(X_test.reshape(-1, lag, 1))\n    prediction = prediction * np.std(tmp) + np.mean(tmp)\n    y_true = y_test * np.std(tmp) + np.mean(tmp)\n    plt.plot(list(range(len(prediction))), prediction, color='coral', label='Prediction')\n    plt.plot(list(range(len(y_true))), y_true, color='purple', label='True Value')\n    xticks = np.arange(0, len(y_true), 7)\n    plt.xticks(xticks, labels=data.Date.iloc[test_index].iloc[xticks], rotation=90)\n    plt.legend()\n    plt.show()\n\n\n\n\nShow the code\n### Simple RNN:\noptimizer=optimizers.RMSprop()\nmodel_RNN=models.Sequential()\nmodel_RNN.add(layers.SimpleRNN(32,input_shape=(lag,1),activation='relu'))\nmodel_RNN.add(layers.Dense(1))\n\nmodel_RNN.compile(optimizer=optimizer,loss='mse',metrics=['mae'])\n\nRnn_history=model_RNN.fit(X_train.reshape(-1,lag,1),y_train.flatten(),batch_size=18,epochs=82,\n                  validation_data=(X_validation.reshape(-1,lag,1),y_validation.flatten()))\n\ntraining_performance(model_RNN,Rnn_history,82)\nplot_prediction(model_RNN,'AZN.Close')\n\n\nEpoch 1/82\n128/128 [==============================] - 3s 11ms/step - loss: 0.0022 - mae: 0.0316 - val_loss: 0.2382 - val_mae: 0.4216\nEpoch 2/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0014 - mae: 0.0266 - val_loss: 0.0773 - val_mae: 0.2364\nEpoch 3/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0014 - mae: 0.0261 - val_loss: 0.0723 - val_mae: 0.2314\nEpoch 4/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0013 - mae: 0.0256 - val_loss: 0.0310 - val_mae: 0.1460\nEpoch 5/82\n128/128 [==============================] - 1s 7ms/step - loss: 0.0013 - mae: 0.0255 - val_loss: 0.0393 - val_mae: 0.1674\nEpoch 6/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0013 - mae: 0.0254 - val_loss: 0.0290 - val_mae: 0.1408\nEpoch 7/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0013 - mae: 0.0252 - val_loss: 0.0244 - val_mae: 0.1278\nEpoch 8/82\n128/128 [==============================] - 1s 7ms/step - loss: 0.0013 - mae: 0.0251 - val_loss: 0.0329 - val_mae: 0.1509\nEpoch 9/82\n128/128 [==============================] - 1s 7ms/step - loss: 0.0013 - mae: 0.0247 - val_loss: 0.0171 - val_mae: 0.1041\nEpoch 10/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0013 - mae: 0.0246 - val_loss: 0.0461 - val_mae: 0.1819\nEpoch 11/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0013 - mae: 0.0247 - val_loss: 0.0450 - val_mae: 0.1787\nEpoch 12/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0012 - mae: 0.0244 - val_loss: 0.0227 - val_mae: 0.1220\nEpoch 13/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0013 - mae: 0.0246 - val_loss: 0.0306 - val_mae: 0.1442\nEpoch 14/82\n128/128 [==============================] - 1s 7ms/step - loss: 0.0012 - mae: 0.0244 - val_loss: 0.0308 - val_mae: 0.1453\nEpoch 15/82\n128/128 [==============================] - 1s 7ms/step - loss: 0.0012 - mae: 0.0240 - val_loss: 0.0242 - val_mae: 0.1269\nEpoch 16/82\n128/128 [==============================] - 1s 7ms/step - loss: 0.0013 - mae: 0.0243 - val_loss: 0.0280 - val_mae: 0.1375\nEpoch 17/82\n128/128 [==============================] - 1s 7ms/step - loss: 0.0012 - mae: 0.0242 - val_loss: 0.0403 - val_mae: 0.1688\nEpoch 18/82\n128/128 [==============================] - 1s 7ms/step - loss: 0.0012 - mae: 0.0240 - val_loss: 0.0132 - val_mae: 0.0899\nEpoch 19/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0012 - mae: 0.0242 - val_loss: 0.0238 - val_mae: 0.1259\nEpoch 20/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0012 - mae: 0.0243 - val_loss: 0.0261 - val_mae: 0.1333\nEpoch 21/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0012 - mae: 0.0240 - val_loss: 0.0413 - val_mae: 0.1705\nEpoch 22/82\n128/128 [==============================] - 1s 7ms/step - loss: 0.0012 - mae: 0.0238 - val_loss: 0.0281 - val_mae: 0.1373\nEpoch 23/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0012 - mae: 0.0242 - val_loss: 0.0236 - val_mae: 0.1243\nEpoch 24/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0012 - mae: 0.0238 - val_loss: 0.0546 - val_mae: 0.1956\nEpoch 25/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0012 - mae: 0.0240 - val_loss: 0.0398 - val_mae: 0.1667\nEpoch 26/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0012 - mae: 0.0239 - val_loss: 0.0361 - val_mae: 0.1586\nEpoch 27/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0012 - mae: 0.0236 - val_loss: 0.0197 - val_mae: 0.1133\nEpoch 28/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0012 - mae: 0.0235 - val_loss: 0.0329 - val_mae: 0.1504\nEpoch 29/82\n128/128 [==============================] - 1s 7ms/step - loss: 0.0012 - mae: 0.0241 - val_loss: 0.0415 - val_mae: 0.1701\nEpoch 30/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0012 - mae: 0.0232 - val_loss: 0.0398 - val_mae: 0.1666\nEpoch 31/82\n128/128 [==============================] - 1s 9ms/step - loss: 0.0012 - mae: 0.0237 - val_loss: 0.0555 - val_mae: 0.1990\nEpoch 32/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0012 - mae: 0.0237 - val_loss: 0.0347 - val_mae: 0.1540\nEpoch 33/82\n128/128 [==============================] - 1s 7ms/step - loss: 0.0012 - mae: 0.0236 - val_loss: 0.0759 - val_mae: 0.2339\nEpoch 34/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0012 - mae: 0.0236 - val_loss: 0.0898 - val_mae: 0.2487\nEpoch 35/82\n128/128 [==============================] - 1s 7ms/step - loss: 0.0012 - mae: 0.0239 - val_loss: 0.0603 - val_mae: 0.2082\nEpoch 36/82\n128/128 [==============================] - 1s 7ms/step - loss: 0.0012 - mae: 0.0235 - val_loss: 0.1142 - val_mae: 0.2862\nEpoch 37/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0012 - mae: 0.0236 - val_loss: 0.0544 - val_mae: 0.1951\nEpoch 38/82\n128/128 [==============================] - 1s 7ms/step - loss: 0.0012 - mae: 0.0237 - val_loss: 0.2086 - val_mae: 0.3871\nEpoch 39/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0012 - mae: 0.0239 - val_loss: 0.1298 - val_mae: 0.3069\nEpoch 40/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0012 - mae: 0.0235 - val_loss: 0.0837 - val_mae: 0.2414\nEpoch 41/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0012 - mae: 0.0233 - val_loss: 0.0933 - val_mae: 0.2565\nEpoch 42/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0012 - mae: 0.0237 - val_loss: 0.0945 - val_mae: 0.2624\nEpoch 43/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0012 - mae: 0.0233 - val_loss: 0.0480 - val_mae: 0.1786\nEpoch 44/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0012 - mae: 0.0234 - val_loss: 0.0563 - val_mae: 0.1951\nEpoch 45/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0012 - mae: 0.0233 - val_loss: 0.0159 - val_mae: 0.0990\nEpoch 46/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0012 - mae: 0.0233 - val_loss: 0.0339 - val_mae: 0.1499\nEpoch 47/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0012 - mae: 0.0235 - val_loss: 0.0116 - val_mae: 0.0820\nEpoch 48/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0012 - mae: 0.0233 - val_loss: 0.0849 - val_mae: 0.2488\nEpoch 49/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0012 - mae: 0.0234 - val_loss: 0.0446 - val_mae: 0.1759\nEpoch 50/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0011 - mae: 0.0229 - val_loss: 0.0147 - val_mae: 0.0936\nEpoch 51/82\n128/128 [==============================] - 1s 9ms/step - loss: 0.0012 - mae: 0.0233 - val_loss: 0.0065 - val_mae: 0.0578\nEpoch 52/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0012 - mae: 0.0231 - val_loss: 0.0127 - val_mae: 0.0920\nEpoch 53/82\n128/128 [==============================] - 1s 7ms/step - loss: 0.0012 - mae: 0.0235 - val_loss: 0.0323 - val_mae: 0.1495\nEpoch 54/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0012 - mae: 0.0232 - val_loss: 0.0263 - val_mae: 0.1337\nEpoch 55/82\n128/128 [==============================] - 1s 7ms/step - loss: 0.0012 - mae: 0.0230 - val_loss: 0.0168 - val_mae: 0.1018\nEpoch 56/82\n128/128 [==============================] - 1s 7ms/step - loss: 0.0012 - mae: 0.0230 - val_loss: 0.0150 - val_mae: 0.0987\nEpoch 57/82\n128/128 [==============================] - 1s 6ms/step - loss: 0.0012 - mae: 0.0233 - val_loss: 0.0240 - val_mae: 0.1246\nEpoch 58/82\n128/128 [==============================] - 1s 7ms/step - loss: 0.0012 - mae: 0.0231 - val_loss: 0.0285 - val_mae: 0.1387\nEpoch 59/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0011 - mae: 0.0232 - val_loss: 0.0116 - val_mae: 0.0817\nEpoch 60/82\n128/128 [==============================] - 1s 7ms/step - loss: 0.0011 - mae: 0.0230 - val_loss: 0.0062 - val_mae: 0.0561\nEpoch 61/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0012 - mae: 0.0229 - val_loss: 0.0084 - val_mae: 0.0683\nEpoch 62/82\n\n\n128/128 [==============================] - 1s 8ms/step - loss: 0.0011 - mae: 0.0230 - val_loss: 0.0255 - val_mae: 0.1297\nEpoch 63/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0011 - mae: 0.0231 - val_loss: 0.0063 - val_mae: 0.0557\nEpoch 64/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0011 - mae: 0.0230 - val_loss: 0.0122 - val_mae: 0.0861\nEpoch 65/82\n128/128 [==============================] - 1s 9ms/step - loss: 0.0011 - mae: 0.0230 - val_loss: 0.0152 - val_mae: 0.0997\nEpoch 66/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0011 - mae: 0.0229 - val_loss: 0.0157 - val_mae: 0.1008\nEpoch 67/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0011 - mae: 0.0231 - val_loss: 0.0459 - val_mae: 0.1821\nEpoch 68/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0012 - mae: 0.0230 - val_loss: 0.0144 - val_mae: 0.0959\nEpoch 69/82\n128/128 [==============================] - 1s 9ms/step - loss: 0.0011 - mae: 0.0230 - val_loss: 0.0444 - val_mae: 0.1796\nEpoch 70/82\n128/128 [==============================] - 1s 9ms/step - loss: 0.0011 - mae: 0.0232 - val_loss: 0.0070 - val_mae: 0.0598\nEpoch 71/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0011 - mae: 0.0229 - val_loss: 0.0183 - val_mae: 0.1105\nEpoch 72/82\n128/128 [==============================] - 1s 7ms/step - loss: 0.0011 - mae: 0.0231 - val_loss: 0.0131 - val_mae: 0.0904\nEpoch 73/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0011 - mae: 0.0231 - val_loss: 0.0159 - val_mae: 0.1037\nEpoch 74/82\n128/128 [==============================] - 1s 7ms/step - loss: 0.0011 - mae: 0.0229 - val_loss: 0.0068 - val_mae: 0.0565\nEpoch 75/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0011 - mae: 0.0229 - val_loss: 0.0259 - val_mae: 0.1338\nEpoch 76/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0011 - mae: 0.0226 - val_loss: 0.0114 - val_mae: 0.0813\nEpoch 77/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0011 - mae: 0.0228 - val_loss: 0.0196 - val_mae: 0.1144\nEpoch 78/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0011 - mae: 0.0228 - val_loss: 0.0061 - val_mae: 0.0537\nEpoch 79/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0011 - mae: 0.0229 - val_loss: 0.0342 - val_mae: 0.1598\nEpoch 80/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0011 - mae: 0.0229 - val_loss: 0.0081 - val_mae: 0.0653\nEpoch 81/82\n128/128 [==============================] - 1s 9ms/step - loss: 0.0011 - mae: 0.0225 - val_loss: 0.0244 - val_mae: 0.1283\nEpoch 82/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0011 - mae: 0.0226 - val_loss: 0.0278 - val_mae: 0.1401\n11/11 [==============================] - 0s 4ms/step\n\n\n\n\n\n11/11 [==============================] - 0s 5ms/step\n\n\n\n\n\n\n\nShow the code\n### LSTM:\nmodel_LSTM=models.Sequential()\nmodel_LSTM.add(layers.LSTM(32,input_shape=(lag,1),activation='relu'))\nmodel_LSTM.add(layers.Dense(1))\nmodel_LSTM.compile(optimizer=optimizers.RMSprop(),loss='mse',metrics=['mae'])\n\nhistory_LSTM=model_LSTM.fit(X_train.reshape(-1,lag,1),y_train.flatten(),batch_size=18,epochs=82,\n                  validation_data=(X_validation.reshape(-1,lag,1),y_validation.flatten()))\n\ntraining_performance(model_LSTM,history_LSTM,82)\nplot_prediction(model_LSTM,'AZN.Close')\n\n\nEpoch 1/82\n128/128 [==============================] - 4s 15ms/step - loss: 0.0247 - mae: 0.0865 - val_loss: 0.0533 - val_mae: 0.1904\nEpoch 2/82\n128/128 [==============================] - 1s 11ms/step - loss: 0.0034 - mae: 0.0430 - val_loss: 0.1948 - val_mae: 0.2912\nEpoch 3/82\n128/128 [==============================] - 1s 12ms/step - loss: 0.0030 - mae: 0.0401 - val_loss: 0.3901 - val_mae: 0.4168\nEpoch 4/82\n128/128 [==============================] - 1s 11ms/step - loss: 0.0027 - mae: 0.0380 - val_loss: 0.9257 - val_mae: 0.6733\nEpoch 5/82\n128/128 [==============================] - 1s 11ms/step - loss: 0.0024 - mae: 0.0358 - val_loss: 0.5050 - val_mae: 0.5139\nEpoch 6/82\n128/128 [==============================] - 2s 12ms/step - loss: 0.0023 - mae: 0.0349 - val_loss: 0.5171 - val_mae: 0.5533\nEpoch 7/82\n128/128 [==============================] - 1s 11ms/step - loss: 0.0022 - mae: 0.0342 - val_loss: 0.1893 - val_mae: 0.3493\nEpoch 8/82\n128/128 [==============================] - 1s 12ms/step - loss: 0.0021 - mae: 0.0327 - val_loss: 0.1420 - val_mae: 0.2924\nEpoch 9/82\n128/128 [==============================] - 2s 12ms/step - loss: 0.0019 - mae: 0.0317 - val_loss: 0.0656 - val_mae: 0.1797\nEpoch 10/82\n128/128 [==============================] - 1s 11ms/step - loss: 0.0018 - mae: 0.0313 - val_loss: 0.1783 - val_mae: 0.2808\nEpoch 11/82\n128/128 [==============================] - 2s 12ms/step - loss: 0.0018 - mae: 0.0304 - val_loss: 0.9737 - val_mae: 0.7143\nEpoch 12/82\n128/128 [==============================] - 2s 12ms/step - loss: 0.0017 - mae: 0.0300 - val_loss: 0.7187 - val_mae: 0.6379\nEpoch 13/82\n128/128 [==============================] - 2s 12ms/step - loss: 0.0017 - mae: 0.0298 - val_loss: 1.2363 - val_mae: 0.8644\nEpoch 14/82\n128/128 [==============================] - 2s 12ms/step - loss: 0.0017 - mae: 0.0291 - val_loss: 1.3104 - val_mae: 0.9093\nEpoch 15/82\n128/128 [==============================] - 2s 13ms/step - loss: 0.0016 - mae: 0.0287 - val_loss: 0.7737 - val_mae: 0.6930\nEpoch 16/82\n128/128 [==============================] - 2s 13ms/step - loss: 0.0016 - mae: 0.0286 - val_loss: 0.7802 - val_mae: 0.6813\nEpoch 17/82\n128/128 [==============================] - 2s 12ms/step - loss: 0.0015 - mae: 0.0280 - val_loss: 0.6150 - val_mae: 0.5594\nEpoch 18/82\n128/128 [==============================] - 2s 12ms/step - loss: 0.0015 - mae: 0.0275 - val_loss: 2.7559 - val_mae: 1.2126\nEpoch 19/82\n128/128 [==============================] - 2s 12ms/step - loss: 0.0015 - mae: 0.0272 - val_loss: 1.5730 - val_mae: 0.8357\nEpoch 20/82\n128/128 [==============================] - 2s 13ms/step - loss: 0.0015 - mae: 0.0276 - val_loss: 1.2974 - val_mae: 0.7360\nEpoch 21/82\n128/128 [==============================] - 2s 12ms/step - loss: 0.0015 - mae: 0.0270 - val_loss: 1.8591 - val_mae: 0.9150\nEpoch 22/82\n128/128 [==============================] - 2s 13ms/step - loss: 0.0014 - mae: 0.0265 - val_loss: 1.3237 - val_mae: 0.7970\nEpoch 23/82\n128/128 [==============================] - 2s 12ms/step - loss: 0.0014 - mae: 0.0269 - val_loss: 1.3082 - val_mae: 0.8086\nEpoch 24/82\n128/128 [==============================] - 1s 11ms/step - loss: 0.0014 - mae: 0.0264 - val_loss: 0.7604 - val_mae: 0.6189\nEpoch 25/82\n128/128 [==============================] - 2s 13ms/step - loss: 0.0014 - mae: 0.0264 - val_loss: 0.4920 - val_mae: 0.4838\nEpoch 26/82\n128/128 [==============================] - 2s 13ms/step - loss: 0.0014 - mae: 0.0262 - val_loss: 0.3198 - val_mae: 0.3882\nEpoch 27/82\n128/128 [==============================] - 2s 13ms/step - loss: 0.0013 - mae: 0.0258 - val_loss: 0.4018 - val_mae: 0.4521\nEpoch 28/82\n128/128 [==============================] - 2s 13ms/step - loss: 0.0013 - mae: 0.0254 - val_loss: 0.2684 - val_mae: 0.3620\nEpoch 29/82\n128/128 [==============================] - 2s 12ms/step - loss: 0.0014 - mae: 0.0259 - val_loss: 0.2030 - val_mae: 0.3151\nEpoch 30/82\n128/128 [==============================] - 2s 13ms/step - loss: 0.0013 - mae: 0.0249 - val_loss: 0.1600 - val_mae: 0.2765\nEpoch 31/82\n128/128 [==============================] - 2s 13ms/step - loss: 0.0013 - mae: 0.0255 - val_loss: 0.1683 - val_mae: 0.2848\nEpoch 32/82\n128/128 [==============================] - 2s 13ms/step - loss: 0.0013 - mae: 0.0253 - val_loss: 0.1987 - val_mae: 0.3198\nEpoch 33/82\n128/128 [==============================] - 2s 12ms/step - loss: 0.0013 - mae: 0.0250 - val_loss: 0.1288 - val_mae: 0.2442\nEpoch 34/82\n128/128 [==============================] - 2s 13ms/step - loss: 0.0013 - mae: 0.0249 - val_loss: 0.1747 - val_mae: 0.2970\nEpoch 35/82\n128/128 [==============================] - 2s 12ms/step - loss: 0.0013 - mae: 0.0252 - val_loss: 0.0673 - val_mae: 0.1707\nEpoch 36/82\n128/128 [==============================] - 2s 13ms/step - loss: 0.0013 - mae: 0.0251 - val_loss: 0.1740 - val_mae: 0.2973\nEpoch 37/82\n128/128 [==============================] - 2s 12ms/step - loss: 0.0013 - mae: 0.0248 - val_loss: 0.1199 - val_mae: 0.2353\nEpoch 38/82\n128/128 [==============================] - 2s 13ms/step - loss: 0.0013 - mae: 0.0250 - val_loss: 0.1461 - val_mae: 0.2643\nEpoch 39/82\n128/128 [==============================] - 2s 13ms/step - loss: 0.0013 - mae: 0.0248 - val_loss: 0.1131 - val_mae: 0.2228\nEpoch 40/82\n128/128 [==============================] - 2s 13ms/step - loss: 0.0013 - mae: 0.0248 - val_loss: 0.0911 - val_mae: 0.1997\nEpoch 41/82\n128/128 [==============================] - 2s 12ms/step - loss: 0.0013 - mae: 0.0243 - val_loss: 0.0724 - val_mae: 0.1750\nEpoch 42/82\n128/128 [==============================] - 2s 12ms/step - loss: 0.0013 - mae: 0.0250 - val_loss: 0.0947 - val_mae: 0.2043\nEpoch 43/82\n128/128 [==============================] - 2s 12ms/step - loss: 0.0013 - mae: 0.0243 - val_loss: 0.1189 - val_mae: 0.2389\nEpoch 44/82\n128/128 [==============================] - 2s 12ms/step - loss: 0.0013 - mae: 0.0245 - val_loss: 0.0764 - val_mae: 0.1801\nEpoch 45/82\n128/128 [==============================] - 2s 13ms/step - loss: 0.0013 - mae: 0.0244 - val_loss: 0.0716 - val_mae: 0.1756\nEpoch 46/82\n128/128 [==============================] - 2s 12ms/step - loss: 0.0012 - mae: 0.0242 - val_loss: 0.1168 - val_mae: 0.2240\nEpoch 47/82\n128/128 [==============================] - 2s 13ms/step - loss: 0.0013 - mae: 0.0245 - val_loss: 0.0859 - val_mae: 0.1906\nEpoch 48/82\n128/128 [==============================] - 2s 13ms/step - loss: 0.0013 - mae: 0.0243 - val_loss: 0.0699 - val_mae: 0.1721\nEpoch 49/82\n128/128 [==============================] - 2s 13ms/step - loss: 0.0012 - mae: 0.0242 - val_loss: 0.1052 - val_mae: 0.2207\nEpoch 50/82\n128/128 [==============================] - 2s 13ms/step - loss: 0.0012 - mae: 0.0238 - val_loss: 0.0834 - val_mae: 0.1917\nEpoch 51/82\n128/128 [==============================] - 2s 13ms/step - loss: 0.0012 - mae: 0.0242 - val_loss: 0.0663 - val_mae: 0.1676\nEpoch 52/82\n128/128 [==============================] - 2s 13ms/step - loss: 0.0012 - mae: 0.0240 - val_loss: 0.0589 - val_mae: 0.1561\nEpoch 53/82\n128/128 [==============================] - 2s 13ms/step - loss: 0.0012 - mae: 0.0242 - val_loss: 0.0384 - val_mae: 0.1244\nEpoch 54/82\n128/128 [==============================] - 2s 13ms/step - loss: 0.0012 - mae: 0.0240 - val_loss: 0.0380 - val_mae: 0.1237\nEpoch 55/82\n128/128 [==============================] - 1s 12ms/step - loss: 0.0012 - mae: 0.0239 - val_loss: 0.0391 - val_mae: 0.1249\nEpoch 56/82\n128/128 [==============================] - 2s 13ms/step - loss: 0.0012 - mae: 0.0236 - val_loss: 0.0578 - val_mae: 0.1554\nEpoch 57/82\n128/128 [==============================] - 2s 13ms/step - loss: 0.0012 - mae: 0.0241 - val_loss: 0.0385 - val_mae: 0.1244\nEpoch 58/82\n128/128 [==============================] - 2s 12ms/step - loss: 0.0012 - mae: 0.0241 - val_loss: 0.0370 - val_mae: 0.1212\nEpoch 59/82\n128/128 [==============================] - 1s 12ms/step - loss: 0.0012 - mae: 0.0240 - val_loss: 0.0477 - val_mae: 0.1436\nEpoch 60/82\n128/128 [==============================] - 2s 13ms/step - loss: 0.0012 - mae: 0.0239 - val_loss: 0.0759 - val_mae: 0.1881\nEpoch 61/82\n128/128 [==============================] - 2s 12ms/step - loss: 0.0012 - mae: 0.0237 - val_loss: 0.0666 - val_mae: 0.1646\nEpoch 62/82\n\n\n128/128 [==============================] - 2s 12ms/step - loss: 0.0012 - mae: 0.0237 - val_loss: 0.0529 - val_mae: 0.1448\nEpoch 63/82\n128/128 [==============================] - 1s 12ms/step - loss: 0.0012 - mae: 0.0238 - val_loss: 0.0736 - val_mae: 0.1711\nEpoch 64/82\n128/128 [==============================] - 2s 12ms/step - loss: 0.0012 - mae: 0.0238 - val_loss: 0.0587 - val_mae: 0.1516\nEpoch 65/82\n128/128 [==============================] - 2s 12ms/step - loss: 0.0012 - mae: 0.0237 - val_loss: 0.0480 - val_mae: 0.1340\nEpoch 66/82\n128/128 [==============================] - 2s 13ms/step - loss: 0.0012 - mae: 0.0238 - val_loss: 0.0876 - val_mae: 0.1817\nEpoch 67/82\n128/128 [==============================] - 2s 12ms/step - loss: 0.0012 - mae: 0.0238 - val_loss: 0.0717 - val_mae: 0.1618\nEpoch 68/82\n128/128 [==============================] - 2s 12ms/step - loss: 0.0012 - mae: 0.0238 - val_loss: 0.0972 - val_mae: 0.1859\nEpoch 69/82\n128/128 [==============================] - 2s 12ms/step - loss: 0.0012 - mae: 0.0237 - val_loss: 0.0619 - val_mae: 0.1476\nEpoch 70/82\n128/128 [==============================] - 2s 12ms/step - loss: 0.0012 - mae: 0.0237 - val_loss: 0.1218 - val_mae: 0.2077\nEpoch 71/82\n128/128 [==============================] - 2s 12ms/step - loss: 0.0012 - mae: 0.0236 - val_loss: 0.0864 - val_mae: 0.1716\nEpoch 72/82\n128/128 [==============================] - 2s 13ms/step - loss: 0.0012 - mae: 0.0238 - val_loss: 0.1382 - val_mae: 0.2143\nEpoch 73/82\n128/128 [==============================] - 2s 12ms/step - loss: 0.0012 - mae: 0.0240 - val_loss: 0.1085 - val_mae: 0.1874\nEpoch 74/82\n128/128 [==============================] - 2s 14ms/step - loss: 0.0012 - mae: 0.0238 - val_loss: 0.1285 - val_mae: 0.2038\nEpoch 75/82\n128/128 [==============================] - 2s 12ms/step - loss: 0.0012 - mae: 0.0237 - val_loss: 0.1576 - val_mae: 0.2251\nEpoch 76/82\n128/128 [==============================] - 2s 12ms/step - loss: 0.0012 - mae: 0.0236 - val_loss: 0.3030 - val_mae: 0.3163\nEpoch 77/82\n128/128 [==============================] - 2s 13ms/step - loss: 0.0012 - mae: 0.0236 - val_loss: 0.2185 - val_mae: 0.2570\nEpoch 78/82\n128/128 [==============================] - 2s 12ms/step - loss: 0.0012 - mae: 0.0236 - val_loss: 0.2339 - val_mae: 0.2620\nEpoch 79/82\n128/128 [==============================] - 2s 12ms/step - loss: 0.0012 - mae: 0.0236 - val_loss: 0.3432 - val_mae: 0.3210\nEpoch 80/82\n128/128 [==============================] - 2s 13ms/step - loss: 0.0012 - mae: 0.0236 - val_loss: 0.2762 - val_mae: 0.2818\nEpoch 81/82\n128/128 [==============================] - 2s 13ms/step - loss: 0.0012 - mae: 0.0234 - val_loss: 0.3578 - val_mae: 0.3194\nEpoch 82/82\n128/128 [==============================] - 2s 13ms/step - loss: 0.0012 - mae: 0.0235 - val_loss: 0.5044 - val_mae: 0.3817\n11/11 [==============================] - 0s 5ms/step\n\n\n\n\n\n11/11 [==============================] - 0s 6ms/step\n\n\n\n\n\n\n\nShow the code\n### GRU:\nmodel_GRU=models.Sequential()\nmodel_GRU.add(layers.GRU(32,input_shape=(lag,1),activation='relu'))\nmodel_GRU.add(layers.Dense(1))\nmodel_GRU.compile(optimizer=optimizers.RMSprop(),loss='mse',metrics=['mae'])\n\nhistory_GRU=model_GRU.fit(X_train.reshape(-1,lag,1),y_train.flatten(),batch_size=18,epochs=82,\n                  validation_data=(X_validation.reshape(-1,lag,1),y_validation.flatten()))\n\ntraining_performance(model_GRU,history_GRU,82)\nplot_prediction(model_GRU,'AZN.Close')\n\n\nEpoch 1/82\n128/128 [==============================] - 3s 13ms/step - loss: 0.0472 - mae: 0.1111 - val_loss: 0.4241 - val_mae: 0.5620\nEpoch 2/82\n128/128 [==============================] - 1s 12ms/step - loss: 0.0017 - mae: 0.0295 - val_loss: 0.3729 - val_mae: 0.5259\nEpoch 3/82\n128/128 [==============================] - 1s 11ms/step - loss: 0.0016 - mae: 0.0284 - val_loss: 0.3304 - val_mae: 0.4927\nEpoch 4/82\n128/128 [==============================] - 1s 11ms/step - loss: 0.0015 - mae: 0.0268 - val_loss: 0.2802 - val_mae: 0.4505\nEpoch 5/82\n128/128 [==============================] - 2s 12ms/step - loss: 0.0014 - mae: 0.0261 - val_loss: 0.2821 - val_mae: 0.4514\nEpoch 6/82\n128/128 [==============================] - 1s 11ms/step - loss: 0.0014 - mae: 0.0259 - val_loss: 0.2521 - val_mae: 0.4247\nEpoch 7/82\n128/128 [==============================] - 2s 12ms/step - loss: 0.0013 - mae: 0.0256 - val_loss: 0.2549 - val_mae: 0.4277\nEpoch 8/82\n128/128 [==============================] - 1s 10ms/step - loss: 0.0014 - mae: 0.0257 - val_loss: 0.2453 - val_mae: 0.4177\nEpoch 9/82\n128/128 [==============================] - 1s 11ms/step - loss: 0.0013 - mae: 0.0249 - val_loss: 0.2230 - val_mae: 0.3971\nEpoch 10/82\n128/128 [==============================] - 1s 11ms/step - loss: 0.0013 - mae: 0.0249 - val_loss: 0.2626 - val_mae: 0.4367\nEpoch 11/82\n128/128 [==============================] - 2s 12ms/step - loss: 0.0013 - mae: 0.0250 - val_loss: 0.2378 - val_mae: 0.4109\nEpoch 12/82\n128/128 [==============================] - 1s 10ms/step - loss: 0.0013 - mae: 0.0246 - val_loss: 0.2263 - val_mae: 0.4002\nEpoch 13/82\n128/128 [==============================] - 1s 11ms/step - loss: 0.0013 - mae: 0.0249 - val_loss: 0.2183 - val_mae: 0.3922\nEpoch 14/82\n128/128 [==============================] - 1s 11ms/step - loss: 0.0013 - mae: 0.0245 - val_loss: 0.2315 - val_mae: 0.4051\nEpoch 15/82\n128/128 [==============================] - 1s 11ms/step - loss: 0.0013 - mae: 0.0244 - val_loss: 0.2355 - val_mae: 0.4080\nEpoch 16/82\n128/128 [==============================] - 2s 12ms/step - loss: 0.0013 - mae: 0.0245 - val_loss: 0.2324 - val_mae: 0.4053\nEpoch 17/82\n128/128 [==============================] - 2s 13ms/step - loss: 0.0013 - mae: 0.0244 - val_loss: 0.2579 - val_mae: 0.4295\nEpoch 18/82\n128/128 [==============================] - 2s 12ms/step - loss: 0.0013 - mae: 0.0241 - val_loss: 0.2212 - val_mae: 0.3935\nEpoch 19/82\n128/128 [==============================] - 1s 11ms/step - loss: 0.0013 - mae: 0.0244 - val_loss: 0.2469 - val_mae: 0.4193\nEpoch 20/82\n128/128 [==============================] - 1s 12ms/step - loss: 0.0013 - mae: 0.0246 - val_loss: 0.2549 - val_mae: 0.4266\nEpoch 21/82\n128/128 [==============================] - 1s 11ms/step - loss: 0.0013 - mae: 0.0242 - val_loss: 0.2541 - val_mae: 0.4245\nEpoch 22/82\n128/128 [==============================] - 2s 12ms/step - loss: 0.0012 - mae: 0.0239 - val_loss: 0.2287 - val_mae: 0.4011\nEpoch 23/82\n128/128 [==============================] - 1s 11ms/step - loss: 0.0013 - mae: 0.0244 - val_loss: 0.2231 - val_mae: 0.3962\nEpoch 24/82\n128/128 [==============================] - 2s 13ms/step - loss: 0.0012 - mae: 0.0242 - val_loss: 0.2132 - val_mae: 0.3857\nEpoch 25/82\n128/128 [==============================] - 2s 15ms/step - loss: 0.0013 - mae: 0.0243 - val_loss: 0.2529 - val_mae: 0.4238\nEpoch 26/82\n128/128 [==============================] - 1s 11ms/step - loss: 0.0012 - mae: 0.0243 - val_loss: 0.2585 - val_mae: 0.4294\nEpoch 27/82\n128/128 [==============================] - 1s 9ms/step - loss: 0.0012 - mae: 0.0239 - val_loss: 0.2260 - val_mae: 0.3982\nEpoch 28/82\n128/128 [==============================] - 1s 9ms/step - loss: 0.0012 - mae: 0.0237 - val_loss: 0.2389 - val_mae: 0.4105\nEpoch 29/82\n128/128 [==============================] - 1s 10ms/step - loss: 0.0013 - mae: 0.0242 - val_loss: 0.2407 - val_mae: 0.4123\nEpoch 30/82\n128/128 [==============================] - 1s 10ms/step - loss: 0.0012 - mae: 0.0235 - val_loss: 0.2489 - val_mae: 0.4208\nEpoch 31/82\n128/128 [==============================] - 1s 10ms/step - loss: 0.0012 - mae: 0.0242 - val_loss: 0.2368 - val_mae: 0.4085\nEpoch 32/82\n128/128 [==============================] - 1s 11ms/step - loss: 0.0012 - mae: 0.0240 - val_loss: 0.2235 - val_mae: 0.3960\nEpoch 33/82\n128/128 [==============================] - 1s 11ms/step - loss: 0.0012 - mae: 0.0238 - val_loss: 0.2365 - val_mae: 0.4089\nEpoch 34/82\n128/128 [==============================] - 1s 11ms/step - loss: 0.0012 - mae: 0.0238 - val_loss: 0.2224 - val_mae: 0.3953\nEpoch 35/82\n128/128 [==============================] - 1s 11ms/step - loss: 0.0012 - mae: 0.0241 - val_loss: 0.2546 - val_mae: 0.4266\nEpoch 36/82\n128/128 [==============================] - 1s 11ms/step - loss: 0.0012 - mae: 0.0240 - val_loss: 0.2192 - val_mae: 0.3915\nEpoch 37/82\n128/128 [==============================] - 2s 13ms/step - loss: 0.0012 - mae: 0.0239 - val_loss: 0.2248 - val_mae: 0.3978\nEpoch 38/82\n128/128 [==============================] - 1s 11ms/step - loss: 0.0012 - mae: 0.0241 - val_loss: 0.2227 - val_mae: 0.3948\nEpoch 39/82\n128/128 [==============================] - 1s 11ms/step - loss: 0.0012 - mae: 0.0241 - val_loss: 0.2375 - val_mae: 0.4090\nEpoch 40/82\n128/128 [==============================] - 1s 11ms/step - loss: 0.0012 - mae: 0.0240 - val_loss: 0.2423 - val_mae: 0.4134\nEpoch 41/82\n128/128 [==============================] - 1s 11ms/step - loss: 0.0012 - mae: 0.0237 - val_loss: 0.2640 - val_mae: 0.4330\nEpoch 42/82\n128/128 [==============================] - 1s 12ms/step - loss: 0.0012 - mae: 0.0242 - val_loss: 0.2399 - val_mae: 0.4110\nEpoch 43/82\n128/128 [==============================] - 2s 12ms/step - loss: 0.0012 - mae: 0.0237 - val_loss: 0.2266 - val_mae: 0.3979\nEpoch 44/82\n128/128 [==============================] - 1s 11ms/step - loss: 0.0012 - mae: 0.0238 - val_loss: 0.2408 - val_mae: 0.4116\nEpoch 45/82\n128/128 [==============================] - 1s 11ms/step - loss: 0.0012 - mae: 0.0238 - val_loss: 0.2537 - val_mae: 0.4233\nEpoch 46/82\n128/128 [==============================] - 1s 11ms/step - loss: 0.0012 - mae: 0.0236 - val_loss: 0.2255 - val_mae: 0.3979\nEpoch 47/82\n128/128 [==============================] - 1s 11ms/step - loss: 0.0012 - mae: 0.0240 - val_loss: 0.2295 - val_mae: 0.4019\nEpoch 48/82\n128/128 [==============================] - 1s 11ms/step - loss: 0.0012 - mae: 0.0239 - val_loss: 0.2366 - val_mae: 0.4082\nEpoch 49/82\n128/128 [==============================] - 2s 13ms/step - loss: 0.0012 - mae: 0.0239 - val_loss: 0.2145 - val_mae: 0.3865\nEpoch 50/82\n128/128 [==============================] - 2s 13ms/step - loss: 0.0012 - mae: 0.0235 - val_loss: 0.2146 - val_mae: 0.3873\nEpoch 51/82\n128/128 [==============================] - 1s 11ms/step - loss: 0.0012 - mae: 0.0237 - val_loss: 0.2295 - val_mae: 0.4016\nEpoch 52/82\n128/128 [==============================] - 1s 11ms/step - loss: 0.0012 - mae: 0.0236 - val_loss: 0.2408 - val_mae: 0.4125\nEpoch 53/82\n128/128 [==============================] - 1s 11ms/step - loss: 0.0012 - mae: 0.0239 - val_loss: 0.2434 - val_mae: 0.4153\nEpoch 54/82\n128/128 [==============================] - 1s 11ms/step - loss: 0.0012 - mae: 0.0237 - val_loss: 0.2414 - val_mae: 0.4141\nEpoch 55/82\n128/128 [==============================] - 1s 11ms/step - loss: 0.0012 - mae: 0.0235 - val_loss: 0.2461 - val_mae: 0.4170\nEpoch 56/82\n128/128 [==============================] - 1s 10ms/step - loss: 0.0012 - mae: 0.0234 - val_loss: 0.2295 - val_mae: 0.4012\nEpoch 57/82\n128/128 [==============================] - 1s 11ms/step - loss: 0.0012 - mae: 0.0240 - val_loss: 0.2508 - val_mae: 0.4214\nEpoch 58/82\n128/128 [==============================] - 1s 11ms/step - loss: 0.0012 - mae: 0.0239 - val_loss: 0.2478 - val_mae: 0.4186\nEpoch 59/82\n128/128 [==============================] - 1s 12ms/step - loss: 0.0012 - mae: 0.0237 - val_loss: 0.2269 - val_mae: 0.3986\nEpoch 60/82\n128/128 [==============================] - 1s 11ms/step - loss: 0.0012 - mae: 0.0237 - val_loss: 0.2035 - val_mae: 0.3754\nEpoch 61/82\n128/128 [==============================] - 2s 12ms/step - loss: 0.0012 - mae: 0.0234 - val_loss: 0.2164 - val_mae: 0.3890\nEpoch 62/82\n\n\n128/128 [==============================] - 1s 11ms/step - loss: 0.0012 - mae: 0.0234 - val_loss: 0.2227 - val_mae: 0.3950\nEpoch 63/82\n128/128 [==============================] - 1s 12ms/step - loss: 0.0012 - mae: 0.0236 - val_loss: 0.2181 - val_mae: 0.3902\nEpoch 64/82\n128/128 [==============================] - 2s 12ms/step - loss: 0.0012 - mae: 0.0235 - val_loss: 0.2210 - val_mae: 0.3931\nEpoch 65/82\n128/128 [==============================] - 1s 11ms/step - loss: 0.0012 - mae: 0.0235 - val_loss: 0.2520 - val_mae: 0.4235\nEpoch 66/82\n128/128 [==============================] - 2s 12ms/step - loss: 0.0012 - mae: 0.0236 - val_loss: 0.2254 - val_mae: 0.3979\nEpoch 67/82\n128/128 [==============================] - 2s 12ms/step - loss: 0.0012 - mae: 0.0237 - val_loss: 0.2377 - val_mae: 0.4098\nEpoch 68/82\n128/128 [==============================] - 2s 12ms/step - loss: 0.0012 - mae: 0.0237 - val_loss: 0.2292 - val_mae: 0.4011\nEpoch 69/82\n128/128 [==============================] - 2s 12ms/step - loss: 0.0012 - mae: 0.0236 - val_loss: 0.2670 - val_mae: 0.4363\nEpoch 70/82\n128/128 [==============================] - 1s 11ms/step - loss: 0.0012 - mae: 0.0236 - val_loss: 0.2022 - val_mae: 0.3750\nEpoch 71/82\n128/128 [==============================] - 1s 11ms/step - loss: 0.0012 - mae: 0.0235 - val_loss: 0.2234 - val_mae: 0.3959\nEpoch 72/82\n128/128 [==============================] - 2s 12ms/step - loss: 0.0012 - mae: 0.0236 - val_loss: 0.2106 - val_mae: 0.3838\nEpoch 73/82\n128/128 [==============================] - 1s 12ms/step - loss: 0.0012 - mae: 0.0237 - val_loss: 0.2251 - val_mae: 0.3987\nEpoch 74/82\n128/128 [==============================] - 1s 11ms/step - loss: 0.0012 - mae: 0.0237 - val_loss: 0.2217 - val_mae: 0.3942\nEpoch 75/82\n128/128 [==============================] - 1s 11ms/step - loss: 0.0012 - mae: 0.0235 - val_loss: 0.2040 - val_mae: 0.3765\nEpoch 76/82\n128/128 [==============================] - 1s 11ms/step - loss: 0.0012 - mae: 0.0235 - val_loss: 0.1910 - val_mae: 0.3635\nEpoch 77/82\n128/128 [==============================] - 1s 11ms/step - loss: 0.0012 - mae: 0.0236 - val_loss: 0.2215 - val_mae: 0.3946\nEpoch 78/82\n128/128 [==============================] - 1s 10ms/step - loss: 0.0012 - mae: 0.0235 - val_loss: 0.2303 - val_mae: 0.4049\nEpoch 79/82\n128/128 [==============================] - 1s 10ms/step - loss: 0.0012 - mae: 0.0234 - val_loss: 0.2059 - val_mae: 0.3786\nEpoch 80/82\n128/128 [==============================] - 1s 11ms/step - loss: 0.0012 - mae: 0.0234 - val_loss: 0.2130 - val_mae: 0.3862\nEpoch 81/82\n128/128 [==============================] - 1s 11ms/step - loss: 0.0012 - mae: 0.0233 - val_loss: 0.2024 - val_mae: 0.3762\nEpoch 82/82\n128/128 [==============================] - 1s 11ms/step - loss: 0.0012 - mae: 0.0235 - val_loss: 0.1948 - val_mae: 0.3680\n11/11 [==============================] - 0s 3ms/step\n\n\n\n\n\n11/11 [==============================] - 0s 6ms/step\n\n\n\n\n\n\n\nShow the code\n### apply regulation on RNN:\n### L1 regulation:\noptimizer=optimizers.RMSprop()\nmodel_RNN_L1=models.Sequential()\nmodel_RNN_L1.add(layers.SimpleRNN(32, activation='relu', input_shape=(lag,1)))\nmodel_RNN_L1.add(layers.Dense(1, activation='linear', kernel_regularizer='l1'))\n\nmodel_RNN_L1.compile(optimizer=optimizer,loss='mse',metrics=['mae'])\n\nhistory_RNN_L1=model_RNN_L1.fit(X_train.reshape(-1,lag,1),y_train.flatten(),batch_size=18,epochs=82,\n                  validation_data=(X_validation.reshape(-1,lag,1),y_validation.flatten()))\n\ntraining_performance(model_RNN_L1,history_RNN_L1,82)\nplot_prediction(model_RNN_L1,'AZN.Close')\n\n\nEpoch 1/82\n128/128 [==============================] - 3s 12ms/step - loss: 0.0730 - mae: 0.0731 - val_loss: 0.3016 - val_mae: 0.4475\nEpoch 2/82\n128/128 [==============================] - 1s 7ms/step - loss: 0.0366 - mae: 0.0419 - val_loss: 0.1948 - val_mae: 0.3647\nEpoch 3/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0216 - mae: 0.0385 - val_loss: 0.2528 - val_mae: 0.4334\nEpoch 4/82\n128/128 [==============================] - 1s 9ms/step - loss: 0.0139 - mae: 0.0396 - val_loss: 0.1956 - val_mae: 0.3738\nEpoch 5/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0116 - mae: 0.0389 - val_loss: 0.2756 - val_mae: 0.4529\nEpoch 6/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0104 - mae: 0.0399 - val_loss: 0.1380 - val_mae: 0.3146\nEpoch 7/82\n128/128 [==============================] - 1s 9ms/step - loss: 0.0095 - mae: 0.0408 - val_loss: 0.0540 - val_mae: 0.1815\nEpoch 8/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0089 - mae: 0.0401 - val_loss: 0.2585 - val_mae: 0.4417\nEpoch 9/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0084 - mae: 0.0398 - val_loss: 0.3711 - val_mae: 0.5498\nEpoch 10/82\n128/128 [==============================] - 1s 9ms/step - loss: 0.0081 - mae: 0.0400 - val_loss: 0.1908 - val_mae: 0.3827\nEpoch 11/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0077 - mae: 0.0398 - val_loss: 0.1467 - val_mae: 0.3284\nEpoch 12/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0075 - mae: 0.0409 - val_loss: 0.1148 - val_mae: 0.2932\nEpoch 13/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0072 - mae: 0.0400 - val_loss: 0.3648 - val_mae: 0.5384\nEpoch 14/82\n128/128 [==============================] - 1s 7ms/step - loss: 0.0071 - mae: 0.0408 - val_loss: 0.0646 - val_mae: 0.2057\nEpoch 15/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0068 - mae: 0.0402 - val_loss: 0.4348 - val_mae: 0.5946\nEpoch 16/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0067 - mae: 0.0406 - val_loss: 0.1835 - val_mae: 0.3769\nEpoch 17/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0064 - mae: 0.0401 - val_loss: 0.1243 - val_mae: 0.3084\nEpoch 18/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0062 - mae: 0.0402 - val_loss: 0.0398 - val_mae: 0.1618\nEpoch 19/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0061 - mae: 0.0402 - val_loss: 0.0609 - val_mae: 0.2095\nEpoch 20/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0060 - mae: 0.0405 - val_loss: 0.1811 - val_mae: 0.3789\nEpoch 21/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0059 - mae: 0.0402 - val_loss: 0.0154 - val_mae: 0.0831\nEpoch 22/82\n128/128 [==============================] - 1s 9ms/step - loss: 0.0058 - mae: 0.0397 - val_loss: 0.0789 - val_mae: 0.2436\nEpoch 23/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0058 - mae: 0.0410 - val_loss: 0.0999 - val_mae: 0.2706\nEpoch 24/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0056 - mae: 0.0398 - val_loss: 0.0533 - val_mae: 0.1945\nEpoch 25/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0056 - mae: 0.0402 - val_loss: 0.0194 - val_mae: 0.1026\nEpoch 26/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0054 - mae: 0.0396 - val_loss: 0.1033 - val_mae: 0.2828\nEpoch 27/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0053 - mae: 0.0398 - val_loss: 0.0575 - val_mae: 0.2047\nEpoch 28/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0052 - mae: 0.0397 - val_loss: 0.1159 - val_mae: 0.3020\nEpoch 29/82\n128/128 [==============================] - 1s 7ms/step - loss: 0.0053 - mae: 0.0403 - val_loss: 0.0249 - val_mae: 0.1108\nEpoch 30/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0052 - mae: 0.0399 - val_loss: 0.1463 - val_mae: 0.3401\nEpoch 31/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0051 - mae: 0.0396 - val_loss: 0.0453 - val_mae: 0.1734\nEpoch 32/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0050 - mae: 0.0391 - val_loss: 0.1624 - val_mae: 0.3432\nEpoch 33/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0050 - mae: 0.0392 - val_loss: 0.1001 - val_mae: 0.2774\nEpoch 34/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0049 - mae: 0.0389 - val_loss: 0.0157 - val_mae: 0.0891\nEpoch 35/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0050 - mae: 0.0390 - val_loss: 0.3699 - val_mae: 0.5392\nEpoch 36/82\n128/128 [==============================] - 1s 7ms/step - loss: 0.0051 - mae: 0.0391 - val_loss: 0.0761 - val_mae: 0.2315\nEpoch 37/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0049 - mae: 0.0384 - val_loss: 0.0371 - val_mae: 0.1607\nEpoch 38/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0049 - mae: 0.0387 - val_loss: 0.0401 - val_mae: 0.1674\nEpoch 39/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0050 - mae: 0.0385 - val_loss: 0.1215 - val_mae: 0.3067\nEpoch 40/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0049 - mae: 0.0384 - val_loss: 0.0138 - val_mae: 0.0814\nEpoch 41/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0047 - mae: 0.0380 - val_loss: 0.0171 - val_mae: 0.0947\nEpoch 42/82\n128/128 [==============================] - 1s 9ms/step - loss: 0.0048 - mae: 0.0382 - val_loss: 0.2162 - val_mae: 0.4152\nEpoch 43/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0047 - mae: 0.0379 - val_loss: 0.7861 - val_mae: 0.7612\nEpoch 44/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0047 - mae: 0.0377 - val_loss: 0.0403 - val_mae: 0.1629\nEpoch 45/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0047 - mae: 0.0377 - val_loss: 0.0422 - val_mae: 0.1696\nEpoch 46/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0047 - mae: 0.0375 - val_loss: 0.0621 - val_mae: 0.2151\nEpoch 47/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0046 - mae: 0.0372 - val_loss: 0.1230 - val_mae: 0.3096\nEpoch 48/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0046 - mae: 0.0376 - val_loss: 0.0129 - val_mae: 0.0774\nEpoch 49/82\n128/128 [==============================] - 1s 7ms/step - loss: 0.0045 - mae: 0.0365 - val_loss: 0.7370 - val_mae: 0.7331\nEpoch 50/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0046 - mae: 0.0369 - val_loss: 0.0929 - val_mae: 0.2522\nEpoch 51/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0045 - mae: 0.0369 - val_loss: 2.1762 - val_mae: 1.2587\nEpoch 52/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0045 - mae: 0.0364 - val_loss: 0.0132 - val_mae: 0.0746\nEpoch 53/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0047 - mae: 0.0365 - val_loss: 0.0685 - val_mae: 0.2296\nEpoch 54/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0044 - mae: 0.0359 - val_loss: 0.1155 - val_mae: 0.2992\nEpoch 55/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0044 - mae: 0.0362 - val_loss: 0.3382 - val_mae: 0.5101\nEpoch 56/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0045 - mae: 0.0364 - val_loss: 0.0521 - val_mae: 0.1885\nEpoch 57/82\n128/128 [==============================] - 1s 7ms/step - loss: 0.0043 - mae: 0.0361 - val_loss: 0.0936 - val_mae: 0.2673\nEpoch 58/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0044 - mae: 0.0368 - val_loss: 0.1116 - val_mae: 0.2944\nEpoch 59/82\n128/128 [==============================] - 1s 7ms/step - loss: 0.0044 - mae: 0.0363 - val_loss: 0.7738 - val_mae: 0.7498\nEpoch 60/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0043 - mae: 0.0360 - val_loss: 0.2649 - val_mae: 0.4495\nEpoch 61/82\n128/128 [==============================] - 1s 7ms/step - loss: 0.0043 - mae: 0.0359 - val_loss: 0.0337 - val_mae: 0.1459\nEpoch 62/82\n\n\n128/128 [==============================] - 1s 8ms/step - loss: 0.0043 - mae: 0.0361 - val_loss: 0.1048 - val_mae: 0.2824\nEpoch 63/82\n128/128 [==============================] - 1s 9ms/step - loss: 0.0043 - mae: 0.0359 - val_loss: 0.1711 - val_mae: 0.3614\nEpoch 64/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0042 - mae: 0.0359 - val_loss: 0.0145 - val_mae: 0.0827\nEpoch 65/82\n128/128 [==============================] - 1s 7ms/step - loss: 0.0042 - mae: 0.0357 - val_loss: 0.6064 - val_mae: 0.6780\nEpoch 66/82\n128/128 [==============================] - 1s 7ms/step - loss: 0.0041 - mae: 0.0350 - val_loss: 0.0248 - val_mae: 0.1213\nEpoch 67/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0042 - mae: 0.0359 - val_loss: 0.1396 - val_mae: 0.3247\nEpoch 68/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0042 - mae: 0.0359 - val_loss: 0.0983 - val_mae: 0.2704\nEpoch 69/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0041 - mae: 0.0347 - val_loss: 0.4083 - val_mae: 0.5542\nEpoch 70/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0041 - mae: 0.0348 - val_loss: 0.0122 - val_mae: 0.0776\nEpoch 71/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0041 - mae: 0.0354 - val_loss: 0.1799 - val_mae: 0.3588\nEpoch 72/82\n128/128 [==============================] - 1s 9ms/step - loss: 0.0041 - mae: 0.0353 - val_loss: 0.0109 - val_mae: 0.0694\nEpoch 73/82\n128/128 [==============================] - 1s 9ms/step - loss: 0.0041 - mae: 0.0357 - val_loss: 0.0999 - val_mae: 0.2760\nEpoch 74/82\n128/128 [==============================] - 1s 7ms/step - loss: 0.0040 - mae: 0.0351 - val_loss: 0.2791 - val_mae: 0.4567\nEpoch 75/82\n128/128 [==============================] - 1s 7ms/step - loss: 0.0040 - mae: 0.0351 - val_loss: 0.0363 - val_mae: 0.1530\nEpoch 76/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0040 - mae: 0.0348 - val_loss: 0.0126 - val_mae: 0.0749\nEpoch 77/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0039 - mae: 0.0346 - val_loss: 2.9297 - val_mae: 1.4706\nEpoch 78/82\n128/128 [==============================] - 1s 7ms/step - loss: 0.0039 - mae: 0.0350 - val_loss: 2.3024 - val_mae: 1.3015\nEpoch 79/82\n128/128 [==============================] - 1s 6ms/step - loss: 0.0039 - mae: 0.0347 - val_loss: 0.0132 - val_mae: 0.0810\nEpoch 80/82\n128/128 [==============================] - 1s 6ms/step - loss: 0.0039 - mae: 0.0348 - val_loss: 0.9981 - val_mae: 0.8598\nEpoch 81/82\n128/128 [==============================] - 1s 6ms/step - loss: 0.0039 - mae: 0.0350 - val_loss: 1.7083 - val_mae: 1.1239\nEpoch 82/82\n128/128 [==============================] - 1s 6ms/step - loss: 0.0039 - mae: 0.0349 - val_loss: 0.3911 - val_mae: 0.5352\n11/11 [==============================] - 0s 4ms/step\n\n\n\n\n\n11/11 [==============================] - 0s 3ms/step\n\n\n\n\n\n\n\nShow the code\nprint('training RMSE:', math.sqrt(np.square(history_RNN_L1.history['loss']).mean()))\nprint('testing RMSE:', math.sqrt(np.square(history_RNN_L1.history['val_loss']).mean()))\n\n\ntraining RMSE: 0.010856092317283548\ntesting RMSE: 0.5713360715104482\n\n\n\n\nShow the code\n## apply L2 to RNN:\noptimizer=optimizers.RMSprop()\nmodel_RNN_L2=models.Sequential()\nmodel_RNN_L2.add(layers.SimpleRNN(32, activation='relu', input_shape=(lag,1)))\nmodel_RNN_L2.add(layers.Dense(1, activation='linear', kernel_regularizer='l2'))\nmodel_RNN_L2.compile(optimizer=optimizer,loss='mse',metrics=['mae'])\n\nhistory_RNN_L2=model_RNN_L2.fit(X_train.reshape(-1,lag,1),y_train.flatten(),batch_size=18,epochs=82,\n                  validation_data=(X_validation.reshape(-1,lag,1),y_validation.flatten()))\n\ntraining_performance(model_RNN_L1,history_RNN_L2,82)\nplot_prediction(model_RNN_L2,'AZN.Close')\n\n\nEpoch 1/82\n128/128 [==============================] - 6s 17ms/step - loss: 0.2918 - mae: 0.2824 - val_loss: 0.0887 - val_mae: 0.2388\nEpoch 2/82\n128/128 [==============================] - 1s 11ms/step - loss: 0.0150 - mae: 0.0408 - val_loss: 0.0695 - val_mae: 0.2123\nEpoch 3/82\n128/128 [==============================] - 1s 9ms/step - loss: 0.0105 - mae: 0.0376 - val_loss: 0.1283 - val_mae: 0.3092\nEpoch 4/82\n128/128 [==============================] - 1s 9ms/step - loss: 0.0085 - mae: 0.0374 - val_loss: 0.0439 - val_mae: 0.1648\nEpoch 5/82\n128/128 [==============================] - 1s 11ms/step - loss: 0.0073 - mae: 0.0360 - val_loss: 0.0943 - val_mae: 0.2614\nEpoch 6/82\n128/128 [==============================] - 1s 10ms/step - loss: 0.0064 - mae: 0.0358 - val_loss: 0.0420 - val_mae: 0.1675\nEpoch 7/82\n128/128 [==============================] - 1s 10ms/step - loss: 0.0058 - mae: 0.0351 - val_loss: 0.0240 - val_mae: 0.1182\nEpoch 8/82\n128/128 [==============================] - 1s 9ms/step - loss: 0.0052 - mae: 0.0343 - val_loss: 0.1029 - val_mae: 0.2775\nEpoch 9/82\n128/128 [==============================] - 1s 9ms/step - loss: 0.0047 - mae: 0.0333 - val_loss: 0.0340 - val_mae: 0.1484\nEpoch 10/82\n128/128 [==============================] - 2s 12ms/step - loss: 0.0043 - mae: 0.0332 - val_loss: 0.0501 - val_mae: 0.1921\nEpoch 11/82\n128/128 [==============================] - 1s 10ms/step - loss: 0.0041 - mae: 0.0325 - val_loss: 0.0199 - val_mae: 0.1072\nEpoch 12/82\n128/128 [==============================] - 1s 9ms/step - loss: 0.0038 - mae: 0.0323 - val_loss: 0.0171 - val_mae: 0.0996\nEpoch 13/82\n128/128 [==============================] - 2s 13ms/step - loss: 0.0036 - mae: 0.0323 - val_loss: 0.0390 - val_mae: 0.1643\nEpoch 14/82\n128/128 [==============================] - 2s 14ms/step - loss: 0.0035 - mae: 0.0326 - val_loss: 0.0096 - val_mae: 0.0667\nEpoch 15/82\n128/128 [==============================] - 1s 10ms/step - loss: 0.0034 - mae: 0.0318 - val_loss: 0.0395 - val_mae: 0.1671\nEpoch 16/82\n128/128 [==============================] - 1s 11ms/step - loss: 0.0033 - mae: 0.0325 - val_loss: 0.0592 - val_mae: 0.2082\nEpoch 17/82\n128/128 [==============================] - 1s 10ms/step - loss: 0.0032 - mae: 0.0318 - val_loss: 0.0678 - val_mae: 0.2253\nEpoch 18/82\n128/128 [==============================] - 1s 10ms/step - loss: 0.0031 - mae: 0.0320 - val_loss: 0.0133 - val_mae: 0.0868\nEpoch 19/82\n128/128 [==============================] - 1s 10ms/step - loss: 0.0030 - mae: 0.0317 - val_loss: 0.0224 - val_mae: 0.1177\nEpoch 20/82\n128/128 [==============================] - 1s 11ms/step - loss: 0.0030 - mae: 0.0318 - val_loss: 0.0628 - val_mae: 0.2204\nEpoch 21/82\n128/128 [==============================] - 1s 12ms/step - loss: 0.0029 - mae: 0.0317 - val_loss: 0.0408 - val_mae: 0.1686\nEpoch 22/82\n128/128 [==============================] - 2s 12ms/step - loss: 0.0028 - mae: 0.0313 - val_loss: 0.0210 - val_mae: 0.1136\nEpoch 23/82\n128/128 [==============================] - 1s 12ms/step - loss: 0.0028 - mae: 0.0318 - val_loss: 0.0079 - val_mae: 0.0597\nEpoch 24/82\n128/128 [==============================] - 2s 13ms/step - loss: 0.0027 - mae: 0.0312 - val_loss: 0.0145 - val_mae: 0.0927\nEpoch 25/82\n128/128 [==============================] - 1s 10ms/step - loss: 0.0028 - mae: 0.0317 - val_loss: 0.0179 - val_mae: 0.1035\nEpoch 26/82\n128/128 [==============================] - 1s 11ms/step - loss: 0.0027 - mae: 0.0311 - val_loss: 0.0876 - val_mae: 0.2586\nEpoch 27/82\n128/128 [==============================] - 1s 12ms/step - loss: 0.0027 - mae: 0.0311 - val_loss: 0.0259 - val_mae: 0.1294\nEpoch 28/82\n128/128 [==============================] - 1s 11ms/step - loss: 0.0026 - mae: 0.0310 - val_loss: 0.0332 - val_mae: 0.1505\nEpoch 29/82\n128/128 [==============================] - 1s 9ms/step - loss: 0.0026 - mae: 0.0314 - val_loss: 0.0744 - val_mae: 0.2362\nEpoch 30/82\n128/128 [==============================] - 1s 9ms/step - loss: 0.0026 - mae: 0.0306 - val_loss: 0.0683 - val_mae: 0.2261\nEpoch 31/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0026 - mae: 0.0308 - val_loss: 0.0205 - val_mae: 0.1134\nEpoch 32/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0025 - mae: 0.0305 - val_loss: 0.0084 - val_mae: 0.0639\nEpoch 33/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0025 - mae: 0.0302 - val_loss: 0.0623 - val_mae: 0.2169\nEpoch 34/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0025 - mae: 0.0306 - val_loss: 0.0178 - val_mae: 0.1041\nEpoch 35/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0025 - mae: 0.0306 - val_loss: 0.1061 - val_mae: 0.2884\nEpoch 36/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0025 - mae: 0.0307 - val_loss: 0.0237 - val_mae: 0.1234\nEpoch 37/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0024 - mae: 0.0303 - val_loss: 0.0349 - val_mae: 0.1564\nEpoch 38/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0024 - mae: 0.0304 - val_loss: 0.0393 - val_mae: 0.1681\nEpoch 39/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0024 - mae: 0.0304 - val_loss: 0.0515 - val_mae: 0.1947\nEpoch 40/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0024 - mae: 0.0303 - val_loss: 0.0355 - val_mae: 0.1567\nEpoch 41/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0024 - mae: 0.0300 - val_loss: 0.0545 - val_mae: 0.2009\nEpoch 42/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0024 - mae: 0.0306 - val_loss: 0.0572 - val_mae: 0.2077\nEpoch 43/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0023 - mae: 0.0297 - val_loss: 0.0230 - val_mae: 0.1220\nEpoch 44/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0023 - mae: 0.0294 - val_loss: 0.0171 - val_mae: 0.1026\nEpoch 45/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0024 - mae: 0.0300 - val_loss: 0.0105 - val_mae: 0.0747\nEpoch 46/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0023 - mae: 0.0297 - val_loss: 0.0147 - val_mae: 0.0950\nEpoch 47/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0023 - mae: 0.0301 - val_loss: 0.0288 - val_mae: 0.1404\nEpoch 48/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0023 - mae: 0.0298 - val_loss: 0.0291 - val_mae: 0.1404\nEpoch 49/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0022 - mae: 0.0291 - val_loss: 0.0081 - val_mae: 0.0628\nEpoch 50/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0022 - mae: 0.0293 - val_loss: 0.0112 - val_mae: 0.0786\nEpoch 51/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0022 - mae: 0.0297 - val_loss: 0.0083 - val_mae: 0.0634\nEpoch 52/82\n128/128 [==============================] - 1s 9ms/step - loss: 0.0022 - mae: 0.0290 - val_loss: 0.0784 - val_mae: 0.2437\nEpoch 53/82\n128/128 [==============================] - 1s 9ms/step - loss: 0.0022 - mae: 0.0295 - val_loss: 0.0665 - val_mae: 0.2224\nEpoch 54/82\n128/128 [==============================] - 1s 9ms/step - loss: 0.0022 - mae: 0.0293 - val_loss: 0.0586 - val_mae: 0.2105\nEpoch 55/82\n128/128 [==============================] - 1s 9ms/step - loss: 0.0022 - mae: 0.0292 - val_loss: 0.0986 - val_mae: 0.2756\nEpoch 56/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0022 - mae: 0.0291 - val_loss: 0.0336 - val_mae: 0.1527\nEpoch 57/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0022 - mae: 0.0290 - val_loss: 0.0399 - val_mae: 0.1667\nEpoch 58/82\n128/128 [==============================] - 1s 9ms/step - loss: 0.0022 - mae: 0.0297 - val_loss: 0.0444 - val_mae: 0.1788\nEpoch 59/82\n128/128 [==============================] - 1s 9ms/step - loss: 0.0021 - mae: 0.0290 - val_loss: 0.0114 - val_mae: 0.0822\nEpoch 60/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0022 - mae: 0.0290 - val_loss: 0.0342 - val_mae: 0.1558\nEpoch 61/82\n128/128 [==============================] - 1s 9ms/step - loss: 0.0021 - mae: 0.0287 - val_loss: 0.0104 - val_mae: 0.0751\nEpoch 62/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0021 - mae: 0.0290 - val_loss: 0.0171 - val_mae: 0.1016\nEpoch 63/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0021 - mae: 0.0289 - val_loss: 0.0323 - val_mae: 0.1484\nEpoch 64/82\n128/128 [==============================] - 1s 7ms/step - loss: 0.0021 - mae: 0.0286 - val_loss: 0.0326 - val_mae: 0.1493\nEpoch 65/82\n128/128 [==============================] - 1s 7ms/step - loss: 0.0021 - mae: 0.0286 - val_loss: 0.0390 - val_mae: 0.1677\nEpoch 66/82\n128/128 [==============================] - 1s 7ms/step - loss: 0.0021 - mae: 0.0283 - val_loss: 0.0102 - val_mae: 0.0731\nEpoch 67/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0021 - mae: 0.0289 - val_loss: 0.0274 - val_mae: 0.1375\nEpoch 68/82\n128/128 [==============================] - 1s 9ms/step - loss: 0.0021 - mae: 0.0287 - val_loss: 0.0375 - val_mae: 0.1627\nEpoch 69/82\n128/128 [==============================] - 1s 9ms/step - loss: 0.0020 - mae: 0.0282 - val_loss: 0.0843 - val_mae: 0.2536\nEpoch 70/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0020 - mae: 0.0278 - val_loss: 0.0105 - val_mae: 0.0749\nEpoch 71/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0020 - mae: 0.0281 - val_loss: 0.0207 - val_mae: 0.1145\nEpoch 72/82\n128/128 [==============================] - 1s 7ms/step - loss: 0.0020 - mae: 0.0287 - val_loss: 0.0145 - val_mae: 0.0932\nEpoch 73/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0020 - mae: 0.0287 - val_loss: 0.0455 - val_mae: 0.1829\nEpoch 74/82\n128/128 [==============================] - 1s 7ms/step - loss: 0.0020 - mae: 0.0283 - val_loss: 0.0137 - val_mae: 0.0896\nEpoch 75/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0020 - mae: 0.0279 - val_loss: 0.0073 - val_mae: 0.0613\nEpoch 76/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0020 - mae: 0.0283 - val_loss: 0.0076 - val_mae: 0.0605\nEpoch 77/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0019 - mae: 0.0275 - val_loss: 0.0502 - val_mae: 0.1958\nEpoch 78/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0019 - mae: 0.0278 - val_loss: 0.0250 - val_mae: 0.1327\nEpoch 79/82\n128/128 [==============================] - 1s 7ms/step - loss: 0.0020 - mae: 0.0281 - val_loss: 0.0223 - val_mae: 0.1215\nEpoch 80/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0020 - mae: 0.0282 - val_loss: 0.0105 - val_mae: 0.0804\nEpoch 81/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0020 - mae: 0.0279 - val_loss: 0.0071 - val_mae: 0.0588\nEpoch 82/82\n128/128 [==============================] - 1s 8ms/step - loss: 0.0020 - mae: 0.0284 - val_loss: 0.0165 - val_mae: 0.1003\n11/11 [==============================] - 0s 3ms/step\n\n\n\n\n\n11/11 [==============================] - 0s 4ms/step\n\n\n\n\n\n\n\nShow the code\nprint('training RMSE:', math.sqrt(np.square(history_RNN_L2.history['loss']).mean()))\nprint('testing RMSE:', math.sqrt(np.square(history_RNN_L2.history['val_loss']).mean()))\n\n\ntraining RMSE: 0.032416972889233786\ntesting RMSE: 0.046631832712572784"
  },
  {
    "objectID": "model_unh_precovid.html#step-1-determine-the-stationality-of-time-series",
    "href": "model_unh_precovid.html#step-1-determine-the-stationality-of-time-series",
    "title": "ARIMA for Pre-COVID UNH",
    "section": "Step 1: Determine the stationality of time series",
    "text": "Step 1: Determine the stationality of time series\nBased on information obtained from both ACF graphs and Augmented Dickey-Fuller Test, the time series data is non-stationary.\n\n\nShow the code\nUNH_acf <- ggAcf(UNH_ts,100)+ggtitle(\"UNH ACF Plot\")\n\nUNH_pacf <- ggPacf(UNH_ts)+ggtitle(\"PACF Plot for UHNs\")\ngrid.arrange(UNH_acf, UNH_pacf,nrow=2)\n\n\n\n\n\n\n\nShow the code\ntseries::adf.test(UNH_ts)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  UNH_ts\nDickey-Fuller = -1.5332, Lag order = 13, p-value = 0.776\nalternative hypothesis: stationary"
  },
  {
    "objectID": "model_unh_precovid.html#step-2-eliminate-non-stationality",
    "href": "model_unh_precovid.html#step-2-eliminate-non-stationality",
    "title": "ARIMA for Pre-COVID UNH",
    "section": "Step 2: Eliminate Non-Stationality",
    "text": "Step 2: Eliminate Non-Stationality\nSince this data is non-stationary, it is important to necessary to convert it to stationary time series. This step employs a series of actions to eliminate non-stationality, i.e. log transformation and differencing the data. It turns out the log transformed and 1st differened data has shown good stationary property, there are no need to go further at 2nd differencing. What is more, the Augmented Dickey-Fuller Test also confirmed that the log transformed and 1st differenced data is stationary. Therefore, the log transformation and 1st differencing would be the actions taken to eliminate the non-stationality.\n\n\nShow the code\nplot1<- ggAcf(log(UNH_ts) %>%diff(), 50, main=\"ACF Plot for Log Transformed & 1st differenced Data\") \nplot2<- ggAcf(log(UNH_ts) %>%diff()%>%diff(),50, main=\"ACF Plot for Log Transformed & 2nd differenced Data\") \n\ngrid.arrange(plot1, plot2,nrow=2)\n\n\n\n\n\n\n\nShow the code\ntseries::adf.test(log(UNH_ts) %>%diff())\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  log(UNH_ts) %>% diff()\nDickey-Fuller = -15.171, Lag order = 13, p-value = 0.01\nalternative hypothesis: stationary"
  },
  {
    "objectID": "model_unh_precovid.html#step-3-determine-pdq-parameters",
    "href": "model_unh_precovid.html#step-3-determine-pdq-parameters",
    "title": "ARIMA for Pre-COVID UNH",
    "section": "Step 3: Determine p,d,q Parameters",
    "text": "Step 3: Determine p,d,q Parameters\n\n\nShow the code\nplot3<- ggPacf(log(UNH_ts) %>%diff(),50, main=\"PACF Plot for Log Transformed & 1st differenced Data\") \n\ngrid.arrange(plot1,plot3)\n\n\n\n\n\nAccording to the PACF plot and ACF plot above, no obivious peaks in neither ACF nor PACF, so both p and q will be 0. Since I only differenced the data once, the d would be 1."
  },
  {
    "objectID": "model_unh_precovid.html#step-4-fit-arimapdq-model",
    "href": "model_unh_precovid.html#step-4-fit-arimapdq-model",
    "title": "ARIMA for Pre-COVID UNH",
    "section": "Step 4: Fit ARIMA(p,d,q) model",
    "text": "Step 4: Fit ARIMA(p,d,q) model\n\n\nShow the code\nfit1 <- Arima(log(UNH_ts), order=c(0, 1, 0),include.drift = TRUE) \nsummary(fit1)\n\n\nSeries: log(UNH_ts) \nARIMA(0,1,0) with drift \n\nCoefficients:\n      drift\n      1e-03\ns.e.  3e-04\n\nsigma^2 = 0.0002069:  log likelihood = 7085.52\nAIC=-14167.03   AICc=-14167.03   BIC=-14155.37\n\nTraining set error measures:\n                       ME      RMSE        MAE           MPE      MAPE\nTraining set 1.295476e-06 0.0143782 0.01042779 -0.0005696724 0.2442792\n                   MASE        ACF1\nTraining set 0.04232194 -0.01978998\n\n\n\nModel Diagnostics\n\nInspection of the time plot of the standardized residuals below shows no obvious patterns.\nNotice that there may be outliers, with a few values exceeding 3 standard deviations in magnitude.\nThe ACF of the standardized residuals shows no apparent departure from the model assumptions, no significant lags shown.\nThe normal Q-Q plot of the residuals shows that the assumption of normality is reasonable, with the exception of the fat-tailed.\nThe model appears to fit well.\n\n\n\nShow the code\nmodel_output <- capture.output(sarima(log(UNH_ts), 0,1,0))\n\n\n\n\n\n\n\nShow the code\ncat(model_output[8:38], model_output[length(model_output)], sep = \"\\n\") #to get rid of the convergence status and details of the optimization algorithm used by the sarima() \n\n\nconverged\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n      constant\n         1e-03\ns.e.     3e-04\n\nsigma^2 estimated as 0.0002068:  log likelihood = 7085.52,  aic = -14167.03\n\n$degrees_of_freedom\n[1] 2509\n\n$ttable\n         Estimate    SE t.value p.value\nconstant    0.001 3e-04  3.3048   0.001\n\n$AIC\n[1] -5.644235\n\n$AICc\n[1] -5.644234\n\n$BIC\n[1] -5.639591\n\n\n\n\nCompare with auto.arima() function\nBoth auto.arima and manually fitted model suggested ARIMA(0,1,0) is the best fit model.\n\n\nShow the code\nauto.arima(log(UNH_ts))\n\n\nSeries: log(UNH_ts) \nARIMA(0,1,0) with drift \n\nCoefficients:\n      drift\n      1e-03\ns.e.  3e-04\n\nsigma^2 = 0.0002069:  log likelihood = 7085.52\nAIC=-14167.03   AICc=-14167.03   BIC=-14155.37"
  },
  {
    "objectID": "model_unh_precovid.html#step-5-forecast",
    "href": "model_unh_precovid.html#step-5-forecast",
    "title": "ARIMA for Pre-COVID UNH",
    "section": "Step 5: Forecast",
    "text": "Step 5: Forecast\nThe blue part in graph below forecast the next 100 values of UNH stock price in 80% and 95% confidence level.\n\n\nShow the code\n(UNH_ts) %>%\n  Arima(order=c(0,1,0),include.drift = TRUE) %>%\n  forecast(300) %>%\n  autoplot() +\n  ylab(\"UNH stock prices prediction\") + xlab(\"Year\")\n\n\n\n\n\n\n\nShow the code\nprecovid_pred <- as.data.frame((UNH_ts) %>%\n  Arima(order=c(0,1,0),include.drift = TRUE) %>%\n  forecast(768))['Point Forecast']\n\nunh_postcovid <- filter(unh,Dates>\"2020-01-9\")\nunh_postcovid$preds <- precovid_pred$`Point Forecast`"
  },
  {
    "objectID": "model_unh_precovid.html#true-unh-stock-price-vs-uhn-arima-prediction-since-covid-19",
    "href": "model_unh_precovid.html#true-unh-stock-price-vs-uhn-arima-prediction-since-covid-19",
    "title": "ARIMA for Pre-COVID UNH",
    "section": "True UNH Stock Price VS UHN ARIMA Prediction since COVID 19",
    "text": "True UNH Stock Price VS UHN ARIMA Prediction since COVID 19\nThe plot below shows the forecast of pre-COVID only UNH stock price during the COVID period and the real-world UNH stock price. According to the plot, the real world UNH stock price illustrate a more upward trend then the prediction. This indicates that as the pandemic continued and the demand for healthcare services increased, UNH stocks rebounded.\n\n\nShow the code\ng1<- ggplot(unh_postcovid, aes(x=Dates)) +\n  geom_line(aes(y=UNH, colour=\"True value\"))+\n  geom_line(aes(y=preds, colour=\"Prediction\"))+\n   labs(\n    title = \"True UNH Stock Price VS UHN ARIMA Prediction since COVID 19\",\n    x = \"Date\",\n    y = \"Adjusted Closing Prices\")+\n    guides(colour=guide_legend(title=\"Healthcare Companies\")) \n\n\n\nggplotly(g1) %>% layout(hovermode = \"x\")"
  },
  {
    "objectID": "models.html#sarima",
    "href": "models.html#sarima",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "SARIMA",
    "text": "SARIMA\nThe problem with plain ARIMA model is it does not support seasonality. If the time series has defined seasonality, then, go for SARIMA(p,d,q)x(P,D,Q) which uses seasonal differencing. Seasonal differencing is similar to regular differencing, but, instead of subtracting consecutive terms, it subtracts the value from previous season. - GDP"
  },
  {
    "objectID": "model_unh.html#step-1-determine-the-stationality-of-time-series",
    "href": "model_unh.html#step-1-determine-the-stationality-of-time-series",
    "title": "ARMA/ARIMA/SARIMA Models for UNH",
    "section": "Step 1: Determine the stationality of time series",
    "text": "Step 1: Determine the stationality of time series\nStationality is a pre-requirement of training ARIMA model. This is because term ‘Auto Regressive’ in ARIMA means it is a linear regression model that uses its own lags as predictors, which work best when the predictors are not correlated and are independent of each other. Stationary time series make sure the statistical properties of time series do not change over time.\nBased on information obtained from both ACF graphs and Augmented Dickey-Fuller Test, the time series data is non-stationary.\n\n\nShow the code\nUNH_acf <- ggAcf(UNH_ts,100)+ggtitle(\"UNH ACF Plot\")\n\nUNH_pacf <- ggPacf(UNH_ts)+ggtitle(\"PACF Plot for UHNs\")\ngrid.arrange(UNH_acf, UNH_pacf,nrow=2)\n\n\n\n\n\n\n\nShow the code\ntseries::adf.test(UNH_ts)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  UNH_ts\nDickey-Fuller = -1.0864, Lag order = 14, p-value = 0.9248\nalternative hypothesis: stationary"
  },
  {
    "objectID": "model_unh.html#step-2-eliminate-non-stationality",
    "href": "model_unh.html#step-2-eliminate-non-stationality",
    "title": "ARMA/ARIMA/SARIMA Models for UNH",
    "section": "Step 2: Eliminate Non-Stationality",
    "text": "Step 2: Eliminate Non-Stationality\nSince this data is non-stationary, it is important to necessary to convert it to stationary time series. This step employs a series of actions to eliminate non-stationality, i.e. log transformation and differencing the data. It turns out the log transformed and 1st differened data has shown good stationary property, there are no need to go further at 2nd differencing. What is more, the Augmented Dickey-Fuller Test also confirmed that the log transformed and 1st differenced data is stationary. Therefore, the log transformation and 1st differencing would be the actions taken to eliminate the non-stationality.\n\n\nShow the code\nplot1<- ggAcf(log(UNH_ts) %>%diff(), 50, main=\"ACF Plot for Log Transformed & 1st differenced Data\") \nplot2<- ggAcf(log(UNH_ts) %>%diff()%>%diff(),50, main=\"ACF Plot for Log Transformed & 2nd differenced Data\") \n\ngrid.arrange(plot1, plot2,nrow=2)\n\n\n\n\n\n\n\nShow the code\ntseries::adf.test(log(UNH_ts) %>%diff())\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  log(UNH_ts) %>% diff()\nDickey-Fuller = -16.952, Lag order = 14, p-value = 0.01\nalternative hypothesis: stationary"
  },
  {
    "objectID": "model_unh.html#step-3-determine-pdq-parameters",
    "href": "model_unh.html#step-3-determine-pdq-parameters",
    "title": "ARMA/ARIMA/SARIMA Models for UNH",
    "section": "Step 3: Determine p,d,q Parameters",
    "text": "Step 3: Determine p,d,q Parameters\nThe standard notation of ARIMA(p,d,q) include p,d,q 3 parameters. Here are the representations: - p: The number of lag observations included in the model, also called the lag order; order of the AR term. - d: The number of times that the raw observations are differenced, also called the degree of differencing; number of differencing required to make the time series stationary. - q: order of moving average; order of the MA term. It refers to the number of lagged forecast errors that should go into the ARIMA Model.\n\n\nShow the code\nplot3<- ggPacf(log(UNH_ts) %>%diff(),50, main=\"PACF Plot for Log Transformed & 1st differenced Data\") \n\ngrid.arrange(plot1,plot3)\n\n\n\n\n\nAccording to the PACF plot and ACF plot above, here choose the range of p value and q value as 1-4 and 1-3, respectively. Since I only differenced the data once, the d would be 1."
  },
  {
    "objectID": "model_unh.html#step-4-fit-arimapdq-model",
    "href": "model_unh.html#step-4-fit-arimapdq-model",
    "title": "ARMA/ARIMA/SARIMA Models for UNH",
    "section": "Step 4: Fit ARIMA(p,d,q) model",
    "text": "Step 4: Fit ARIMA(p,d,q) model\nBefore fitting the data with ARIMA(p,d,q) model, we will need to choose the set of parameters based on error measurement and model diagnostics. Based on the result below, ARIMA(4,1,3) has lowest error measurement.\n\n\nShow the code\n################ Check for different combinations ######\n\n\nd=1\ni=1\ntemp= data.frame()\nls=matrix(rep(NA,6*12),nrow=12) # roughly nrow = 3x4x2\n\n\nfor (p in 2:5)# p=1,2,3,4 : 4\n{\n  for(q in 2:4)# q=1,2,3 :3\n  {\n    for(d in 1:1)# d=1,2 :2\n    {\n      \n      if(p-1+d+q-1<=8)\n      {\n        \n        model<- Arima(log(UNH_ts),order=c(p-1,d,q-1),include.drift=TRUE) \n        ls[i,]= c(p-1,d,q-1,model$aic,model$bic,model$aicc)\n        i=i+1\n        #print(i)\n        \n      }\n      \n    }\n  }\n}\n\ntemp= as.data.frame(ls)\nnames(temp)= c(\"p\",\"d\",\"q\",\"AIC\",\"BIC\",\"AICc\")\n\nkable(temp) %>%\n  kable_styling(font_size = 12)\n\n\n\n\n \n  \n    p \n    d \n    q \n    AIC \n    BIC \n    AICc \n  \n \n\n  \n    1 \n    1 \n    1 \n    -17710.12 \n    -17685.76 \n    -17710.11 \n  \n  \n    1 \n    1 \n    2 \n    -17719.11 \n    -17688.66 \n    -17719.09 \n  \n  \n    1 \n    1 \n    3 \n    -17745.61 \n    -17709.07 \n    -17745.59 \n  \n  \n    2 \n    1 \n    1 \n    -17716.76 \n    -17686.31 \n    -17716.74 \n  \n  \n    2 \n    1 \n    2 \n    -17772.64 \n    -17736.10 \n    -17772.61 \n  \n  \n    2 \n    1 \n    3 \n    -17747.50 \n    -17704.86 \n    -17747.46 \n  \n  \n    3 \n    1 \n    1 \n    -17746.22 \n    -17709.68 \n    -17746.19 \n  \n  \n    3 \n    1 \n    2 \n    -17745.07 \n    -17702.44 \n    -17745.04 \n  \n  \n    3 \n    1 \n    3 \n    -17789.34 \n    -17740.62 \n    -17789.30 \n  \n  \n    4 \n    1 \n    1 \n    -17728.60 \n    -17685.97 \n    -17728.56 \n  \n  \n    4 \n    1 \n    2 \n    -17757.45 \n    -17708.73 \n    -17757.41 \n  \n  \n    4 \n    1 \n    3 \n    -17798.95 \n    -17744.14 \n    -17798.90 \n  \n\n\n\n\n\n\nError measurement\n\n\nShow the code\ntemp[which.min(temp$AIC),] \n\n\n   p d q       AIC       BIC     AICc\n12 4 1 3 -17798.95 -17744.14 -17798.9\n\n\n\n\n   p d q       AIC       BIC     AICc\n12 4 1 3 -17798.95 -17744.14 -17798.9\n\n\n\n\n   p d q       AIC       BIC     AICc\n12 4 1 3 -17798.95 -17744.14 -17798.9\n\n\n\n\nShow the code\nfit1 <- Arima(log(UNH_ts), order=c(4, 1, 3),include.drift = TRUE) \nsummary(fit1)\n\n\nSeries: log(UNH_ts) \nARIMA(4,1,3) with drift \n\nCoefficients:\n          ar1     ar2     ar3      ar4     ma1      ma2      ma3  drift\n      -0.6673  0.7666  0.7124  -0.0768  0.6040  -0.7506  -0.7104  9e-04\ns.e.   0.0485  0.0455  0.0545   0.0218  0.0461   0.0382   0.0512  2e-04\n\nsigma^2 = 0.0002496:  log likelihood = 8908.48\nAIC=-17798.95   AICc=-17798.9   BIC=-17744.14\n\nTraining set error measures:\n                       ME       RMSE        MAE           MPE      MAPE\nTraining set 4.453197e-06 0.01577582 0.01103893 -0.0006003827 0.2395419\n                   MASE          ACF1\nTraining set 0.04432579 -0.0008705228\n\n\n\n\nModel Diagnostics\n\nInspection of the time plot of the standardized residuals below shows no obvious patterns.\nNotice that there may be outliers, with a few values exceeding 3 standard deviations in magnitude.\nThe ACF of the standardized residuals shows no apparent departure from the model assumptions, no significant lags shown.\nThe normal Q-Q plot of the residuals shows that the assumption of normality is reasonable, with the exception of the fat-tailed.\nThe model appears to fit well.\n\n\n\nShow the code\nmodel_output <- capture.output(sarima(log(UNH_ts), 4,1,3))\n\n\n\n\n\n\n\nShow the code\ncat(model_output[91:130], model_output[length(model_output)], sep = \"\\n\") #to get rid of the convergence status and details of the optimization algorithm used by the sarima() \n\n\nfinal  value -4.149087 \nconverged\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n          ar1     ar2     ar3      ar4     ma1      ma2      ma3  constant\n      -0.6673  0.7666  0.7124  -0.0768  0.6040  -0.7506  -0.7104     9e-04\ns.e.   0.0485  0.0455  0.0545   0.0218  0.0461   0.0382   0.0512     2e-04\n\nsigma^2 estimated as 0.0002489:  log likelihood = 8908.48,  aic = -17798.95\n\n$degrees_of_freedom\n[1] 3255\n\n$ttable\n         Estimate     SE  t.value p.value\nar1       -0.6673 0.0485 -13.7699   0e+00\nar2        0.7666 0.0455  16.8610   0e+00\nar3        0.7124 0.0545  13.0760   0e+00\nar4       -0.0768 0.0218  -3.5325   4e-04\nma1        0.6040 0.0461  13.1042   0e+00\nma2       -0.7506 0.0382 -19.6542   0e+00\nma3       -0.7104 0.0512 -13.8675   0e+00\nconstant   0.0009 0.0002   6.1278   0e+00\n\n$AIC\n[1] -5.454781\n\n$AICc\n[1] -5.454768\n\n$BIC\n[1] -5.437983\n\nNA\n\n\n\n\nCompare with auto.arima() function\nauto.arima() returns best ARIMA model according to either AIC, AICc or BIC value. The function conducts a search over possible model within the order constraints provided. However, this method is not reliable sometimes. It fits a different model than ACF/PACF plots suggest. This is because auto.arima() usually return models that are more complex as it prefers more parameters compared than to the for example BIC.\n\n\nShow the code\nauto.arima(log(UNH_ts))\n\n\nSeries: log(UNH_ts) \nARIMA(1,1,0) with drift \n\nCoefficients:\n          ar1  drift\n      -0.0821  9e-04\ns.e.   0.0174  3e-04\n\nsigma^2 = 0.0002582:  log likelihood = 8850.38\nAIC=-17694.76   AICc=-17694.76   BIC=-17676.49"
  },
  {
    "objectID": "model_unh.html#step-5-forecast",
    "href": "model_unh.html#step-5-forecast",
    "title": "ARMA/ARIMA/SARIMA Models for UNH",
    "section": "Step 5: Forecast",
    "text": "Step 5: Forecast\nThe blue part in graph below forecast the next 100 values of UNH stock price in 80% and 95% confidence level.\n\n\nShow the code\nlog(UNH_ts) %>%\n  Arima(order=c(4,1,3),include.drift = TRUE) %>%\n  forecast(100) %>%\n  autoplot() +\n  ylab(\"UNH stock prices prediction\") + xlab(\"Year\")"
  },
  {
    "objectID": "model_unh.html#step-6-compare-arima-model-with-the-benchmark-methods",
    "href": "model_unh.html#step-6-compare-arima-model-with-the-benchmark-methods",
    "title": "ARMA/ARIMA/SARIMA Models for UNH",
    "section": "Step 6: Compare ARIMA model with the benchmark methods",
    "text": "Step 6: Compare ARIMA model with the benchmark methods\nForecasting benchmarks are very important when testing new forecasting methods, to see how well they perform against some simple alternatives.\n\nAverage method\nHere, the forecast of all future values are equal to the average of the historical data. The residual plot of this method is not stationary.\n\n\nShow the code\nf1<-meanf(log(UNH_ts), h=251) #mean\n#summary(f1)\ncheckresiduals(f1)#serial correlation ; Lung Box p <0.05\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Mean\nQ* = 1046300, df = 501, p-value < 2.2e-16\n\nModel df: 1.   Total lags used: 502\n\n\n\n\nNaive method\nThis method simply set all forecasts to be the value of the last observation. According to error measurement here, ARIMA(4,1,3) outperform the average method.\n\n\nShow the code\nf2<-naive(log(UNH_ts), h=11) # naive method\nsummary(f2)\n\n\n\nForecast method: Naive method\n\nModel Information:\nCall: naive(y = log(UNH_ts), h = 11) \n\nResidual sd: 0.0161 \n\nError measures:\n                       ME       RMSE        MAE        MPE      MAPE       MASE\nTraining set 0.0009204042 0.01614298 0.01110733 0.01941981 0.2408094 0.04460044\n                    ACF1\nTraining set -0.08216434\n\nForecasts:\n         Point Forecast    Lo 80    Hi 80    Lo 95    Hi 95\n2023.004       6.257173 6.236484 6.277861 6.225533 6.288812\n2023.008       6.257173 6.227915 6.286430 6.212427 6.301918\n2023.012       6.257173 6.221340 6.293005 6.202371 6.311974\n2023.016       6.257173 6.215796 6.298549 6.193893 6.320452\n2023.020       6.257173 6.210913 6.303432 6.186424 6.327921\n2023.024       6.257173 6.206497 6.307848 6.179671 6.334674\n2023.028       6.257173 6.202437 6.311908 6.173462 6.340883\n2023.032       6.257173 6.198658 6.315687 6.167682 6.346663\n2023.036       6.257173 6.195108 6.319237 6.162254 6.352092\n2023.040       6.257173 6.191751 6.322594 6.157119 6.357226\n2023.044       6.257173 6.188558 6.325787 6.152236 6.362109\n\n\nShow the code\ncheckresiduals(f2)#serial correlation ; Lung Box p <0.05\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Naive method\nQ* = 685.37, df = 502, p-value = 8.794e-08\n\nModel df: 0.   Total lags used: 502\n\n\n\n\nSeasonal naive method\nThis method is useful for highly seasonal data, which can set each forecast to be equal to the last observed value from the same season of the year. Here seasonal naive is used to forecast the next 4 values for the UNH stock price series.\n\n\nShow the code\nf3<-snaive(log(UNH_ts), h=4) #seasonal naive method\nsummary(f3)\n\n\n\nForecast method: Seasonal naive method\n\nModel Information:\nCall: snaive(y = log(UNH_ts), h = 4) \n\nResidual sd: 0.2754 \n\nError measures:\n                    ME      RMSE       MAE      MPE     MAPE MASE      ACF1\nTraining set 0.2432513 0.2753742 0.2490408 5.098921 5.207824    1 0.9842087\n\nForecasts:\n         Point Forecast    Lo 80    Hi 80    Lo 95    Hi 95\n2023.004       6.172414 5.819508 6.525321 5.632691 6.712138\n2023.008       6.186518 5.833612 6.539424 5.646794 6.726241\n2023.012       6.189044 5.836138 6.541951 5.649321 6.728768\n2023.016       6.197327 5.844421 6.550233 5.657603 6.737050\n\n\nShow the code\ncheckresiduals(f3) #serial correlation ; Lung Box p <0.05\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Seasonal naive method\nQ* = 226574, df = 502, p-value < 2.2e-16\n\nModel df: 0.   Total lags used: 502\n\n\n\n\nDrift Method\nA variation on the naïve method is to allow the forecasts to increase or decrease over time, where the amount of change over time is set to be the average change seen in the historical data.\n\n\nShow the code\nf4 <- rwf(log(UNH_ts),drift=TRUE, h=20) \nsummary(f4)\n\n\n\nForecast method: Random walk with drift\n\nModel Information:\nCall: rwf(y = log(UNH_ts), h = 20, drift = TRUE) \n\nDrift: 9e-04  (se 3e-04)\nResidual sd: 0.0161 \n\nError measures:\n                       ME       RMSE       MAE           MPE      MAPE\nTraining set 1.071097e-16 0.01611672 0.0110743 -0.0004837745 0.2401268\n                   MASE        ACF1\nTraining set 0.04446783 -0.08216434\n\nForecasts:\n         Point Forecast    Lo 80    Hi 80    Lo 95    Hi 95\n2023.004       6.258093 6.237432 6.278754 6.226495 6.289691\n2023.008       6.259013 6.229790 6.288237 6.214320 6.303706\n2023.012       6.259934 6.224137 6.295730 6.205188 6.314680\n2023.016       6.260854 6.219514 6.302195 6.197629 6.324079\n2023.020       6.261775 6.215547 6.308002 6.191076 6.332473\n2023.024       6.262695 6.212048 6.313342 6.185237 6.340153\n2023.028       6.263615 6.208902 6.318329 6.179938 6.347292\n2023.032       6.264536 6.206036 6.323036 6.175068 6.354004\n2023.036       6.265456 6.203398 6.327514 6.170546 6.360366\n2023.040       6.266377 6.200952 6.331802 6.166318 6.366435\n2023.044       6.267297 6.198668 6.335926 6.162338 6.372256\n2023.048       6.268217 6.196526 6.339909 6.158575 6.377860\n2023.052       6.269138 6.194508 6.343768 6.155001 6.383275\n2023.056       6.270058 6.192599 6.347517 6.151595 6.388522\n2023.060       6.270979 6.190788 6.351169 6.148338 6.393619\n2023.064       6.271899 6.189066 6.354732 6.145217 6.398581\n2023.068       6.272819 6.187424 6.358214 6.142219 6.403420\n2023.072       6.273740 6.185856 6.361624 6.139333 6.408147\n2023.076       6.274660 6.184354 6.364966 6.136549 6.412771\n2023.080       6.275581 6.182914 6.368247 6.133860 6.417301\n\n\nShow the code\ncheckresiduals(f4)\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Random walk with drift\nQ* = 685.37, df = 501, p-value = 7.477e-08\n\nModel df: 1.   Total lags used: 502\n\n\n\n\nShow the code\nautoplot(UNH_ts) +\n  autolayer(meanf(UNH_ts, h=100),\n            series=\"Mean.tr\", PI=FALSE) +\n  autolayer(naive((UNH_ts), h=100),\n            series=\"Naïve.tr\", PI=FALSE) +\n  autolayer(rwf((UNH_ts), drift=TRUE, h=100),\n            series=\"Drift.tr\", PI=FALSE) +\n  autolayer(forecast(Arima((UNH_ts), order=c(4, 1, 3),include.drift = TRUE),100), \n            series=\"fit\",PI=FALSE) +\n  ggtitle(\"UNH Stock Price\") +\n  xlab(\"Time\") + ylab(\"Log(Price)\") +\n  guides(colour=guide_legend(title=\"Forecast\"))\n\n\n\n\n\nAccording to the graph above, ARIMA(4,1,3) outperform most of benchmark method, though its performance is very similar to drift method."
  },
  {
    "objectID": "conclusion.html",
    "href": "conclusion.html",
    "title": "Conclusion",
    "section": "",
    "text": "The conclusion section summarizes the project by answering key data science questions in introduction pages:\n\nHow has the stock price of healthcare companies evolved?\nOver the past 10 years, the US healthcare industry has undergone significant changes and faced a range of challenges, including regulatory changes, technological advancements, and the COVID-19 pandemic. Despite fluctuations and challenges, the US healthcare sector has generally experienced overall growth over the past decade, with stock prices of many healthcare companies increasing significantly. This is due in part to the essential nature of healthcare services, as well as demographic trends such as an aging population.\nThe pharmaceutical and biotechnology sectors, e.g. LLY and MRNA, have been among the strongest performers in the healthcare industry over the past decade, with many companies experiencing significant growth in stock prices. This is due in part to the development of new treatments and therapies, as well as the increasing demand for personalized medicine and targeted therapies.\n\n\nWhat type of healthcare industry create biggest sales?\nHealthcare services companies, such as UHN, have also experienced growth in stock prices over the past decade, as demand for healthcare services has increased. This includes companies such as hospitals, clinics, and insurance providers. UnitedHealth Group (UNH) has become one of the largest healthcare companies in the world, with a market capitalization of over $450 billion as of April 2023. The company’s stock price is also one of the highest in the healthcare industry.\n\n\n\n\n\n\n\n\nHow pandemic affect healthcare industry?\nEven though the healthcare sector has always been a relatively stable markets because of the necessity of the industry, the novel COVID-19 pandemic still exert some mixed effects on the industry. At the beginning of the pandemic, many healthcare stocks initially decreased as investors worried about the potential financial impact of the pandemic on the healthcare industry. However, as the pandemic continued and the demand for healthcare services increased, many healthcare stocks rebounded and some even reached record highs. For example, by comparing the prediction value of pre-COVID value to the real time value, it suggeststhat UNH stock price would not be that high without COVID-19.\n\n\n\nWhen would be think as more risky to invest in healthcare stock market?\nWhile healthcare stock prices have generally experienced growth over the past decade, there has also been significant volatility and uncertainty, particularly around changes in healthcare policy and the impact of the COVID-19 pandemic. The higher the volatility, the riskier the security. High volatility can also create risks for companies and their shareholders, as it can make it difficult to predict earnings or plan for the future. On the other hand, volatility can create opportunities for investors to buy stocks at a lower price and earn a higher return when the market recovers.\n\n\n\nIs there any factors affecting healthcare stock prices?\nThe stock price is influenced by a wide range of factors, including the company’s financial performance, strategic decisions, and broader trends in the healthcare industry and the economy, such as GDP and Unemployment rate. Especially during the COVID-19 time, COVID-19 case numbers and COVID-19 vaccine rates can affect the stock prices. The COVID-19 vaccine rates would mainly affect the biotechnology companies, such as Moderna, while other healthcare companies who are involved in other areas are less likely affected by number of COVID-19 vaccine rates, such as United Health Group.\n\n\nAre we able to predict the stock prices of these companies?\nThere are a bunch of models can predict the stock prices, as we discussed in this project, including ARIMA, ARIMAX, ARCH, deep learning and etc. An ARIMA model is far easier to set up and should be considered, especially with its ability to be interpretable, but a neural network is an excellent alternative."
  },
  {
    "objectID": "ts.html#calculaing-returns",
    "href": "ts.html#calculaing-returns",
    "title": "Financial Time Series Models: CVS Example",
    "section": "Calculaing Returns",
    "text": "Calculaing Returns\nFit an appropriate AR+ARCH/ARMA+GARCH or ARIMA-ARCH/GARCH for the returns data.\n\n\nShow the code\n#### calculating Returns\nreturns = log(CVS_ts) %>% diff()\nautoplot(returns) +ggtitle(\"Returns of CVS Stock Price\")"
  },
  {
    "objectID": "ts.html#acf-pacf-plots-of-the-returns",
    "href": "ts.html#acf-pacf-plots-of-the-returns",
    "title": "Financial Time Series Models: CVS Example",
    "section": "ACF, PACF plots of the returns",
    "text": "ACF, PACF plots of the returns\n\n\nShow the code\nggAcf(returns)\n\n\n\n\n\n\n\nShow the code\nggPacf(returns)\n\n\n\n\n\nThese plots here shows a closer look of ACF and PACF plots, which are weakly stationary.\n\n\nShow the code\n## have a closer look\nacf(returns)\n\n\n\n\n\n\n\nShow the code\npacf(returns)\n\n\n\n\n\nLet’s look at the ACF of absolute values of the returns and squared values. We can see clear correlation in both plots. This correlation is comming from the correlation in conditional variation.\n\n\nShow the code\nacf(abs(returns))\n\n\n\n\n\n\n\nShow the code\nacf(returns^2)"
  },
  {
    "objectID": "ts.html#model-fitting-method",
    "href": "ts.html#model-fitting-method",
    "title": "Financial Time Series Models: CVS Example",
    "section": "Model Fitting Method",
    "text": "Model Fitting Method\nThere are two ways we can think of fitting the models. First is that we fit the ARIMA model first and fit a GARCH model for the residual. Second method will be fitting a GARRCH model for the squared returns directly."
  },
  {
    "objectID": "ts.html#model-fitting-method-1-garchpq-model-fittin",
    "href": "ts.html#model-fitting-method-1-garchpq-model-fittin",
    "title": "Financial Time Series Models: CVS Example",
    "section": "Model Fitting Method 1: GARCH(p,q) model fittin",
    "text": "Model Fitting Method 1: GARCH(p,q) model fittin\n\nArchTest\n\n\nShow the code\nlibrary(FinTS)\n\n\n\nAttaching package: 'FinTS'\n\n\nThe following object is masked from 'package:forecast':\n\n    Acf\n\n\nShow the code\nArchTest(returns, lags=1, demean=TRUE)\n\n\n\n    ARCH LM-test; Null hypothesis: no ARCH effects\n\ndata:  returns\nChi-squared = 185.64, df = 1, p-value < 2.2e-16\n\n\nBecause the p-value is < 0.05, we reject the null hypothesis and conclude the presence of ARCH(1) effects.\n\n\nFitting an ARIMA model\nLet’s fit the ARIMA model first.\n\n\nShow the code\nARIMA.c=function(p1,p2,q1,q2,data){\ntemp=c()\nd=1\ni=1\ntemp= data.frame()\nls=matrix(rep(NA,6*50),nrow=50)\n\n\nfor (p in p1:p2)#\n{\n  for(q in q1:q2)#\n  {\n    for(d in 0:2)#\n    {\n      \n      if(p+d+q<=6)\n      {\n        \n        model<- Arima(data,order=c(p,d,q))\n        ls[i,]= c(p,d,q,model$aic,model$bic,model$aicc)\n        i=i+1\n  \n        \n      }\n      \n    }\n  }\n}\n\n\ntemp= as.data.frame(ls)\nnames(temp)= c(\"p\",\"d\",\"q\",\"AIC\",\"BIC\",\"AICc\")\n\ntemp\n}\n\n\n\n\nShow the code\n#na.omit(log(bts)))\noutput <- ARIMA.c(0,2,0,2,data=log(CVS_ts))\noutput\n\n\n    p  d  q        AIC        BIC       AICc\n1   0  0  0   5188.446   5201.428   5188.449\n2   0  1  0 -28803.633 -28797.142 -28803.632\n3   0  2  0 -25285.070 -25278.580 -25285.069\n4   0  0  1  -1368.727  -1349.255  -1368.722\n5   0  1  1 -28805.510 -28792.528 -28805.507\n6   0  2  1 -28787.667 -28774.686 -28787.665\n7   0  0  2  -6872.140  -6846.177  -6872.132\n8   0  1  2 -28808.808 -28789.336 -28808.803\n9   0  2  2 -28789.562 -28770.090 -28789.557\n10  1  0  0 -28798.783 -28779.311 -28798.778\n11  1  1  0 -28805.766 -28792.784 -28805.763\n12  1  2  0 -26889.073 -26876.092 -26889.070\n13  1  0  1 -28800.561 -28774.598 -28800.553\n14  1  1  1 -28807.187 -28787.715 -28807.182\n15  1  2  1 -28789.827 -28770.356 -28789.822\n16  1  0  2 -28803.967 -28771.513 -28803.955\n17  1  1  2 -28806.786 -28780.823 -28806.778\n18  1  2  2 -28785.523 -28759.561 -28785.514\n19  2  0  0 -28800.794 -28774.831 -28800.786\n20  2  1  0 -28808.910 -28789.438 -28808.905\n21  2  2  0 -27443.617 -27424.146 -27443.612\n22  2  0  1 -28802.321 -28769.867 -28802.308\n23  2  1  1 -28806.931 -28780.969 -28806.923\n24  2  2  1 -28792.937 -28766.976 -28792.929\n25  2  0  2 -28796.612 -28757.666 -28796.594\n26  2  1  2 -28804.961 -28772.508 -28804.949\n27  2  2  2 -28787.125 -28754.673 -28787.113\n28 NA NA NA         NA         NA         NA\n29 NA NA NA         NA         NA         NA\n30 NA NA NA         NA         NA         NA\n31 NA NA NA         NA         NA         NA\n32 NA NA NA         NA         NA         NA\n33 NA NA NA         NA         NA         NA\n34 NA NA NA         NA         NA         NA\n35 NA NA NA         NA         NA         NA\n36 NA NA NA         NA         NA         NA\n37 NA NA NA         NA         NA         NA\n38 NA NA NA         NA         NA         NA\n39 NA NA NA         NA         NA         NA\n40 NA NA NA         NA         NA         NA\n41 NA NA NA         NA         NA         NA\n42 NA NA NA         NA         NA         NA\n43 NA NA NA         NA         NA         NA\n44 NA NA NA         NA         NA         NA\n45 NA NA NA         NA         NA         NA\n46 NA NA NA         NA         NA         NA\n47 NA NA NA         NA         NA         NA\n48 NA NA NA         NA         NA         NA\n49 NA NA NA         NA         NA         NA\n50 NA NA NA         NA         NA         NA\n\n\n\n\nShow the code\noutput[which.min(output$AIC),] \n\n\n   p d q       AIC       BIC      AICc\n20 2 1 0 -28808.91 -28789.44 -28808.91\n\n\n\n\nShow the code\noutput[which.min(output$BIC),] \n\n\n  p d q       AIC       BIC      AICc\n2 0 1 0 -28803.63 -28797.14 -28803.63\n\n\n\n\nShow the code\noutput[which.min(output$AICc),] \n\n\n   p d q       AIC       BIC      AICc\n20 2 1 0 -28808.91 -28789.44 -28808.91\n\n\n\n\nShow the code\nauto.arima(log(CVS_ts))\n\n\nSeries: log(CVS_ts) \nARIMA(3,1,0) \n\nCoefficients:\n          ar1     ar2     ar3\n      -0.0284  0.0327  0.0050\ns.e.   0.0143  0.0143  0.0144\n\nsigma^2 = 0.0001576:  log likelihood = 14407.52\nAIC=-28807.03   AICc=-28807.02   BIC=-28781.07\n\n\n\n\nShow the code\ndata=log(CVS_ts)\nsarima(data, 0,1,0) #has lower BIC\n\n\ninitial  value -4.377149 \niter   1 value -4.377149\nfinal  value -4.377149 \nconverged\ninitial  value -4.377149 \niter   1 value -4.377149\nfinal  value -4.377149 \nconverged\n\n\n\n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n      constant\n         2e-04\ns.e.     2e-04\n\nsigma^2 estimated as 0.0001578:  log likelihood = 14403.53,  aic = -28803.05\n\n$degrees_of_freedom\n[1] 4868\n\n$ttable\n         Estimate    SE t.value p.value\nconstant    2e-04 2e-04  1.1879  0.2349\n\n$AIC\n[1] -5.915599\n\n$AICc\n[1] -5.915599\n\n$BIC\n[1] -5.912933\n\n\nI’m going to choose ARIMA(0,1,0) because it has the lowest BIC and the hole model diagnostics are the same.\n\n\nFit the GARCH model\nFirst fit the ARIMA model and fitting a GARCH model to the residuals of the ARIMA model.\n\n\nShow the code\narima.fit<-Arima(data,order=c(0,1,0),include.drift = TRUE)\narima.res<-arima.fit$residuals\n\nacf(arima.res)\n\n\n\n\n\n\n\nShow the code\nacf(arima.res^2) #clear correlation 1,3,4,5,6,7\n\n\n\n\n\n\n\nShow the code\npacf(arima.res^2) #1,3,4,7\n\n\n\n\n\n\n\nShow the code\nmodel <- list() ## set counter\ncc <- 1\nfor (p in 1:7) {\n  for (q in 1:7) {\n  \nmodel[[cc]] <- garch(arima.res,order=c(q,p),trace=F)\ncc <- cc + 1\n}\n} \n\n## get AIC values for model evaluation\nGARCH_AIC <- sapply(model, AIC) ## model with lowest AIC is the best\nwhich(GARCH_AIC == min(GARCH_AIC))\n\n\n[1] 7\n\n\n\n\nShow the code\nmodel[[which(GARCH_AIC == min(GARCH_AIC))]]\n\n\n\nCall:\ngarch(x = arima.res, order = c(q, p), trace = F)\n\nCoefficient(s):\n       a0         a1         b1         b2         b3         b4         b5  \n9.954e-06  1.886e-01  1.034e-08  7.187e-02  5.132e-03  7.429e-02  1.092e-01  \n       b6         b7  \n2.588e-01  2.296e-01  \n\n\n\n\nShow the code\nsummary(garchFit(~garch(1,3), arima.res,trace = F))\n\n\n\nTitle:\n GARCH Modelling \n\nCall:\n garchFit(formula = ~garch(1, 3), data = arima.res, trace = F) \n\nMean and Variance Equation:\n data ~ garch(1, 3)\n<environment: 0x000001953db8c950>\n [data = arima.res]\n\nConditional Distribution:\n norm \n\nCoefficient(s):\n        mu       omega      alpha1       beta1       beta2       beta3  \n6.5984e-06  7.7096e-06  1.1708e-01  1.0000e-08  6.0691e-01  2.3160e-01  \n\nStd. Errors:\n based on Hessian \n\nError Analysis:\n        Estimate  Std. Error  t value Pr(>|t|)    \nmu     6.598e-06   1.553e-04    0.042  0.96612    \nomega  7.710e-06   2.105e-06    3.663  0.00025 ***\nalpha1 1.171e-01   2.496e-02    4.690 2.73e-06 ***\nbeta1  1.000e-08   2.671e-01    0.000  1.00000    \nbeta2  6.069e-01   8.319e-02    7.296 2.97e-13 ***\nbeta3  2.316e-01   1.851e-01    1.251  0.21076    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nLog Likelihood:\n 14765.65    normalized:  3.031961 \n\nDescription:\n Sat May  6 19:21:46 2023 by user: yujia \n\n\nStandardised Residuals Tests:\n                                Statistic p-Value   \n Jarque-Bera Test   R    Chi^2  13768.98  0         \n Shapiro-Wilk Test  R    W      0.8849537 0         \n Ljung-Box Test     R    Q(10)  16.40464  0.08862022\n Ljung-Box Test     R    Q(15)  21.02519  0.1360265 \n Ljung-Box Test     R    Q(20)  24.48768  0.2217382 \n Ljung-Box Test     R^2  Q(10)  12.52544  0.251428  \n Ljung-Box Test     R^2  Q(15)  23.88171  0.0671297 \n Ljung-Box Test     R^2  Q(20)  37.28578  0.01081188\n LM Arch Test       R    TR^2   13.95831  0.3033781 \n\nInformation Criterion Statistics:\n      AIC       BIC       SIC      HQIC \n-6.061457 -6.053460 -6.061460 -6.058651 \n\n\nbeta 3 is not significant. So I’m going to try GARCH(1,1) and GARCH(1,2).\n\n\nShow the code\nsummary(garchFit(~garch(1,2), arima.res,trace = F))\n\n\n\nTitle:\n GARCH Modelling \n\nCall:\n garchFit(formula = ~garch(1, 2), data = arima.res, trace = F) \n\nMean and Variance Equation:\n data ~ garch(1, 2)\n<environment: 0x000001953da0c198>\n [data = arima.res]\n\nConditional Distribution:\n norm \n\nCoefficient(s):\n        mu       omega      alpha1       beta1       beta2  \n6.5984e-06  6.6528e-06  9.7007e-02  1.5783e-01  7.0639e-01  \n\nStd. Errors:\n based on Hessian \n\nError Analysis:\n        Estimate  Std. Error  t value Pr(>|t|)    \nmu     6.598e-06   1.563e-04    0.042    0.966    \nomega  6.653e-06   1.450e-06    4.588 4.48e-06 ***\nalpha1 9.701e-02   1.186e-02    8.183 2.22e-16 ***\nbeta1  1.578e-01   3.856e-02    4.092 4.27e-05 ***\nbeta2  7.064e-01   3.909e-02   18.073  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nLog Likelihood:\n 14757.81    normalized:  3.03035 \n\nDescription:\n Sat May  6 19:21:48 2023 by user: yujia \n\n\nStandardised Residuals Tests:\n                                Statistic p-Value   \n Jarque-Bera Test   R    Chi^2  13277.74  0         \n Shapiro-Wilk Test  R    W      0.885234  0         \n Ljung-Box Test     R    Q(10)  16.12467  0.0961187 \n Ljung-Box Test     R    Q(15)  20.97592  0.1376006 \n Ljung-Box Test     R    Q(20)  24.21644  0.2330654 \n Ljung-Box Test     R^2  Q(10)  13.41383  0.2014453 \n Ljung-Box Test     R^2  Q(15)  24.89258  0.05140473\n Ljung-Box Test     R^2  Q(20)  38.30603  0.00812319\n LM Arch Test       R    TR^2   14.54909  0.2670261 \n\nInformation Criterion Statistics:\n      AIC       BIC       SIC      HQIC \n-6.058647 -6.051983 -6.058649 -6.056308 \n\n\n\n\nShow the code\nsummary(garchFit(~garch(1,1), arima.res,trace = F))\n\n\n\nTitle:\n GARCH Modelling \n\nCall:\n garchFit(formula = ~garch(1, 1), data = arima.res, trace = F) \n\nMean and Variance Equation:\n data ~ garch(1, 1)\n<environment: 0x000001953bee5b98>\n [data = arima.res]\n\nConditional Distribution:\n norm \n\nCoefficient(s):\n        mu       omega      alpha1       beta1  \n6.5984e-06  3.2972e-06  5.0081e-02  9.3069e-01  \n\nStd. Errors:\n based on Hessian \n\nError Analysis:\n        Estimate  Std. Error  t value Pr(>|t|)    \nmu     6.598e-06   1.578e-04    0.042    0.967    \nomega  3.297e-06   7.454e-07    4.423 9.72e-06 ***\nalpha1 5.008e-02   6.617e-03    7.569 3.77e-14 ***\nbeta1  9.307e-01   9.836e-03   94.616  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nLog Likelihood:\n 14736.05    normalized:  3.025883 \n\nDescription:\n Sat May  6 19:21:49 2023 by user: yujia \n\n\nStandardised Residuals Tests:\n                                Statistic p-Value   \n Jarque-Bera Test   R    Chi^2  15641.72  0         \n Shapiro-Wilk Test  R    W      0.8830468 0         \n Ljung-Box Test     R    Q(10)  15.06102  0.1298529 \n Ljung-Box Test     R    Q(15)  19.81811  0.1790219 \n Ljung-Box Test     R    Q(20)  23.49551  0.2651236 \n Ljung-Box Test     R^2  Q(10)  14.27008  0.1610251 \n Ljung-Box Test     R^2  Q(15)  24.61863  0.05530551\n Ljung-Box Test     R^2  Q(20)  35.98932  0.01542563\n LM Arch Test       R    TR^2   16.29535  0.1780798 \n\nInformation Criterion Statistics:\n      AIC       BIC       SIC      HQIC \n-6.050124 -6.044793 -6.050125 -6.048253 \n\n\nSince all the models has similar AIC ,BIC values, I would go with GARCH(1,1) which all the coefficients are significant.\n\n\nFinal Model\n\n\nShow the code\nsummary(arima.fit<-Arima(data,order=c(0,1,0),include.drift = TRUE))\n\n\nSeries: data \nARIMA(0,1,0) with drift \n\nCoefficients:\n      drift\n      2e-04\ns.e.  2e-04\n\nsigma^2 = 0.0001578:  log likelihood = 14403.53\nAIC=-28803.05   AICc=-28803.05   BIC=-28790.07\n\nTraining set error measures:\n                       ME       RMSE         MAE           MPE      MAPE\nTraining set 6.598419e-07 0.01255991 0.007331295 -0.0001026668 0.1847525\n                   MASE       ACF1\nTraining set 0.03791226 -0.0294275\n\n\n\n\nShow the code\nsummary(final.fit <- garchFit(~garch(1,1), arima.res,trace = F)) \n\n\n\nTitle:\n GARCH Modelling \n\nCall:\n garchFit(formula = ~garch(1, 1), data = arima.res, trace = F) \n\nMean and Variance Equation:\n data ~ garch(1, 1)\n<environment: 0x000001953d506130>\n [data = arima.res]\n\nConditional Distribution:\n norm \n\nCoefficient(s):\n        mu       omega      alpha1       beta1  \n6.5984e-06  3.2972e-06  5.0081e-02  9.3069e-01  \n\nStd. Errors:\n based on Hessian \n\nError Analysis:\n        Estimate  Std. Error  t value Pr(>|t|)    \nmu     6.598e-06   1.578e-04    0.042    0.967    \nomega  3.297e-06   7.454e-07    4.423 9.72e-06 ***\nalpha1 5.008e-02   6.617e-03    7.569 3.77e-14 ***\nbeta1  9.307e-01   9.836e-03   94.616  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nLog Likelihood:\n 14736.05    normalized:  3.025883 \n\nDescription:\n Sat May  6 19:21:51 2023 by user: yujia \n\n\nStandardised Residuals Tests:\n                                Statistic p-Value   \n Jarque-Bera Test   R    Chi^2  15641.72  0         \n Shapiro-Wilk Test  R    W      0.8830468 0         \n Ljung-Box Test     R    Q(10)  15.06102  0.1298529 \n Ljung-Box Test     R    Q(15)  19.81811  0.1790219 \n Ljung-Box Test     R    Q(20)  23.49551  0.2651236 \n Ljung-Box Test     R^2  Q(10)  14.27008  0.1610251 \n Ljung-Box Test     R^2  Q(15)  24.61863  0.05530551\n Ljung-Box Test     R^2  Q(20)  35.98932  0.01542563\n LM Arch Test       R    TR^2   16.29535  0.1780798 \n\nInformation Criterion Statistics:\n      AIC       BIC       SIC      HQIC \n-6.050124 -6.044793 -6.050125 -6.048253 \n\n\n\n\nForecast\n\n\nShow the code\npredict(final.fit, n.ahead = 100, plot=TRUE)\n\n\n\n\n\n    meanForecast  meanError standardDeviation lowerInterval upperInterval\n1   6.598419e-06 0.01476927        0.01476927   -0.02894064    0.02895384\n2   6.598419e-06 0.01473888        0.01473888   -0.02888108    0.02889428\n3   6.598419e-06 0.01470902        0.01470902   -0.02882255    0.02883574\n4   6.598419e-06 0.01467967        0.01467967   -0.02876502    0.02877822\n5   6.598419e-06 0.01465083        0.01465083   -0.02870849    0.02872169\n6   6.598419e-06 0.01462248        0.01462248   -0.02865294    0.02866614\n7   6.598419e-06 0.01459463        0.01459463   -0.02859835    0.02861155\n8   6.598419e-06 0.01456726        0.01456726   -0.02854471    0.02855791\n9   6.598419e-06 0.01454037        0.01454037   -0.02849200    0.02850520\n10  6.598419e-06 0.01451395        0.01451395   -0.02844022    0.02845341\n11  6.598419e-06 0.01448799        0.01448799   -0.02838933    0.02840253\n12  6.598419e-06 0.01446248        0.01446248   -0.02833934    0.02835253\n13  6.598419e-06 0.01443742        0.01443742   -0.02829022    0.02830342\n14  6.598419e-06 0.01441279        0.01441279   -0.02824196    0.02825516\n15  6.598419e-06 0.01438861        0.01438861   -0.02819455    0.02820775\n16  6.598419e-06 0.01436484        0.01436484   -0.02814797    0.02816117\n17  6.598419e-06 0.01434150        0.01434150   -0.02810222    0.02811541\n18  6.598419e-06 0.01431856        0.01431856   -0.02805727    0.02807047\n19  6.598419e-06 0.01429603        0.01429603   -0.02801312    0.02802631\n20  6.598419e-06 0.01427391        0.01427391   -0.02796974    0.02798294\n21  6.598419e-06 0.01425217        0.01425217   -0.02792714    0.02794033\n22  6.598419e-06 0.01423082        0.01423082   -0.02788529    0.02789849\n23  6.598419e-06 0.01420984        0.01420984   -0.02784418    0.02785738\n24  6.598419e-06 0.01418924        0.01418924   -0.02780381    0.02781701\n25  6.598419e-06 0.01416901        0.01416901   -0.02776416    0.02777735\n26  6.598419e-06 0.01414914        0.01414914   -0.02772521    0.02773840\n27  6.598419e-06 0.01412962        0.01412962   -0.02768696    0.02770015\n28  6.598419e-06 0.01411046        0.01411046   -0.02764939    0.02766259\n29  6.598419e-06 0.01409163        0.01409163   -0.02761249    0.02762569\n30  6.598419e-06 0.01407315        0.01407315   -0.02757626    0.02758946\n31  6.598419e-06 0.01405499        0.01405499   -0.02754068    0.02755387\n32  6.598419e-06 0.01403716        0.01403716   -0.02750573    0.02751893\n33  6.598419e-06 0.01401965        0.01401965   -0.02747142    0.02748462\n34  6.598419e-06 0.01400246        0.01400246   -0.02743772    0.02745092\n35  6.598419e-06 0.01398558        0.01398558   -0.02740463    0.02741783\n36  6.598419e-06 0.01396900        0.01396900   -0.02737214    0.02738534\n37  6.598419e-06 0.01395272        0.01395272   -0.02734024    0.02735343\n38  6.598419e-06 0.01393674        0.01393674   -0.02730891    0.02732210\n39  6.598419e-06 0.01392104        0.01392104   -0.02727815    0.02729134\n40  6.598419e-06 0.01390563        0.01390563   -0.02724794    0.02726114\n41  6.598419e-06 0.01389050        0.01389050   -0.02721829    0.02723149\n42  6.598419e-06 0.01387565        0.01387565   -0.02718917    0.02720237\n43  6.598419e-06 0.01386106        0.01386106   -0.02716059    0.02717379\n44  6.598419e-06 0.01384675        0.01384675   -0.02713252    0.02714572\n45  6.598419e-06 0.01383269        0.01383269   -0.02710497    0.02711817\n46  6.598419e-06 0.01381888        0.01381888   -0.02707792    0.02709111\n47  6.598419e-06 0.01380533        0.01380533   -0.02705136    0.02706456\n48  6.598419e-06 0.01379203        0.01379203   -0.02702529    0.02703848\n49  6.598419e-06 0.01377897        0.01377897   -0.02699969    0.02701289\n50  6.598419e-06 0.01376615        0.01376615   -0.02697456    0.02698776\n51  6.598419e-06 0.01375357        0.01375357   -0.02694990    0.02696309\n52  6.598419e-06 0.01374121        0.01374121   -0.02692568    0.02693888\n53  6.598419e-06 0.01372908        0.01372908   -0.02690191    0.02691511\n54  6.598419e-06 0.01371718        0.01371718   -0.02687858    0.02689177\n55  6.598419e-06 0.01370549        0.01370549   -0.02685567    0.02686887\n56  6.598419e-06 0.01369402        0.01369402   -0.02683319    0.02684639\n57  6.598419e-06 0.01368276        0.01368276   -0.02681112    0.02682432\n58  6.598419e-06 0.01367171        0.01367171   -0.02678946    0.02680265\n59  6.598419e-06 0.01366086        0.01366086   -0.02676819    0.02678139\n60  6.598419e-06 0.01365021        0.01365021   -0.02674732    0.02676052\n61  6.598419e-06 0.01363976        0.01363976   -0.02672684    0.02674003\n62  6.598419e-06 0.01362950        0.01362950   -0.02670673    0.02671993\n63  6.598419e-06 0.01361943        0.01361943   -0.02668700    0.02670019\n64  6.598419e-06 0.01360955        0.01360955   -0.02666763    0.02668082\n65  6.598419e-06 0.01359985        0.01359985   -0.02664862    0.02666181\n66  6.598419e-06 0.01359033        0.01359033   -0.02662996    0.02664315\n67  6.598419e-06 0.01358099        0.01358099   -0.02661164    0.02662484\n68  6.598419e-06 0.01357182        0.01357182   -0.02659367    0.02660687\n69  6.598419e-06 0.01356282        0.01356282   -0.02657603    0.02658923\n70  6.598419e-06 0.01355398        0.01355398   -0.02655872    0.02657192\n71  6.598419e-06 0.01354532        0.01354532   -0.02654173    0.02655493\n72  6.598419e-06 0.01353681        0.01353681   -0.02652506    0.02653826\n73  6.598419e-06 0.01352846        0.01352846   -0.02650870    0.02652189\n74  6.598419e-06 0.01352027        0.01352027   -0.02649264    0.02650583\n75  6.598419e-06 0.01351223        0.01351223   -0.02647688    0.02649008\n76  6.598419e-06 0.01350434        0.01350434   -0.02646141    0.02647461\n77  6.598419e-06 0.01349659        0.01349659   -0.02644624    0.02645943\n78  6.598419e-06 0.01348899        0.01348899   -0.02643135    0.02644454\n79  6.598419e-06 0.01348154        0.01348154   -0.02641673    0.02642993\n80  6.598419e-06 0.01347422        0.01347422   -0.02640239    0.02641559\n81  6.598419e-06 0.01346704        0.01346704   -0.02638831    0.02640151\n82  6.598419e-06 0.01345999        0.01345999   -0.02637450    0.02638770\n83  6.598419e-06 0.01345308        0.01345308   -0.02636095    0.02637415\n84  6.598419e-06 0.01344629        0.01344629   -0.02634765    0.02636085\n85  6.598419e-06 0.01343964        0.01343964   -0.02633461    0.02634780\n86  6.598419e-06 0.01343310        0.01343310   -0.02632180    0.02633500\n87  6.598419e-06 0.01342669        0.01342669   -0.02630924    0.02632243\n88  6.598419e-06 0.01342040        0.01342040   -0.02629691    0.02631011\n89  6.598419e-06 0.01341423        0.01341423   -0.02628481    0.02629801\n90  6.598419e-06 0.01340818        0.01340818   -0.02627294    0.02628614\n91  6.598419e-06 0.01340223        0.01340223   -0.02626130    0.02627449\n92  6.598419e-06 0.01339640        0.01339640   -0.02624987    0.02626306\n93  6.598419e-06 0.01339068        0.01339068   -0.02623866    0.02625185\n94  6.598419e-06 0.01338507        0.01338507   -0.02622765    0.02624085\n95  6.598419e-06 0.01337956        0.01337956   -0.02621686    0.02623006\n96  6.598419e-06 0.01337416        0.01337416   -0.02620627    0.02621946\n97  6.598419e-06 0.01336885        0.01336885   -0.02619587    0.02620907\n98  6.598419e-06 0.01336365        0.01336365   -0.02618568    0.02619887\n99  6.598419e-06 0.01335855        0.01335855   -0.02617567    0.02618887\n100 6.598419e-06 0.01335354        0.01335354   -0.02616586    0.02617905\n\n\n\n\nVolatality plot\n\n\nShow the code\nht <- final.fit@h.t \n\ndata= data.frame(ht,CVS_df$Date)\nggplot(data, aes(y = ht, x = CVS_df$Date)) + geom_line(col = '#ff9933') + ylab('Conditional Variance') + xlab('Date')+ggtitle(\"Volatility plot of CVS Stock Price\")\n\n\n\n\n\nThere’s obvious volatality 2016 that’s when U.S. presidential election and potential changes to healthcare policy, then even more volatality in 2020 because of COVID."
  },
  {
    "objectID": "ts.html#model-fitting-method-2-garchpq-model-fitting",
    "href": "ts.html#model-fitting-method-2-garchpq-model-fitting",
    "title": "Financial Time Series Models: CVS Example",
    "section": "Model Fitting Method 2: GARCH(p,q) model fitting",
    "text": "Model Fitting Method 2: GARCH(p,q) model fitting\nHere is going to fit a GARCH model directly.\n\n\nShow the code\npacf(returns^2) #p=1,3,4\n\n\n\n\n\n\n\nShow the code\nmodel <- list() ## set counter\ncc <- 1\nfor (p in 1:7) {\n  for (q in 1:7) {\n  \nmodel[[cc]] <- garch(returns,order=c(q,p),trace=F)\ncc <- cc + 1\n}\n} \n\n## get AIC values for model evaluation\nGARCH_AIC <- sapply(model, AIC) ## model with lowest AIC is the best\nwhich(GARCH_AIC == min(GARCH_AIC))\n\n\n[1] 7\n\n\n\n\nShow the code\nmodel[[which(GARCH_AIC == min(GARCH_AIC))]]\n\n\n\nCall:\ngarch(x = returns, order = c(q, p), trace = F)\n\nCoefficient(s):\n       a0         a1         b1         b2         b3         b4         b5  \n9.365e-06  1.889e-01  3.534e-03  7.306e-02  1.971e-09  7.335e-02  1.122e-01  \n       b6         b7  \n2.612e-01  2.304e-01  \n\n\n\n\nShow the code\nsummary(garchFit(~garch(1,3), returns,trace = F))\n\n\n\nTitle:\n GARCH Modelling \n\nCall:\n garchFit(formula = ~garch(1, 3), data = returns, trace = F) \n\nMean and Variance Equation:\n data ~ garch(1, 3)\n<environment: 0x000001953daa83a8>\n [data = returns]\n\nConditional Distribution:\n norm \n\nCoefficient(s):\n        mu       omega      alpha1       beta1       beta2       beta3  \n3.5413e-04  7.7412e-06  1.1746e-01  1.0000e-08  6.0492e-01  2.3303e-01  \n\nStd. Errors:\n based on Hessian \n\nError Analysis:\n        Estimate  Std. Error  t value Pr(>|t|)    \nmu     3.541e-04   1.554e-04    2.279 0.022696 *  \nomega  7.741e-06   2.103e-06    3.681 0.000232 ***\nalpha1 1.175e-01   2.465e-02    4.765 1.89e-06 ***\nbeta1  1.000e-08   2.635e-01    0.000 1.000000    \nbeta2  6.049e-01   8.425e-02    7.180 6.97e-13 ***\nbeta3  2.330e-01   1.812e-01    1.286 0.198408    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nLog Likelihood:\n 14762.73    normalized:  3.031983 \n\nDescription:\n Sat May  6 19:21:54 2023 by user: yujia \n\n\nStandardised Residuals Tests:\n                                Statistic p-Value   \n Jarque-Bera Test   R    Chi^2  13757.84  0         \n Shapiro-Wilk Test  R    W      0.8853728 0         \n Ljung-Box Test     R    Q(10)  16.46796  0.08699736\n Ljung-Box Test     R    Q(15)  21.05861  0.1349671 \n Ljung-Box Test     R    Q(20)  24.53261  0.2199005 \n Ljung-Box Test     R^2  Q(10)  12.43878  0.2567628 \n Ljung-Box Test     R^2  Q(15)  23.71456  0.07010195\n Ljung-Box Test     R^2  Q(20)  37.0202   0.01163694\n LM Arch Test       R    TR^2   13.88701  0.3079816 \n\nInformation Criterion Statistics:\n      AIC       BIC       SIC      HQIC \n-6.061502 -6.053504 -6.061505 -6.058695 \n\n\nBeta 3 is not significant. So I’m going to try GARCH(1,1), GARCH(2,2), GARCH(1,2) and GARCH(2,1)\n\n\nShow the code\nsummary(garchFit(~garch(1,1), returns,trace = F)) #all significant\n\n\n\nTitle:\n GARCH Modelling \n\nCall:\n garchFit(formula = ~garch(1, 1), data = returns, trace = F) \n\nMean and Variance Equation:\n data ~ garch(1, 1)\n<environment: 0x000001953e0d4e60>\n [data = returns]\n\nConditional Distribution:\n norm \n\nCoefficient(s):\n        mu       omega      alpha1       beta1  \n3.5116e-04  3.2985e-06  5.0176e-02  9.3060e-01  \n\nStd. Errors:\n based on Hessian \n\nError Analysis:\n        Estimate  Std. Error  t value Pr(>|t|)    \nmu     3.512e-04   1.577e-04    2.226    0.026 *  \nomega  3.298e-06   7.474e-07    4.413 1.02e-05 ***\nalpha1 5.018e-02   6.646e-03    7.550 4.37e-14 ***\nbeta1  9.306e-01   9.880e-03   94.191  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nLog Likelihood:\n 14733.01    normalized:  3.02588 \n\nDescription:\n Sat May  6 19:21:55 2023 by user: yujia \n\n\nStandardised Residuals Tests:\n                                Statistic p-Value   \n Jarque-Bera Test   R    Chi^2  15608.8   0         \n Shapiro-Wilk Test  R    W      0.8834462 0         \n Ljung-Box Test     R    Q(10)  15.12868  0.1274403 \n Ljung-Box Test     R    Q(15)  19.86589  0.1771381 \n Ljung-Box Test     R    Q(20)  23.54299  0.2629252 \n Ljung-Box Test     R^2  Q(10)  14.20457  0.1638634 \n Ljung-Box Test     R^2  Q(15)  24.50026  0.05707071\n Ljung-Box Test     R^2  Q(20)  35.79802  0.0162434 \n LM Arch Test       R    TR^2   16.24371  0.1803268 \n\nInformation Criterion Statistics:\n      AIC       BIC       SIC      HQIC \n-6.050116 -6.044784 -6.050118 -6.048245 \n\n\n\n\nShow the code\nsummary(garchFit(~garch(1,2), returns,trace = F)) #all significant\n\n\n\nTitle:\n GARCH Modelling \n\nCall:\n garchFit(formula = ~garch(1, 2), data = returns, trace = F) \n\nMean and Variance Equation:\n data ~ garch(1, 2)\n<environment: 0x000001953f631e88>\n [data = returns]\n\nConditional Distribution:\n norm \n\nCoefficient(s):\n        mu       omega      alpha1       beta1       beta2  \n3.4949e-04  6.6517e-06  9.7054e-02  1.5883e-01  7.0537e-01  \n\nStd. Errors:\n based on Hessian \n\nError Analysis:\n        Estimate  Std. Error  t value Pr(>|t|)    \nmu     3.495e-04   1.562e-04    2.237   0.0253 *  \nomega  6.652e-06   1.451e-06    4.583 4.58e-06 ***\nalpha1 9.705e-02   1.189e-02    8.165 2.22e-16 ***\nbeta1  1.588e-01   3.879e-02    4.094 4.24e-05 ***\nbeta2  7.054e-01   3.928e-02   17.955  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nLog Likelihood:\n 14754.64    normalized:  3.030323 \n\nDescription:\n Sat May  6 19:21:56 2023 by user: yujia \n\n\nStandardised Residuals Tests:\n                                Statistic p-Value    \n Jarque-Bera Test   R    Chi^2  13263.95  0          \n Shapiro-Wilk Test  R    W      0.885622  0          \n Ljung-Box Test     R    Q(10)  16.18024  0.09458772 \n Ljung-Box Test     R    Q(15)  21.00404  0.1367002  \n Ljung-Box Test     R    Q(20)  24.2514   0.2315831  \n Ljung-Box Test     R^2  Q(10)  13.31937  0.2063598  \n Ljung-Box Test     R^2  Q(15)  24.71042  0.05397016 \n Ljung-Box Test     R^2  Q(20)  38.0359   0.008766562\n LM Arch Test       R    TR^2   14.46658  0.2719102  \n\nInformation Criterion Statistics:\n      AIC       BIC       SIC      HQIC \n-6.058593 -6.051928 -6.058595 -6.056254 \n\n\n\n\nShow the code\ngarch.fit1 <- garchFit(~garch(1,1), data = returns, trace = F)\nsummary(garch.fit1)\n\n\n\nTitle:\n GARCH Modelling \n\nCall:\n garchFit(formula = ~garch(1, 1), data = returns, trace = F) \n\nMean and Variance Equation:\n data ~ garch(1, 1)\n<environment: 0x000001953e153bc0>\n [data = returns]\n\nConditional Distribution:\n norm \n\nCoefficient(s):\n        mu       omega      alpha1       beta1  \n3.5116e-04  3.2985e-06  5.0176e-02  9.3060e-01  \n\nStd. Errors:\n based on Hessian \n\nError Analysis:\n        Estimate  Std. Error  t value Pr(>|t|)    \nmu     3.512e-04   1.577e-04    2.226    0.026 *  \nomega  3.298e-06   7.474e-07    4.413 1.02e-05 ***\nalpha1 5.018e-02   6.646e-03    7.550 4.37e-14 ***\nbeta1  9.306e-01   9.880e-03   94.191  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nLog Likelihood:\n 14733.01    normalized:  3.02588 \n\nDescription:\n Sat May  6 19:21:56 2023 by user: yujia \n\n\nStandardised Residuals Tests:\n                                Statistic p-Value   \n Jarque-Bera Test   R    Chi^2  15608.8   0         \n Shapiro-Wilk Test  R    W      0.8834462 0         \n Ljung-Box Test     R    Q(10)  15.12868  0.1274403 \n Ljung-Box Test     R    Q(15)  19.86589  0.1771381 \n Ljung-Box Test     R    Q(20)  23.54299  0.2629252 \n Ljung-Box Test     R^2  Q(10)  14.20457  0.1638634 \n Ljung-Box Test     R^2  Q(15)  24.50026  0.05707071\n Ljung-Box Test     R^2  Q(20)  35.79802  0.0162434 \n LM Arch Test       R    TR^2   16.24371  0.1803268 \n\nInformation Criterion Statistics:\n      AIC       BIC       SIC      HQIC \n-6.050116 -6.044784 -6.050118 -6.048245 \n\n\n\n\nShow the code\ngarch.fit11<- garch(returns,order=c(1,1),trace=F)\ncheckresiduals(garch.fit11)\n\n\nWarning in modeldf.default(object): Could not find appropriate degrees of\nfreedom for this model.\n\n\n\n\n\nThere’s still correlation left.\n\nVolatality plot\n\n\nShow the code\nht <- garch.fit1@h.t #a numeric vector with the conditional variances (h.t = sigma.t^delta)\n\n\n\ndata= data.frame(ht,CVS_df$Date[-length(CVS_df$Date)])\nnames(data) <- c('ht','Date')\nggplot(data, aes(y = ht, x = Date)) + geom_line(col = '#ff9933') + ylab('Conditional Variance') + xlab('Date')+ggtitle(\"Volatality plot\")"
  },
  {
    "objectID": "ts.html#model-comparison",
    "href": "ts.html#model-comparison",
    "title": "Financial Time Series Models: CVS Example",
    "section": "Model Comparison",
    "text": "Model Comparison\nWe were looking at Method 1: ARIMA(0,1,0)+GARCH(1,1) and Method 2: GARCH(1,1) Perhaps Method 1 is better because Method 2 has correlation left."
  },
  {
    "objectID": "dl.html#deep-learning-codes",
    "href": "dl.html#deep-learning-codes",
    "title": "Deep Learning for TS",
    "section": "Deep Learning Codes",
    "text": "Deep Learning Codes\nPython Code Download."
  },
  {
    "objectID": "dl.html#rnn",
    "href": "dl.html#rnn",
    "title": "Deep Learning for TS",
    "section": "RNN",
    "text": "RNN\nA simple RNN is a type of recurrent neural network that can be used as a time series model. In a simple RNN, each time step takes an input vector and a hidden state vector, which is updated based on the previous hidden state and the current input. The hidden state serves as a memory for the model, allowing it to capture dependencies and patterns in the time series data.\nThe figures below shows the result of fitting AZN stock price into RNN model.\n\n\nThe model has: Train RMSE: 0.028; Test RMSE: 0.172\n\nRNN with L1 regularization\nThis section add both L1 and L2 regularization to simple RNN model to avoid overfitting. Generally, RNN with L2 regularization have lower error measurement comparing to the simple RNN model, while L1 regularization have higher error test RMSE than the simple RNN model.\n\n\nThe model has: Train RMSE 0.011; Test RMSE: 0.571\n\n\nRNN with L2 regularization\n\n\nThe model has: Training RMSE: 0.032; Testing RMSE: 0.047"
  },
  {
    "objectID": "dl.html#lstm",
    "href": "dl.html#lstm",
    "title": "Deep Learning for TS",
    "section": "LSTM",
    "text": "LSTM\nLSTM stands for Long Short-Term Memory, which is a type of recurrent neural network that is designed to address the issue of the vanishing gradient problem in simple RNNs when used for time series modeling. LSTM networks are particularly effective for modeling long-term dependencies and patterns in sequential data, such as time series data. In an LSTM network, each time step takes an input vector, a hidden state vector, and a cell state vector, which are updated based on the previous hidden state, cell state, and the current input. The cell state serves as a long-term memory for the model, allowing it to capture and remember important information about the time series data over long periods of time. The hidden state, on the other hand, serves as a short-term memory that allows the model to capture short-term patterns and dependencies in the data.\nThe figures below shows the result of fitting AZN stock price into LSTM model.\n\n\nThe model has: Train RMSE: 0.012; Test RMSE: 0.079"
  },
  {
    "objectID": "dl.html#gru",
    "href": "dl.html#gru",
    "title": "Deep Learning for TS",
    "section": "GRU",
    "text": "GRU\nGRU stands for Gated Recurrent Unit, which is a type of recurrent neural network that is designed to address the vanishing gradient problem in simple RNNs when used for time series modeling. GRU networks are similar to LSTM networks in that they use gates to control the flow of information in the network, but are computationally less expensive and have fewer parameters. In a GRU network, each time step takes an input vector and a hidden state vector, which are updated based on the previous hidden state and the current input. The GRU has two gates, a reset gate and an update gate, which control the flow of information into and out of the hidden state.\nThe figures below shows the result of fitting AZN stock price into GRU model.\n\n\nThe model has: Train RMSE: 0.008; Test RMSE: 0.078"
  },
  {
    "objectID": "dl.html#result-discussion",
    "href": "dl.html#result-discussion",
    "title": "Deep Learning for TS",
    "section": "Result Discussion",
    "text": "Result Discussion\n\nError Measurements of Deep Learning Models\n\n\nModel\nTrain RMSE\nTest RMSE\n\n\n\n\nRNN\n0.028\n0.172\n\n\nRNN L1\n0.011\n0.571\n\n\nRNN L2\n0.032\n0.047\n\n\nLSTM\n0.012\n0.079\n\n\nGRU\n0.008\n0.078\n\n\n\nOverall, GRU has the lowest test RMSE (0.078) among all three models, LSTM ranks the second (0.079), and simple RNN has the highest test RMSE (0.172). Besides, RNN with L2 regularization has the lowest test RMSE (0.047).\nIn general, deep learning models can make accurate predictions for horizons ranging from a few time steps to several hundred time steps, but the accuracy may decrease as the forecasting horizon increases."
  },
  {
    "objectID": "dl.html#deep-learning-vs-arima",
    "href": "dl.html#deep-learning-vs-arima",
    "title": "Deep Learning for TS",
    "section": "Deep Learning VS ARIMA",
    "text": "Deep Learning VS ARIMA\nHere is the comparison plots between the ARIMA Model and Deep Learning methods. GRU method was chosen here since it works best in all the deep learning methods. According to error measurement, deep learning methods (RMSE 0.008) slightly outperforms the ARIMA model (0.016). However, the ARIMA model is more straightforward than a neural network with far fewer parameters and easier to understand. It can be concluded that both method have shown that it is very likely to have an up-ward trend for AZN stock price in the near future. An ARIMA model is far easier to set up and should be considered, especially with its ability to be interpretable, but a neural network is an excellent alternative."
  },
  {
    "objectID": "arimax.html#arimax",
    "href": "arimax.html#arimax",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "ARIMAX",
    "text": "ARIMAX\nARIMAX model assumes that future values of a variable linearly depend on its past values, as well as on the values of past (stochastic) shocks. It is an extended version of the ARIMA model, with other independent (predictor) variables. The model is also referred to as the dynamic regression model. The X added to the end stands for “exogenous”. In other words, it suggests adding a separate different outside variable to help measure our endogenous variable.\nThe ‘exogenous’ variables added here are COVID-19 case numbers and COVID-19 vaccine rates. The number of COVID-19 cases in US can affect healthcare stock prices, e.g. UNH. Higher case numbers may result in increased demand for healthcare services, such as hospitalizations, treatments, and testing, which could positively impact the stock prices of healthcare companies involved in providing those services. Conversely, lower case numbers may lead to reduced demand for healthcare services, potentially resulting in lower stock prices for healthcare companies.\nVaccine rates, specifically the rate at which a population is vaccinated against COVID-19, can also impact healthcare stock prices. Higher vaccine rates are generally seen as positive for healthcare companies, as vaccines are considered a key tool in controlling the spread of the virus and reducing the severity of illness. Higher vaccine rates may lead to decreased demand for COVID-19 treatments and testing, but increased demand for vaccines and other preventive healthcare measures, which could positively impact the stock prices of healthcare companies involved in vaccine production or distribution, as well as other preventive healthcare services.\nIn this section, we choose the UNH stock price to fit ARIMAX model with COVID-19 case numbers and COVID-19 vaccine rates.\n\nStep 1: Data Prepare\n\n\nShow the code\ngetSymbols(\"UNH\", from=\"2020-12-16\", src=\"yahoo\")\n\n\n\n\nShow the code\nUNH_df <- as.data.frame(UNH)\nUNH_df$Date <- rownames(UNH_df)\nUNH_df <- UNH_df[c('Date','UNH.Adjusted')]\nUNH_df <- UNH_df %>%\n  mutate(Date = as.Date(Date)) %>%\n  complete(Date = seq.Date(min(Date), max(Date), by=\"day\"))\n\n# fill missing values in stock \nUNH_df <- UNH_df %>% fill(UNH.Adjusted)\n\nnew_dates <- seq(as.Date('2020-12-16'), as.Date('2023-3-21'),'week')\n\nUNH_df <- UNH_df[which((UNH_df$Date) %in% new_dates),]\n\nvaccine_df <- read.csv('data/vaccine_clean.csv')\n\nnew_dates <- seq(as.Date('2020-12-16'), as.Date('2023-3-22'),'week')\n\n#vaccine_df\nvaccine_df$Date <- as.Date(vaccine_df$Date)\nvaccine_df <- vaccine_df[which((vaccine_df$Date) %in% new_dates),]\n\n#covid_df\ncovid_df <- read.csv('data/covid.csv')\n\n#covid_ts <- ts(covid_df$Weekly.Cases, start = c(2020,1,29), frequency = 54)\ncovid_df$Date <- as.Date(covid_df$Date)\ncovid_df <- covid_df[covid_df$Date >= '2020-12-16'&covid_df$Date < '2023-03-22',]\n\n# combine all data, create dataframe\ndf <- data.frame(UNH_df, vaccine_df$total_doses, covid_df$Weekly.Cases)\ncolnames(df) <- c('Date', 'stock_price', 'vaccine_dose','covid_case')\n\nknitr::kable(head(df))\n\n\n\n\n\nDate\nstock_price\nvaccine_dose\ncovid_case\n\n\n\n\n2020-12-16\n329.2309\n160010\n1526464\n\n\n2020-12-23\n327.5330\n577895\n1499703\n\n\n2020-12-30\n334.7126\n848447\n1299023\n\n\n2021-01-06\n348.5672\n1029958\n1584212\n\n\n2021-01-13\n344.4632\n1334188\n1716354\n\n\n2021-01-20\n340.3883\n1614134\n1391546\n\n\n\n\n\n\n\nStep 2: Plotting the Data\n\n\nShow the code\ndf.ts<-ts(df,star=decimal_date(as.Date(\"2020-12-16\",format = \"%Y-%m-%d\")),frequency = 52)\n\nautoplot(df.ts[,c(2:4)], facets=TRUE) +\n  xlab(\"Date\") + ylab(\"\") +\n  ggtitle(\"Variables influencing UNH Stock Price in USA\")\n\n\n\n\n\nUNH stock price, Covid, Vaccine values\n\n\nStep 3: Fitting the model using ’auto.arima()`\nHere I’m using auto.arima() function to fit the ARIMAX model. Here we are trying to predict UNH stock price using COVID vaccine dose and COVID cases. All variables are time series and the exogenous variables in this case are vaccine_dose and covid_case.\n\n\nShow the code\nxreg <- cbind(Vac = df.ts[, \"vaccine_dose\"],\n              Imp = df.ts[, \"covid_case\"])\n\nfit <- auto.arima(df.ts[, \"stock_price\"], xreg = xreg)\nsummary(fit)\n\n\nSeries: df.ts[, \"stock_price\"] \nRegression with ARIMA(0,1,0) errors \n\nCoefficients:\n        Vac    Imp\n      0e+00  0e+00\ns.e.  1e-04  1e-04\n\nsigma^2 = 177.9:  log likelihood = -468.1\nAIC=942.2   AICc=942.41   BIC=950.49\n\nTraining set error measures:\n                   ME     RMSE      MAE      MPE     MAPE      MASE       ACF1\nTraining set 1.097616 13.16639 10.30662 0.233993 2.273228 0.1046469 0.04011524\n\n\n\n\nShow the code\ncheckresiduals(fit)\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Regression with ARIMA(0,1,0) errors\nQ* = 21.112, df = 24, p-value = 0.6321\n\nModel df: 0.   Total lags used: 24\n\n\nThis is an ARIMA model. This is a Regression model with ARIMA(0,1,0) errors.\n\n\nStep 4: Fitting the model manually\nHere we will first have to fit the linear regression model predicting stock price using Covid cases and vaccine doses.\nThen for the residuals, we will fit an ARIMA/SARIMA model.\n\n\nShow the code\ndf$stock_price <- ts(df$stock_price,star=decimal_date(as.Date(\"2020-12-16\",format = \"%Y-%m-%d\")),frequency = 52)\ndf$vaccine_dose <-ts(df$vaccine_dose,star=decimal_date(as.Date(\"2020-12-16\",format = \"%Y-%m-%d\")),frequency = 52)\ndf$covid_case<-ts(df$covid_case,star=decimal_date(as.Date(\"2020-12-16\",format = \"%Y-%m-%d\")),frequency = 52)\n\n######### First fit the linear model#######\nfit.reg <- lm(stock_price ~ vaccine_dose+covid_case, data=df)\nsummary(fit.reg)\n\n\n\nCall:\nlm(formula = stock_price ~ vaccine_dose + covid_case, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-154.24  -42.59    8.62   42.91   82.36 \n\nCoefficients:\n               Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   4.954e+02  7.932e+00  62.461  < 2e-16 ***\nvaccine_dose -4.341e-05  5.043e-06  -8.607 4.47e-14 ***\ncovid_case   -3.275e-06  5.345e-06  -0.613    0.541    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 52.02 on 115 degrees of freedom\nMultiple R-squared:  0.3944,    Adjusted R-squared:  0.3839 \nF-statistic: 37.45 on 2 and 115 DF,  p-value: 2.988e-13\n\n\n\n\nShow the code\nres.fit<-ts(residuals(fit.reg),star=decimal_date(as.Date(\"2020-12-16\",format = \"%Y-%m-%d\")),frequency = 52)\n\n########## Then look at the residuals ########\nacf(res.fit)\n\n\n\n\n\n\n\nShow the code\nPacf(res.fit)\n\n\n\n\n\n\n\nShow the code\nres.fit %>% diff() %>% ggtsdisplay()\n\n\n\n\n\n\n\nShow the code\nres.fit %>% diff() %>% diff(52) %>% ggtsdisplay()\n\n\n\n\n\nFinding the model parameters.\n\n\nShow the code\nd=1\ni=1\ntemp= data.frame()\nls=matrix(rep(NA,6*23),nrow=23) # roughly nrow = 3x4x2\n\n\nfor (p in 1:3)# p=1,2,\n{\n  for(q in 1:3)# q=1,2,\n  {\n    for(d in 0:1)# \n    {\n      \n      if(p-1+d+q-1<=8)\n      {\n        \n        model<- Arima(res.fit,order=c(p-1,d,q-1),include.drift=TRUE) \n        ls[i,]= c(p-1,d,q-1,model$aic,model$bic,model$aicc)\n        i=i+1\n        #print(i)\n        \n      }\n      \n    }\n  }\n}\n\ntemp= as.data.frame(ls)\nnames(temp)= c(\"p\",\"d\",\"q\",\"AIC\",\"BIC\",\"AICc\")\n\n#temp\nknitr::kable(temp)\n\n\n\n\n\np\nd\nq\nAIC\nBIC\nAICc\n\n\n\n\n0\n0\n0\n1217.4187\n1225.7307\n1217.6292\n\n\n0\n1\n0\n993.5902\n999.1146\n993.6955\n\n\n0\n0\n1\n1117.7725\n1128.8552\n1118.1264\n\n\n0\n1\n1\n993.1256\n1001.4121\n993.3380\n\n\n0\n0\n2\n1058.3969\n1072.2503\n1058.9326\n\n\n0\n1\n2\n994.2046\n1005.2533\n994.5617\n\n\n1\n0\n0\n1005.4074\n1016.4901\n1005.7614\n\n\n1\n1\n0\n992.7214\n1001.0079\n992.9338\n\n\n1\n0\n1\n1003.8974\n1017.7508\n1004.4331\n\n\n1\n1\n1\n994.2405\n1005.2892\n994.5976\n\n\n1\n0\n2\n1003.9563\n1020.5804\n1004.7130\n\n\n1\n1\n2\n995.7980\n1009.6089\n996.3386\n\n\n2\n0\n0\n1002.8800\n1016.7334\n1003.4157\n\n\n2\n1\n0\n994.1790\n1005.2277\n994.5362\n\n\n2\n0\n1\n999.6163\n1016.2404\n1000.3730\n\n\n2\n1\n1\n995.8971\n1009.7080\n996.4376\n\n\n2\n0\n2\n1005.7699\n1025.1647\n1006.7881\n\n\n2\n1\n2\n995.6381\n1012.2111\n996.4017\n\n\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\n\n\n\n\nShow the code\nprint('Minimum AIC')\n\n\n[1] \"Minimum AIC\"\n\n\nShow the code\ntemp[which.min(temp$AIC),] \n\n\n  p d q      AIC      BIC     AICc\n8 1 1 0 992.7214 1001.008 992.9338\n\n\n\n\nShow the code\nprint('Minimum BIC')\n\n\n[1] \"Minimum BIC\"\n\n\nShow the code\ntemp[which.min(temp$BIC),] \n\n\n  p d q      AIC      BIC     AICc\n2 0 1 0 993.5902 999.1146 993.6955\n\n\n\n\nShow the code\nprint('Minimum AICc')\n\n\n[1] \"Minimum AICc\"\n\n\nShow the code\ntemp[which.min(temp$AICc),] \n\n\n  p d q      AIC      BIC     AICc\n8 1 1 0 992.7214 1001.008 992.9338\n\n\n\n\nShow the code\nset.seed(1234)\n\nmodel_output12 <- capture.output(sarima(res.fit, 0,1,0)) \n\n\n\n\n\n\n\nShow the code\ncat(model_output12[9:38], model_output12[length(model_output12)], sep = \"\\n\")\n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n      constant\n        1.0885\ns.e.    1.5357\n\nsigma^2 estimated as 275.9:  log likelihood = -494.8,  aic = 993.59\n\n$degrees_of_freedom\n[1] 116\n\n$ttable\n         Estimate     SE t.value p.value\nconstant   1.0885 1.5357  0.7088  0.4799\n\n$AIC\n[1] 8.492224\n\n$AICc\n[1] 8.492522\n\n$BIC\n[1] 8.539441\n\n\n\n\nShow the code\nset.seed(1234)\n\nmodel_output13 <- capture.output(sarima(res.fit, 1,1,0)) \n\n\n\n\n\n\n\nShow the code\ncat(model_output13[16:46], model_output13[length(model_output13)], sep = \"\\n\")\n\n\n$fit\n\nCall:\narima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, Q), period = S), \n    xreg = constant, transform.pars = trans, fixed = fixed, optim.control = list(trace = trc, \n        REPORT = 1, reltol = tol))\n\nCoefficients:\n         ar1  constant\n      0.1556    1.1047\ns.e.  0.0913    1.7935\n\nsigma^2 estimated as 269.2:  log likelihood = -493.36,  aic = 992.72\n\n$degrees_of_freedom\n[1] 115\n\n$ttable\n         Estimate     SE t.value p.value\nar1        0.1556 0.0913  1.7049  0.0909\nconstant   1.1047 1.7935  0.6160  0.5391\n\n$AIC\n[1] 8.484798\n\n$AICc\n[1] 8.485698\n\n$BIC\n[1] 8.555623\n\n\nARIMA(0,1,0) and ARIMA(1,1,0) both look okay.\n\n\nStep 5: Using Cross Validation\n\n\nShow the code\nk <- 36 # minimum data length for fitting a model \nn <- length(res.fit)\nn-k # rest of the observations\n\n\n[1] 82\n\n\n\n\nShow the code\ni=1\nerr1 = c()\nerr2 = c()\n\nrmse1 <- c()\nrmse2 <- c()\n\nfor(i in 1:(n-k))\n{\n  xtrain <- res.fit[1:(k-1)+i] #observations from 1 to 75\n  xtest <- res.fit[k+i] #76th observation as the test set\n  \n  fit <- Arima(xtrain, order=c(1,1,0),include.drift=FALSE, method=\"ML\")\n  fcast1 <- forecast(fit, h=1)\n  \n  fit2 <- Arima(xtrain, order=c(0,1,0),include.drift=FALSE, method=\"ML\")\n  fcast2 <- forecast(fit2, h=1)\n  \n  #capture error for each iteration\n  # This is mean absolute error\n  err1 = c(err1, abs(fcast1$mean-xtest)) \n  err2 = c(err2, abs(fcast2$mean-xtest))\n  \n  # This is mean squared error\n  err3 = c(err1, (fcast1$mean-xtest)^2)\n  err4 = c(err2, (fcast2$mean-xtest)^2)\n  \n  rmse1 <- c(rmse1, sqrt((fcast1$mean-xtest)^2))\n  rmse2 <- c(rmse2, sqrt((fcast2$mean-xtest)^2))\n  \n}\n\n(MAE1=mean(err1)) # This is mean absolute error\n\n\n[1] 13.45554\n\n\n\n\nShow the code\n(MAE2=mean(err2)) #has slightly higher error\n\n\n[1] 13.46804\n\n\n\n\nShow the code\nMSE1=mean(err1) #fit 1,1,0\nMSE2=mean(err2)#fit 0,1,0\n\nMSE1\n\n\n[1] 13.45554\n\n\n\n\nShow the code\nMSE2\n\n\n[1] 13.46804\n\n\n\n\nShow the code\nrmse_df <- data.frame(rmse1,rmse2)\nrmse_df$x <- as.numeric(rownames(rmse_df))\n\nplot(rmse_df$x, rmse_df$rmse1, type = 'l', col=2, xlab=\"horizon\", ylab=\"RMSE\")\nlines(rmse_df$x, rmse_df$rmse2, type=\"l\",col=3)\nlegend(\"topleft\",legend=c(\"fit1\",\"fit2\"),col=2:4,lty=1)\n\n\n\n\n\n\n\nStep 6: forcasting\n\n\nShow the code\nvac_fit<-auto.arima(df$vaccine_dose) #fiting an ARIMA model to the vaccine_dose variable\nsummary(vac_fit)\n\n\nSeries: df$vaccine_dose \nARIMA(1,1,1) \n\nCoefficients:\n         ar1      ma1\n      0.8341  -0.6298\ns.e.  0.0916   0.1128\n\nsigma^2 = 4.824e+10:  log likelihood = -1604.18\nAIC=3214.36   AICc=3214.57   BIC=3222.65\n\nTraining set error measures:\n                    ME     RMSE      MAE          MPE     MAPE      MASE\nTraining set -1592.486 216829.1 132002.8 0.0009291879 13.08388 0.1149613\n                    ACF1\nTraining set -0.06829849\n\n\n\n\nShow the code\nfvac<-forecast(vac_fit)\n\ncovid_fit<-auto.arima(df$covid_case) #fiting an ARIMA model to the covid_case variable\nsummary(covid_fit)\n\n\nSeries: df$covid_case \nARIMA(2,0,2) with non-zero mean \n\nCoefficients:\n         ar1      ar2     ma1     ma2      mean\n      1.3306  -0.5128  0.8165  0.4848  733469.2\ns.e.  0.1153   0.1117  0.1363  0.1076  189902.4\n\nsigma^2 = 2.904e+10:  log likelihood = -1588.94\nAIC=3189.87   AICc=3190.63   BIC=3206.5\n\nTraining set error measures:\n                    ME     RMSE      MAE       MPE     MAPE      MASE\nTraining set -424.0988 166757.6 99191.78 -6.959882 17.02194 0.1095754\n                    ACF1\nTraining set -0.00122146\n\n\n\n\nShow the code\nfcov<-forecast(covid_fit)\n\nfxreg <- cbind(Vac = fvac$mean,\n              Cov = fcov$mean)\n\nfcast <- forecast(fit, xreg=fxreg) #fimp$mean gives the forecasted values\n\n\nWarning in forecast.forecast_ARIMA(fit, xreg = fxreg): xreg not required by this\nmodel, ignoring the provided regressors\n\n\nShow the code\nautoplot(fcast) + xlab(\"Date\") +\n  ylab(\"Price\")\n\n\n\n\n\n\n\nDiscussion\nBased on the result above, the number of daily COVID-19 vaccination number is not significant on predicting the UNH stock price, while the number of COVID-19 cases is. Though the number of COVID-19 vaccination number is not significant here, it does not mean that it does not affect healthcare stock price. While UNH is a healthcare company that is involved in health insurance, healthcare services, and technology solutions, it is not directly involved in COVID-19 vaccine development or production. Therefore, the number of COVID-19 vaccine doses administered may not have a direct impact on UNH’s operations or revenue."
  },
  {
    "objectID": "arimax.html#var",
    "href": "arimax.html#var",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "VAR",
    "text": "VAR\nVAR models (vector autoregressive models) are used for multivariate time series. The structure is that each variable is a linear function of past lags of itself and past lags of the other variables. A Vector autoregressive (VAR) model is useful when one is interested in predicting multiple time series variables using a single model.\nThe variables we are interested here are GDP and Unemployment rate in US. The overall health of the economy, as reflected by the GDP, can influence healthcare stock prices. A strong GDP generally indicates a healthy economy with higher levels of consumer spending, business investment, and economic growth. In such an environment, healthcare companies may experience increased demand for their products and services, which could positively impact their stock prices. Conversely, a weak GDP may signal an economic slowdown or recession, which could result in reduced demand for healthcare products and services, potentially leading to lower stock prices for healthcare companies. The unemployment rate, which reflects the percentage of the labor force that is unemployed, can also affect healthcare stock prices. A low unemployment rate is generally indicative of a strong labor market, with more people employed and potentially having access to employer-sponsored healthcare benefits. This may result in increased demand for healthcare services, positively impacting healthcare stock prices. On the other hand, a high unemployment rate may indicate a weak labor market, with more people losing their jobs and potentially losing access to healthcare benefits, leading to reduced demand for healthcare services and potentially lower stock prices for healthcare companies.\nIn this section, I’m fitting a VAR model to find multivariate relationship between the series UNH stock price, GDP, and Employment.\n\nStep 1: Data Preparing\n\n\nShow the code\ngetSymbols(\"UNH\", from=\"1989-12-29\", src=\"yahoo\")\n\n\n[1] \"UNH\"\n\n\nShow the code\nUNH_df <- as.data.frame(UNH)\nUNH_df$Date <- rownames(UNH_df)\nUNH_df <- UNH_df[c('Date','UNH.Adjusted')]\nUNH_df <- UNH_df %>%\n  mutate(Date = as.Date(Date)) %>%\n  complete(Date = seq.Date(min(Date), max(Date), by=\"day\"))\n\n# fill missing values in stock \nUNH_df <- UNH_df %>% fill(UNH.Adjusted)\n#UNH_df\nnew_dates <- seq(as.Date('1990-01-01'), as.Date('2022-10-01'),'quarter')\n#new_dates\nUNH_df <- UNH_df[which((UNH_df$Date) %in% new_dates),]\n\ngdp <- read.csv('data/GDP59.CSV')\ngdp$DATE <- as.Date(gdp$DATE)\ngdp <- gdp[which((gdp$DATE) %in% new_dates),]\n\nemp <- read.csv('data/Employment59.CSV')\nemp$DATE <- as.Date(emp$DATE)\nemp <- emp[which((emp$DATE) %in% new_dates),]\n\n\n\n\nShow the code\ndd <- data.frame(UNH_df,gdp,emp)\ndd <- dd[,c(1,2,4,6)]\ncolnames(dd) <- c('DATE', 'stock_price','GDP','Employment')\nknitr::kable(head(dd))\n\n\n\n\n\n\nDATE\nstock_price\nGDP\nEmployment\n\n\n\n\n125\n1990-01-01\n0.308110\n5872.701\n109418.7\n\n\n126\n1990-04-01\n0.247759\n5960.028\n109786.7\n\n\n127\n1990-07-01\n0.438343\n6015.116\n109651.0\n\n\n128\n1990-10-01\n0.435356\n6004.733\n109255.0\n\n\n129\n1991-01-01\n0.591069\n6035.178\n108783.0\n\n\n130\n1991-04-01\n0.985114\n6126.862\n108312.3\n\n\n\n\n\n\n\nStep 2: Plotting the data\n\n\nShow the code\ndd.ts<-ts(dd,star=decimal_date(as.Date(\"1990-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\n\nautoplot(dd.ts[,c(2:4)], facets=TRUE) +\n  xlab(\"Year\") + ylab(\"\") +\n  ggtitle(\"UNH Stock Price, GDP and Employment in USA\")\n\n\n\n\n\n\n\nStep 3: Fitting a VAR model\n\n\nShow the code\nVARselect(dd[, c(2:4)], lag.max=10, type=\"both\")\n\n\n$selection\nAIC(n)  HQ(n)  SC(n) FPE(n) \n    10      9      9      9 \n\n$criteria\n                  1            2            3            4            5\nAIC(n) 2.941404e+01 2.865601e+01 2.849637e+01 2.814189e+01 2.812288e+01\nHQ(n)  2.955407e+01 2.888005e+01 2.880444e+01 2.853398e+01 2.859898e+01\nSC(n)  2.975880e+01 2.920762e+01 2.925484e+01 2.910721e+01 2.929506e+01\nFPE(n) 5.948597e+12 2.788633e+12 2.379347e+12 1.671830e+12 1.644336e+12\n                  6            7            8            9           10\nAIC(n) 2.758294e+01 2.723307e+01 2.696700e+01 2.669062e+01 2.668670e+01\nHQ(n)  2.814305e+01 2.787720e+01 2.769516e+01 2.750279e+01 2.758289e+01\nSC(n)  2.896196e+01 2.881895e+01 2.875974e+01 2.869021e+01 2.889314e+01\nFPE(n) 9.616264e+11 6.809549e+11 5.251270e+11 4.014819e+11 4.038690e+11\n\n\nIt’s clear that according to selection criteria p=10 and 9 are good.\nI’m fitting several models with p=1(for simplicity), 5, and 9.=> VAR(1), VAR(5), VAR(9)\n\n\nShow the code\nsummary(VAR(dd[, c(2:4)], p=1, type='both'))\n\n\n\nVAR Estimation Results:\n========================= \nEndogenous variables: stock_price, GDP, Employment \nDeterministic variables: both \nSample size: 131 \nLog Likelihood: -2461.36 \nRoots of the characteristic polynomial:\n1.028 0.9422  0.78\nCall:\nVAR(y = dd[, c(2:4)], p = 1, type = \"both\")\n\n\nEstimation results for equation stock_price: \n============================================ \nstock_price = stock_price.l1 + GDP.l1 + Employment.l1 + const + trend \n\n                 Estimate Std. Error t value Pr(>|t|)    \nstock_price.l1  1.0341243  0.0394781  26.195   <2e-16 ***\nGDP.l1         -0.0005115  0.0039797  -0.129   0.8979    \nEmployment.l1  -0.0006163  0.0003038  -2.029   0.0446 *  \nconst          69.2145818 34.7475964   1.992   0.0485 *  \ntrend           0.3006699  0.4555344   0.660   0.5104    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 14.62 on 126 degrees of freedom\nMultiple R-Squared: 0.9849, Adjusted R-squared: 0.9844 \nF-statistic:  2057 on 4 and 126 DF,  p-value: < 2.2e-16 \n\n\nEstimation results for equation GDP: \n==================================== \nGDP = stock_price.l1 + GDP.l1 + Employment.l1 + const + trend \n\n                 Estimate Std. Error t value Pr(>|t|)    \nstock_price.l1    2.55503    0.64584   3.956 0.000126 ***\nGDP.l1            0.79020    0.06511  12.137  < 2e-16 ***\nEmployment.l1    -0.01069    0.00497  -2.151 0.033349 *  \nconst          2355.16290  568.45522   4.143 6.24e-05 ***\ntrend            27.89290    7.45234   3.743 0.000275 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 239.2 on 126 degrees of freedom\nMultiple R-Squared: 0.998,  Adjusted R-squared: 0.998 \nF-statistic: 1.607e+04 on 4 and 126 DF,  p-value: < 2.2e-16 \n\n\nEstimation results for equation Employment: \n=========================================== \nEmployment = stock_price.l1 + GDP.l1 + Employment.l1 + const + trend \n\n                 Estimate Std. Error t value Pr(>|t|)    \nstock_price.l1  6.183e+00  4.862e+00   1.272  0.20584    \nGDP.l1         -5.610e-01  4.901e-01  -1.145  0.25458    \nEmployment.l1   9.253e-01  3.741e-02  24.733  < 2e-16 ***\nconst           1.161e+04  4.279e+03   2.712  0.00762 ** \ntrend           8.544e+01  5.610e+01   1.523  0.13028    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 1801 on 126 degrees of freedom\nMultiple R-Squared: 0.9786, Adjusted R-squared: 0.9779 \nF-statistic:  1441 on 4 and 126 DF,  p-value: < 2.2e-16 \n\n\n\nCovariance matrix of residuals:\n            stock_price    GDP Employment\nstock_price       213.8   1386      12148\nGDP              1385.8  57224     396720\nEmployment      12147.6 396720    3242998\n\nCorrelation matrix of residuals:\n            stock_price    GDP Employment\nstock_price      1.0000 0.3962     0.4613\nGDP              0.3962 1.0000     0.9209\nEmployment       0.4613 0.9209     1.0000\n\n\n\n\nShow the code\nsummary(VAR(dd[, c(2:4)], p=5, type='both'))\n\n\n\nVAR Estimation Results:\n========================= \nEndogenous variables: stock_price, GDP, Employment \nDeterministic variables: both \nSample size: 127 \nLog Likelihood: -2267.863 \nRoots of the characteristic polynomial:\n1.054 0.9435 0.9435 0.8781 0.8781 0.7952 0.7952 0.7806 0.7806 0.7709 0.7709 0.6959 0.4107 0.4107 0.3355\nCall:\nVAR(y = dd[, c(2:4)], p = 5, type = \"both\")\n\n\nEstimation results for equation stock_price: \n============================================ \nstock_price = stock_price.l1 + GDP.l1 + Employment.l1 + stock_price.l2 + GDP.l2 + Employment.l2 + stock_price.l3 + GDP.l3 + Employment.l3 + stock_price.l4 + GDP.l4 + Employment.l4 + stock_price.l5 + GDP.l5 + Employment.l5 + const + trend \n\n                 Estimate Std. Error t value Pr(>|t|)    \nstock_price.l1  5.454e-01  1.048e-01   5.204 9.13e-07 ***\nGDP.l1          6.007e-02  1.789e-02   3.358  0.00108 ** \nEmployment.l1  -7.709e-03  2.385e-03  -3.232  0.00162 ** \nstock_price.l2  2.323e-01  1.468e-01   1.583  0.11639    \nGDP.l2         -6.188e-02  2.695e-02  -2.297  0.02353 *  \nEmployment.l2   8.708e-03  4.117e-03   2.115  0.03668 *  \nstock_price.l3 -6.701e-02  1.636e-01  -0.410  0.68293    \nGDP.l3          1.256e-02  2.614e-02   0.480  0.63198    \nEmployment.l3  -3.778e-03  4.122e-03  -0.917  0.36136    \nstock_price.l4  2.159e-01  1.796e-01   1.202  0.23192    \nGDP.l4         -1.114e-02  2.275e-02  -0.490  0.62525    \nEmployment.l4   2.759e-03  3.733e-03   0.739  0.46146    \nstock_price.l5  2.480e-01  1.956e-01   1.268  0.20751    \nGDP.l5         -1.060e-02  1.379e-02  -0.769  0.44375    \nEmployment.l5  -6.394e-04  2.197e-03  -0.291  0.77155    \nconst           1.189e+02  3.723e+01   3.194  0.00183 ** \ntrend           1.493e+00  5.610e-01   2.661  0.00897 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 12.06 on 110 degrees of freedom\nMultiple R-Squared: 0.9909, Adjusted R-squared: 0.9896 \nF-statistic: 750.5 on 16 and 110 DF,  p-value: < 2.2e-16 \n\n\nEstimation results for equation GDP: \n==================================== \nGDP = stock_price.l1 + GDP.l1 + Employment.l1 + stock_price.l2 + GDP.l2 + Employment.l2 + stock_price.l3 + GDP.l3 + Employment.l3 + stock_price.l4 + GDP.l4 + Employment.l4 + stock_price.l5 + GDP.l5 + Employment.l5 + const + trend \n\n                 Estimate Std. Error t value Pr(>|t|)    \nstock_price.l1   -1.30445    1.54245  -0.846 0.399555    \nGDP.l1            1.92765    0.26327   7.322 4.28e-11 ***\nEmployment.l1    -0.18525    0.03510  -5.277 6.65e-07 ***\nstock_price.l2    7.85075    2.15990   3.635 0.000425 ***\nGDP.l2           -0.49800    0.39656  -1.256 0.211852    \nEmployment.l2     0.14315    0.06059   2.363 0.019907 *  \nstock_price.l3    1.97665    2.40806   0.821 0.413510    \nGDP.l3           -0.68441    0.38474  -1.779 0.078023 .  \nEmployment.l3     0.07846    0.06066   1.293 0.198571    \nstock_price.l4   -3.50097    2.64351  -1.324 0.188127    \nGDP.l4           -0.53338    0.33481  -1.593 0.114012    \nEmployment.l4     0.03338    0.05493   0.608 0.544643    \nstock_price.l5   -4.52830    2.87918  -1.573 0.118643    \nGDP.l5            0.62554    0.20300   3.081 0.002603 ** \nEmployment.l5    -0.07633    0.03233  -2.361 0.019978 *  \nconst          1593.63764  547.91857   2.909 0.004394 ** \ntrend            21.22796    8.25642   2.571 0.011473 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 177.5 on 110 degrees of freedom\nMultiple R-Squared: 0.999,  Adjusted R-squared: 0.9988 \nF-statistic:  6815 on 16 and 110 DF,  p-value: < 2.2e-16 \n\n\nEstimation results for equation Employment: \n=========================================== \nEmployment = stock_price.l1 + GDP.l1 + Employment.l1 + stock_price.l2 + GDP.l2 + Employment.l2 + stock_price.l3 + GDP.l3 + Employment.l3 + stock_price.l4 + GDP.l4 + Employment.l4 + stock_price.l5 + GDP.l5 + Employment.l5 + const + trend \n\n                 Estimate Std. Error t value Pr(>|t|)    \nstock_price.l1 -3.786e+01  1.190e+01  -3.182  0.00190 ** \nGDP.l1          6.787e+00  2.030e+00   3.343  0.00113 ** \nEmployment.l1  -1.385e-02  2.707e-01  -0.051  0.95928    \nstock_price.l2  7.770e+01  1.666e+01   4.664 8.77e-06 ***\nGDP.l2         -2.705e+00  3.058e+00  -0.885  0.37831    \nEmployment.l2   6.207e-01  4.673e-01   1.328  0.18686    \nstock_price.l3  4.014e+00  1.857e+01   0.216  0.82927    \nGDP.l3         -8.103e+00  2.967e+00  -2.731  0.00736 ** \nEmployment.l3   1.098e+00  4.678e-01   2.346  0.02075 *  \nstock_price.l4 -1.641e+01  2.039e+01  -0.805  0.42269    \nGDP.l4         -1.499e+00  2.582e+00  -0.581  0.56270    \nEmployment.l4  -1.900e-01  4.237e-01  -0.448  0.65470    \nstock_price.l5 -2.700e+01  2.221e+01  -1.216  0.22657    \nGDP.l5          4.674e+00  1.566e+00   2.985  0.00349 ** \nEmployment.l5  -5.865e-01  2.493e-01  -2.352  0.02044 *  \nconst           1.222e+04  4.226e+03   2.891  0.00462 ** \ntrend           1.203e+02  6.368e+01   1.890  0.06146 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 1369 on 110 degrees of freedom\nMultiple R-Squared: 0.9879, Adjusted R-squared: 0.9861 \nF-statistic: 560.4 on 16 and 110 DF,  p-value: < 2.2e-16 \n\n\n\nCovariance matrix of residuals:\n            stock_price      GDP Employment\nstock_price       145.5    455.4       5503\nGDP               455.4  31506.6     225145\nEmployment       5503.3 225144.8    1874014\n\nCorrelation matrix of residuals:\n            stock_price    GDP Employment\nstock_price      1.0000 0.2127     0.3333\nGDP              0.2127 1.0000     0.9266\nEmployment       0.3333 0.9266     1.0000\n\n\n\n\nShow the code\nsummary(VAR(dd[, c(2:4)], p=9, type='both'))\n\n\n\nVAR Estimation Results:\n========================= \nEndogenous variables: stock_price, GDP, Employment \nDeterministic variables: both \nSample size: 123 \nLog Likelihood: -2076.371 \nRoots of the characteristic polynomial:\n1.058 1.051 1.018 1.018 1.012 1.012 1.012 1.012 1.005 1.005 1.003 1.003 0.9793 0.9793 0.9726 0.9726 0.8256 0.8256 0.8098 0.8098 0.7926 0.7926 0.6662 0.6662 0.4868 0.4868 0.0237\nCall:\nVAR(y = dd[, c(2:4)], p = 9, type = \"both\")\n\n\nEstimation results for equation stock_price: \n============================================ \nstock_price = stock_price.l1 + GDP.l1 + Employment.l1 + stock_price.l2 + GDP.l2 + Employment.l2 + stock_price.l3 + GDP.l3 + Employment.l3 + stock_price.l4 + GDP.l4 + Employment.l4 + stock_price.l5 + GDP.l5 + Employment.l5 + stock_price.l6 + GDP.l6 + Employment.l6 + stock_price.l7 + GDP.l7 + Employment.l7 + stock_price.l8 + GDP.l8 + Employment.l8 + stock_price.l9 + GDP.l9 + Employment.l9 + const + trend \n\n                 Estimate Std. Error t value Pr(>|t|)    \nstock_price.l1  0.6177986  0.1140057   5.419 4.61e-07 ***\nGDP.l1          0.0415175  0.0139527   2.976 0.003717 ** \nEmployment.l1  -0.0069645  0.0020733  -3.359 0.001131 ** \nstock_price.l2  0.3603716  0.1652332   2.181 0.031680 *  \nGDP.l2         -0.0157162  0.0211664  -0.743 0.459633    \nEmployment.l2   0.0048987  0.0033424   1.466 0.146093    \nstock_price.l3  0.0177119  0.1607489   0.110 0.912499    \nGDP.l3         -0.0009781  0.0212811  -0.046 0.963439    \nEmployment.l3  -0.0018913  0.0034992  -0.540 0.590135    \nstock_price.l4 -0.1568989  0.1711382  -0.917 0.361596    \nGDP.l4         -0.0352143  0.0218848  -1.609 0.110953    \nEmployment.l4   0.0072039  0.0035652   2.021 0.046168 *  \nstock_price.l5  0.3383735  0.2044761   1.655 0.101294    \nGDP.l5         -0.0156536  0.0222246  -0.704 0.482964    \nEmployment.l5   0.0016545  0.0036673   0.451 0.652934    \nstock_price.l6  0.1416245  0.2112799   0.670 0.504299    \nGDP.l6         -0.0184066  0.0221573  -0.831 0.408236    \nEmployment.l6  -0.0012687  0.0036506  -0.348 0.728965    \nstock_price.l7 -0.9691876  0.2444899  -3.964 0.000144 ***\nGDP.l7          0.0190303  0.0219980   0.865 0.389190    \nEmployment.l7  -0.0017353  0.0035712  -0.486 0.628163    \nstock_price.l8  1.0643895  0.2599676   4.094 8.96e-05 ***\nGDP.l8          0.0123808  0.0212167   0.584 0.560927    \nEmployment.l8  -0.0058935  0.0033845  -1.741 0.084893 .  \nstock_price.l9 -0.2681893  0.2116088  -1.267 0.208149    \nGDP.l9          0.0063637  0.0130395   0.488 0.626663    \nEmployment.l9   0.0035409  0.0021429   1.652 0.101786    \nconst          79.8647702 39.9369777   2.000 0.048411 *  \ntrend           0.9524397  0.5617195   1.696 0.093276 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 8.687 on 94 degrees of freedom\nMultiple R-Squared: 0.9959, Adjusted R-squared: 0.9947 \nF-statistic:   819 on 28 and 94 DF,  p-value: < 2.2e-16 \n\n\nEstimation results for equation GDP: \n==================================== \nGDP = stock_price.l1 + GDP.l1 + Employment.l1 + stock_price.l2 + GDP.l2 + Employment.l2 + stock_price.l3 + GDP.l3 + Employment.l3 + stock_price.l4 + GDP.l4 + Employment.l4 + stock_price.l5 + GDP.l5 + Employment.l5 + stock_price.l6 + GDP.l6 + Employment.l6 + stock_price.l7 + GDP.l7 + Employment.l7 + stock_price.l8 + GDP.l8 + Employment.l8 + stock_price.l9 + GDP.l9 + Employment.l9 + const + trend \n\n                 Estimate Std. Error t value Pr(>|t|)    \nstock_price.l1 -5.605e+00  1.777e+00  -3.154  0.00217 ** \nGDP.l1          1.818e+00  2.175e-01   8.360 5.58e-13 ***\nEmployment.l1  -1.497e-01  3.232e-02  -4.631 1.17e-05 ***\nstock_price.l2  1.352e+01  2.576e+00   5.250 9.43e-07 ***\nGDP.l2         -7.098e-01  3.300e-01  -2.151  0.03403 *  \nEmployment.l2   1.262e-01  5.211e-02   2.422  0.01734 *  \nstock_price.l3 -1.116e+00  2.506e+00  -0.445  0.65722    \nGDP.l3         -2.534e-01  3.318e-01  -0.764  0.44683    \nEmployment.l3   3.585e-02  5.455e-02   0.657  0.51262    \nstock_price.l4  1.673e+00  2.668e+00   0.627  0.53208    \nGDP.l4         -8.506e-02  3.412e-01  -0.249  0.80365    \nEmployment.l4   1.384e-02  5.558e-02   0.249  0.80384    \nstock_price.l5 -7.495e-01  3.188e+00  -0.235  0.81462    \nGDP.l5          1.171e-01  3.465e-01   0.338  0.73604    \nEmployment.l5  -2.497e-02  5.717e-02  -0.437  0.66327    \nstock_price.l6 -1.389e+01  3.294e+00  -4.216 5.71e-05 ***\nGDP.l6          2.705e-02  3.454e-01   0.078  0.93775    \nEmployment.l6  -9.452e-03  5.691e-02  -0.166  0.86846    \nstock_price.l7 -1.613e+00  3.812e+00  -0.423  0.67319    \nGDP.l7         -2.916e-01  3.429e-01  -0.850  0.39727    \nEmployment.l7   7.971e-02  5.567e-02   1.432  0.15551    \nstock_price.l8  7.701e+00  4.053e+00   1.900  0.06049 .  \nGDP.l8          9.947e-02  3.308e-01   0.301  0.76428    \nEmployment.l8  -1.132e-01  5.276e-02  -2.144  0.03457 *  \nstock_price.l9  1.073e+00  3.299e+00   0.325  0.74571    \nGDP.l9          1.156e-01  2.033e-01   0.569  0.57100    \nEmployment.l9   3.477e-02  3.341e-02   1.041  0.30064    \nconst           1.603e+03  6.226e+02   2.575  0.01159 *  \ntrend           2.134e+01  8.757e+00   2.437  0.01669 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 135.4 on 94 degrees of freedom\nMultiple R-Squared: 0.9995, Adjusted R-squared: 0.9993 \nF-statistic:  6211 on 28 and 94 DF,  p-value: < 2.2e-16 \n\n\nEstimation results for equation Employment: \n=========================================== \nEmployment = stock_price.l1 + GDP.l1 + Employment.l1 + stock_price.l2 + GDP.l2 + Employment.l2 + stock_price.l3 + GDP.l3 + Employment.l3 + stock_price.l4 + GDP.l4 + Employment.l4 + stock_price.l5 + GDP.l5 + Employment.l5 + stock_price.l6 + GDP.l6 + Employment.l6 + stock_price.l7 + GDP.l7 + Employment.l7 + stock_price.l8 + GDP.l8 + Employment.l8 + stock_price.l9 + GDP.l9 + Employment.l9 + const + trend \n\n                 Estimate Std. Error t value Pr(>|t|)    \nstock_price.l1 -6.801e+01  1.216e+01  -5.595 2.17e-07 ***\nGDP.l1          6.439e+00  1.488e+00   4.328 3.75e-05 ***\nEmployment.l1   1.372e-01  2.211e-01   0.621  0.53643    \nstock_price.l2  1.137e+02  1.762e+01   6.453 4.74e-09 ***\nGDP.l2         -4.116e+00  2.257e+00  -1.824  0.07136 .  \nEmployment.l2   5.335e-01  3.564e-01   1.497  0.13778    \nstock_price.l3 -4.159e+00  1.714e+01  -0.243  0.80882    \nGDP.l3         -3.838e+00  2.269e+00  -1.691  0.09408 .  \nEmployment.l3   5.003e-01  3.731e-01   1.341  0.18318    \nstock_price.l4  1.066e+01  1.825e+01   0.584  0.56038    \nGDP.l4         -6.501e-02  2.334e+00  -0.028  0.97783    \nEmployment.l4   1.168e-01  3.802e-01   0.307  0.75925    \nstock_price.l5  2.259e+01  2.180e+01   1.036  0.30277    \nGDP.l5          8.889e-01  2.370e+00   0.375  0.70845    \nEmployment.l5  -2.821e-01  3.911e-01  -0.721  0.47245    \nstock_price.l6 -1.130e+02  2.253e+01  -5.017 2.48e-06 ***\nGDP.l6         -1.567e+00  2.363e+00  -0.663  0.50891    \nEmployment.l6   2.161e-01  3.893e-01   0.555  0.58019    \nstock_price.l7 -2.070e+01  2.607e+01  -0.794  0.42930    \nGDP.l7         -1.096e+00  2.346e+00  -0.467  0.64133    \nEmployment.l7   4.753e-01  3.808e-01   1.248  0.21512    \nstock_price.l8  8.864e+01  2.772e+01   3.198  0.00189 ** \nGDP.l8          1.933e+00  2.262e+00   0.854  0.39515    \nEmployment.l8  -1.204e+00  3.609e-01  -3.336  0.00122 ** \nstock_price.l9 -3.272e+01  2.256e+01  -1.450  0.15042    \nGDP.l9          8.076e-01  1.390e+00   0.581  0.56273    \nEmployment.l9   4.222e-01  2.285e-01   1.848  0.06777 .  \nconst           1.262e+04  4.259e+03   2.963  0.00386 ** \ntrend           9.703e+01  5.990e+01   1.620  0.10859    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResidual standard error: 926.4 on 94 degrees of freedom\nMultiple R-Squared: 0.9945, Adjusted R-squared: 0.9928 \nF-statistic: 603.2 on 28 and 94 DF,  p-value: < 2.2e-16 \n\n\n\nCovariance matrix of residuals:\n            stock_price      GDP Employment\nstock_price       75.47    290.5       2750\nGDP              290.52  18342.3     112144\nEmployment      2749.66 112143.8     858125\n\nCorrelation matrix of residuals:\n            stock_price    GDP Employment\nstock_price      1.0000 0.2469     0.3417\nGDP              0.2469 1.0000     0.8939\nEmployment       0.3417 0.8939     1.0000\n\n\n\n\nStep 4: Using Cross Validation\n\n\nShow the code\nn=length(dd$stock_price)\nk=39\n\n#n-k=92; 92/4=23;\n\nrmse1 <- matrix(NA, 96,3)\nrmse2 <- matrix(NA, 96,3)\nrmse3 <- matrix(NA,23,4)\nyear<-c()\n\n# Convert data frame to time series object\nts_obj <- ts(dd[, c(2:4)], star=decimal_date(as.Date(\"1990-01-01\",format = \"%Y-%m-%d\")),frequency = 4)\n\nst <- tsp(ts_obj )[1]+(k-1)/4 \n\n\nfor(i in 1:23)\n{\n  \n  xtrain <- window(ts_obj, end=st + i-1)\n  xtest <- window(ts_obj, start=st + (i-1) + 1/4, end=st + i)\n  \n  \n  fit <- VAR(ts_obj, p=5, type='both')\n  fcast <- predict(fit, n.ahead = 4)\n  \n  fgdp<-fcast$fcst$GDP\n  femp<-fcast$fcst$Employment\n  fsp<-fcast$fcst$stock_price\n  ff<-data.frame(fsp[,1],fgdp[,1],femp[,1])\n  \n  year<-st + (i-1) + 1/4\n  \n  ff<-ts(ff,start=c(year,1),frequency = 4)\n  \n  a = 4*i-3\n  b= 4*i\n  rmse1[c(a:b),]  <-sqrt((ff-xtest)^2)\n  \n  fit2 <- VAR(ts_obj, p=9, type='both')\n  fcast2 <- predict(fit2, n.ahead = 4)\n  \n  fgdp<-fcast2$fcst$GDP\n  femp<-fcast2$fcst$Employment\n  fsp<-fcast2$fcst$stock_price\n  ff2<-data.frame(fsp[,1],fgdp[,1],femp[,1])\n  \n  year<-st + (i-1) + 1/4\n  \n  ff2<-ts(ff2,start=c(year,1),frequency = 4)\n  \n  a = 4*i-3\n  b= 4*i\n  rmse2[c(a:b),]  <-sqrt((ff2-xtest)^2)\n}\n\nyr = rep(c(1999:2022),each =4)\nqr = rep(paste0(\"Q\",1:4),24)\n\nrmse1 = data.frame(yr,qr,rmse1)\nnames(rmse1) =c(\"Year\", \"Quater\",\"Stock_Price\",\"GDP\",\"Employment\")\nrmse2 = data.frame(yr,qr,rmse2)\nnames(rmse2) =c(\"Year\", \"Quater\",\"Stock_Price\",\"GDP\",\"Employment\")\n\nggplot() + \n  geom_line(data = rmse1, aes(x = Year, y = Stock_Price),color = \"blue\") +\n  geom_line(data = rmse2, aes(x = Year, y = Stock_Price),color = \"red\") +\n  labs(\n    title = \"CV RMSE for Stock_Price\",\n    x = \"Date\",\n    y = \"RMSE\",\n    guides(colour=guide_legend(title=\"Fit\")))\n\n\n\n\n\n\n\nShow the code\nggplot() + \n  geom_line(data = rmse1, aes(x = Year, y = GDP),color = \"blue\") +\n  geom_line(data = rmse2, aes(x = Year, y = GDP),color = \"red\") +\n  labs(\n    title = \"CV RMSE for GDP\",\n    x = \"Date\",\n    y = \"RMSE\",\n    guides(colour=guide_legend(title=\"Fit\")))\n\n\nWarning: Removed 4 rows containing missing values (`geom_line()`).\nRemoved 4 rows containing missing values (`geom_line()`).\n\n\n\n\n\n\n\nShow the code\nggplot() + \n  geom_line(data = rmse1, aes(x = Year, y = Employment),color = \"blue\") +\n  geom_line(data = rmse2, aes(x = Year, y = Employment),color = \"red\") +\n  labs(\n    title = \"CV RMSE for Employment\",\n    x = \"Date\",\n    y = \"RMSE\",\n    guides(colour=guide_legend(title=\"Fit\")))\n\n\nWarning: Removed 4 rows containing missing values (`geom_line()`).\nRemoved 4 rows containing missing values (`geom_line()`).\n\n\n\n\n\nfit 1 is better\n\n\nStep 5: Forecast\n\n\nShow the code\nforecasts <- predict(VAR(dd[, c(2:4)], p=5, type='both'))\n\n# visualize the iterated forecasts\nplot(forecasts)"
  }
]